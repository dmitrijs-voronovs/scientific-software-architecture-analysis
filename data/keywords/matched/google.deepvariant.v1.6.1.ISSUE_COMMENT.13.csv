id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/287:27,usability,command,command,27,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:215,usability,command,command,215,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:339,usability,command,command,339,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:462,usability,document,documentation,462,"Hi,. Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:. https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:88,availability,error,error,88,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:249,availability,error,error,249,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4491,availability,checkpoint,checkpoint,4491," the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4817,availability,operat,operations,4817,"3:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using confi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4869,availability,operat,operations,4869,"] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5108,availability,servic,service,5108,"967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5116,availability,servic,service,5116,"ake_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5260,availability,servic,service,5260,". user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5268,availability,servic,service,5268,"m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:6390,availability,Cluster,ClusterSpec,6390,"process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7687,availability,slo,sloppy,7687,"tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future versio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8744,availability,operat,operator,8744,"al_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 4713834524",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9309,availability,Restor,Restoring,9309,"ns will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9661,availability,Restor,Restoring,9661,"moved in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:14050,availability,error,error,14050,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2113,deployability,log,login,2113,"stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5108,deployability,servic,service,5108,"967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5116,deployability,servic,service,5116,"ake_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5260,deployability,servic,service,5260,". user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5268,deployability,servic,service,5268,"m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5326,deployability,Version,Version,5326,"me /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:6390,deployability,Cluster,ClusterSpec,6390,"process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7050,deployability,version,version,7050,"ry_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7076,deployability,updat,updating,7076,"step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7516,deployability,version,version,7516,"tion_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7542,deployability,updat,updating,7542,"ief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8129,deployability,version,version,8129,"layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 es",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8155,deployability,updat,updating,8155,"566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8684,deployability,version,version,8684,"loppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8710,deployability,updat,updating,8710,", use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9013,deployability,version,version,9013,"nd_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9039,deployability,updat,updating,9039,"python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11376,deployability,Fail,Failed,11376,"/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11608,deployability,modul,module,11608,"ng: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13041,deployability,modul,module,13041,"p/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2099,energy efficiency,Power,Power,2099,"RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4508,energy efficiency,model,models,4508,"you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4519,energy efficiency,model,model,4519,"in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4656,energy efficiency,core,core,4656,"3:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.75054",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4722,energy efficiency,optim,optimized,4722,"h/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4775,energy efficiency,CPU,CPU,4775,"0.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4979,energy efficiency,core,core,4979,"327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5024,energy efficiency,CPU,CPU,5024,"les.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5028,energy efficiency,Frequenc,Frequency,5028,"535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5376,energy efficiency,core,core,5376,"""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5577,energy efficiency,model,modeling,5577,"s.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5607,energy efficiency,model,model,5607,"o 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5675,energy efficiency,estimat,estimator,5675,"e_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5793,energy efficiency,estimat,estimator,5793,"rformance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8303,energy efficiency,optim,optimizations,8303,"ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8405,energy efficiency,estimat,estimator,8405,"rom tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8578,energy efficiency,model,modeling,8578,"ve(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modelin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9131,energy efficiency,estimat,estimator,9131,". Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile """,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9352,energy efficiency,model,model,9352,"lementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/molda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9575,energy efficiency,model,modeling,9575,"odeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postpr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9704,energy efficiency,model,model,9704,"or updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:74,integrability,messag,message,74,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1196,integrability,event,event,1196,"string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further developmen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1215,integrability,pub,public,1215,"m still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1513,integrability,buffer,buffer,1513,"@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2023,integrability,pub,publication,2023,"G_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5108,integrability,servic,service,5108,"967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5116,integrability,servic,service,5116,"ake_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5260,integrability,servic,service,5260,". user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5268,integrability,servic,service,5268,"m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5326,integrability,Version,Version,5326,"me /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7050,integrability,version,version,7050,"ry_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7516,integrability,version,version,7516,"tion_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8072,integrability,batch,batching,8072,"updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8129,integrability,version,version,8129,"layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 es",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8684,integrability,version,version,8684,"loppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9013,integrability,version,version,9013,"nd_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11194,integrability,Transform,Transforming,11194,"scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qua",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13337,integrability,sub,subprocess,13337,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13430,integrability,sub,subprocess,13430,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13511,integrability,sub,subprocess,13511,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:74,interoperability,messag,message,74,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4661,interoperability,platform,platform,4661,".388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4984,interoperability,platform,platform,4984,"32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5181,interoperability,platform,platform,5181,"857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11194,interoperability,Transform,Transforming,11194,"scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qua",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11709,interoperability,platform,platform,11709," 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:26,modifiability,variab,variables,26,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:188,modifiability,PAC,PACBIO,188,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1318,modifiability,interm,intermediate,1318,"0.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1366,modifiability,Interm,Intermediate,1366,"ickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3478,modifiability,deco,decod,3478,"base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5108,modifiability,servic,service,5108,"967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5116,modifiability,servic,service,5116,"ake_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5260,modifiability,servic,service,5260,". user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5268,modifiability,servic,service,5268,"m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5326,modifiability,Version,Version,5326,"me /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': Non",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:6830,modifiability,pac,packages,6830,"dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7050,modifiability,version,version,7050,"ry_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7133,modifiability,layer,layers,7133,"ice_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7516,modifiability,version,version,7516,"tion_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8129,modifiability,version,version,8129,"layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 es",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8684,modifiability,version,version,8684,"loppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8865,modifiability,pac,packages,8865,"22731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 4713834524537",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8882,modifiability,layer,layers,8882,"5376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8889,modifiability,layer,layers,8889,"precation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8905,modifiability,Layer,Layer,8905,"] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9013,modifiability,version,version,9013,"nd_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9062,modifiability,layer,layer,9062,"al.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/mold",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9319,modifiability,paramet,parameters,9319,"ke care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9671,modifiability,paramet,parameters,9671," future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11608,modifiability,modul,module,11608,"ng: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11677,modifiability,pac,packages,11677,"ngle_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13041,modifiability,modul,module,13041,"p/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13101,modifiability,pac,packages,13101,"pvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13201,modifiability,pac,packages,13201,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:88,performance,error,error,88,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:249,performance,error,error,249,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:484,performance,time,time,484,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1473,performance,time,time,1473,"salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1488,performance,parallel,parallel,1488,"0:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_qualit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1994,performance,Parallel,Parallel,1994,"r/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2071,performance,Parallel,Parallel,2071,"he directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2285,performance,Parallel,Parallel,2285," Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2348,performance,parallel,parallel,2348,"xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4328,performance,time,time,4328,"3 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Vers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4722,performance,optimiz,optimized,4722,"h/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4775,performance,CPU,CPU,4775,"0.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4796,performance,perform,performance,4796,"veSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5024,performance,CPU,CPU,5024,"les.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5477,performance,Tune,Tune,5477,"ord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5527,performance,perform,performance,5527,"0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8072,performance,batch,batching,8072,"updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8303,performance,optimiz,optimizations,8303,"ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9998,performance,time,time,9998," in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13551,performance,time,time,13551,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:14050,performance,error,error,14050,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1043,reliability,doe,does,1043,"'t include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please ci",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4491,reliability,checkpoint,checkpoint,4491," the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7687,reliability,slo,sloppy,7687,"tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future versio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9309,reliability,Restor,Restoring,9309,"ns will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9661,reliability,Restor,Restoring,9661,"moved in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11376,reliability,Fail,Failed,11376,"/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:88,safety,error,error,88,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:249,safety,error,error,249,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:381,safety,test,testdata,381,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1610,safety,test,testdata,1610,"t:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /sc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1693,safety,test,testdata,1693,"\. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2113,safety,log,login,2113,"stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2644,safety,test,testdata,2644,"ta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 13421772",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2775,safety,input,inputs,2775,"frecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2885,safety,test,testdata,2885,"Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3453,safety,input,input,3453," apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3753,safety,test,testdata,3753,"py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3917,safety,test,testdata,3917,"bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7076,safety,updat,updating,7076,"step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7542,safety,updat,updating,7542,"ief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8155,safety,updat,updating,8155,"566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8710,safety,updat,updating,8710,", use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9039,safety,updat,updating,9039,"python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:10085,safety,test,testdata,10085,"ead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sortin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11608,safety,modul,module,11608,"ng: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13041,safety,modul,module,13041,"p/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13637,safety,test,testdata,13637,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:14050,safety,error,error,14050,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2086,security,Command-Lin,Command-Line,2086,"nset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2113,security,log,login,2113,"stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4508,security,model,models,4508,"you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4519,security,model,model,4519,"in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5577,security,model,modeling,5577,"s.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5607,security,model,model,5607,"o 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7076,security,updat,updating,7076,"step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:7542,security,updat,updating,7542,"ief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz. W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8155,security,updat,updating,8155,"566 47138345245376 data_providers.py:369] self.input_read_threads=8. W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8578,security,model,modeling,8578,"ve(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modelin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:8710,security,updat,updating,8710,", use `tf.da ta.Options.experimental_determinstic`. I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48. W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9039,security,updat,updating,9039,"python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9352,security,model,model,9352,"lementation. I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn. W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/molda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9575,security,model,modeling,9575,"odeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postpr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9704,security,model,model,9704,"or updating:. Deprecated in favor of operator or tf.math.divide. W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:381,testability,test,testdata,381,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:739,testability,unit,unittest,739,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1610,testability,test,testdata,1610,"t:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /sc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1635,testability,unit,unittest,1635,"opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quicks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1693,testability,test,testdata,1693,"\. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2113,testability,log,login,2113,"stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2644,testability,test,testdata,2644,"ta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 13421772",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2885,testability,test,testdata,2885,"Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3753,testability,test,testdata,3753,"py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3917,testability,test,testdata,3917,"bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:10085,testability,test,testdata,10085,"ead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sortin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:10110,testability,unit,unittest,10110,"8 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:11451,testability,Trace,Traceback,11451,"ants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls. I0327 13:32:43.693562 47816170105536 postprocess_variants.py:976] CVO sorting took 3.3 83557001749674e-05 minutes. I0327 13:32:43.694538 47816170105536 postprocess_variants.py:978] Transforming call_va riants_output to variants. I0327 13:32:43.694916 47816170105536 postprocess_variants.py:1004] Merging and writing variants to VCF and gVCF. [E::hts_open_format] Failed to open file /scratch/moldach/bin/quickstart-output/output .vcf.gz. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1039, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"" , line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:12943,testability,Trace,Traceback,12943,"gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13637,testability,test,testdata,13637,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13662,testability,unit,unittest,13662,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:88,usability,error,error,88,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:249,usability,error,error,249,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:995,usability,user,user,995,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1126,usability,user,user,1126,"lace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:. ## Set the environment. ```. [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0"". [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata"". [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output"". [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USEN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:1458,usability,command,command,1458,"cdr767 bin]$ salloc --time=0:30:0 --mem=8000. [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. > docker://google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. > --num_shards=1. WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m appin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2086,usability,Command,Command-Line,2086,"nset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2105,usability,Tool,Tool,2105,"ME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2167,usability,help,helps,2167,"ying to pull image in the event that it is a public image. I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-0000",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:2775,usability,input,inputs,2775,"frecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:3453,usability,input,input,3453," apping_quality: 10. min_base_quality: 10. min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs. I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]. I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz. I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4266,usability,user,user,4266,"63xxmwmi/gvcf.tfrecord-00000-of-00001.gz. I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4313,usability,command,command,4313,"3:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref. 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728. I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:4796,usability,perform,performance,4796,"veSamReader. I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]. I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants. I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s. user 0m5.478s. sys 0m3.350s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:5527,usability,perform,performance,5527,"0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0. 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags. 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz. 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:. 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version. 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance. I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters. W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m. I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9935,usability,user,user,9935,"hon.kera s.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.69",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:9983,usability,command,command,9983,"ll be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn. I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized. I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op. I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p. I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA... I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt. I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]. I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s. user 0m20.774s. sys 0m5.396s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz. 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86. 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86. 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls. 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:12847,usability,user,user,12847,"/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/absl_py/absl/app.py"", line 251, in _run_ main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/deepvariant/postp rocess_variants.py"", line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13359,usability,command,command,13359,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13542,usability,Command,Command,13542,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:13974,usability,statu,status,13974,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:14050,usability,error,error,14050,"line 1007, in main. FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \. File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__. self._writer = self._native_writer(output_path, **kwargs). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer. exclude_header=exclude_header). File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__. writer_options). ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s. user 0m2.899s. sys 0m0.651s. I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1. [moldach@cdr767 bin]$ ^C. [moldach@cdr767 bin]$ exit. exit. srun: error: cdr767: task 0: Exited with exit code 130. ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:286,availability,error,error,286,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:27,deployability,log,logs,27,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:286,performance,error,error,286,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:27,safety,log,logs,27,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:286,safety,error,error,286,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:27,security,log,logs,27,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:27,testability,log,logs,27,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:286,usability,error,error,286,"Hi @moldach . somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```. singularity run -B $PWD,/usr/lib/locale/ \. ```. instead of . ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/287:53,reliability,Doe,Does,53,"@moldach Actually, I think I might know the problem. Does your OUTPUT_DIR exist? I suggest you add a line below:. ```. OUTPUT_DIR=""${PWD}/quickstart-output"". mkdir -p ${OUTPUT_DIR}. ```. to first ensure the directory is created.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287
https://github.com/google/deepvariant/issues/288:68,safety,input,input,68,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:176,safety,input,input,176,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:280,safety,except,except,280,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:403,safety,input,input,403,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:243,security,modif,modifying,243,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:68,usability,input,input,68,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:176,usability,input,input,176,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/288:403,usability,input,input,403,"There is an answer from @akolesnikov . ---. I'm afraid that passing input reads through stdin would not be possible. BAM reader expects an index file to be the same name is an input BAM file with "".bai"" added. Console output can be removed by modifying run_deepvariant.py script, except the TensorFlow warning, which originates from TensorFlow. Bottom line is I'm afraid it wouldn't be possible to read input from stdin and write output to stdout. ---",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/288
https://github.com/google/deepvariant/issues/289:282,availability,ping,ping,282,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:78,deployability,version,version,78,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:78,integrability,version,version,78,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:78,modifiability,version,version,78,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:119,modifiability,pac,packages,119,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:246,performance,time,time,246,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:10,deployability,Updat,Update,10,"@Stikus . Update: @gunjanbaid will start woking on this, and we'll get back to you soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:10,safety,Updat,Update,10,"@Stikus . Update: @gunjanbaid will start woking on this, and we'll get back to you soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:10,security,Updat,Update,10,"@Stikus . Update: @gunjanbaid will start woking on this, and we'll get back to you soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/289:183,usability,close,close,183,"Hi @Stikus, the Ubuntu 18 CLIF binary has now been uploaded, so scripts should work as intended. We are working on adding other CLIF binaries soon. Thanks for pointing this out. I'll close this issue, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289
https://github.com/google/deepvariant/issues/290:80,deployability,releas,released,80,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:103,deployability,version,version,103,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:163,deployability,version,version,163,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:188,deployability,updat,update,188,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:103,integrability,version,version,103,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:163,integrability,version,version,163,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:103,modifiability,version,version,103,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:163,modifiability,version,version,163,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:188,safety,updat,update,188,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:188,security,updat,update,188,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:51,usability,visual,visualization,51,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:215,usability,visual,visualization,215,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:18,availability,avail,available,18,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:43,deployability,releas,release,43,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:18,reliability,availab,available,18,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:18,safety,avail,available,18,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:18,security,availab,available,18,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:276,availability,avail,available,276,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:59,deployability,version,version,59,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:289,deployability,version,version,289,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:350,deployability,pipelin,pipeline,350,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:59,integrability,version,version,59,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:289,integrability,version,version,289,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:350,integrability,pipelin,pipeline,350,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:59,modifiability,version,version,59,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:289,modifiability,version,version,289,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:276,reliability,availab,available,276,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:276,safety,avail,available,276,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:276,security,availab,available,276,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md. The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:55,usability,visual,visualize,55,"Hi, I'm running the deepvariant_1.0.0.sif and I cannot visualize the output of the file. Has to be open from the same file folder?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:278,testability,understand,understand,278,"Hi @leorippel , can you provide more information on your run? For example, can you try commands similar to https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md , and let us know what commands you ran, and what files you expected that are not there? I understand that you're running with Singularity. It'll help us look into your issue if you provide more details. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:87,usability,command,commands,87,"Hi @leorippel , can you provide more information on your run? For example, can you try commands similar to https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md , and let us know what commands you ran, and what files you expected that are not there? I understand that you're running with Singularity. It'll help us look into your issue if you provide more details. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:210,usability,command,commands,210,"Hi @leorippel , can you provide more information on your run? For example, can you try commands similar to https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md , and let us know what commands you ran, and what files you expected that are not there? I understand that you're running with Singularity. It'll help us look into your issue if you provide more details. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:333,usability,help,help,333,"Hi @leorippel , can you provide more information on your run? For example, can you try commands similar to https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md , and let us know what commands you ran, and what files you expected that are not there? I understand that you're running with Singularity. It'll help us look into your issue if you provide more details. . Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:792,energy efficiency,gpu,gpus,792,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:896,energy efficiency,gpu,gpu,896,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:27,integrability,messag,message,27,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:61,integrability,sub,subset,61,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:27,interoperability,messag,message,27,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:792,performance,gpu,gpus,792,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:896,performance,gpu,gpu,896,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:82,safety,test,test,82,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:250,safety,permiss,permissions,250,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:370,safety,input,input,370,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:510,safety,input,input,510,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:821,safety,input,input,821,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:972,safety,input,input,972,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:999,safety,input,input,999,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:82,testability,test,test,82,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:129,usability,visual,visualize,129,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:283,usability,help,help,283,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:370,usability,input,input,370,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:428,usability,command,commands,428,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:510,usability,input,input,510,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:821,usability,input,input,821,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:972,usability,input,input,972,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:999,usability,input,input,999,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0"". BASE=""${PWD}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""10consensus.fasta"". BAM=""268_041_m10.sorted.bam"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""M10.output.vcf.gz"". OUTPUT_GVCF=""M10.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}"". mkdir -p ""${INPUT_DIR}"". mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:312,deployability,version,version,312,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:312,integrability,version,version,312,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:312,modifiability,version,version,312,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:228,safety,compl,completely,228,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:228,security,compl,completely,228,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:109,usability,user,user-images,109,"From the output.visual_report.zip file you attached, I got the HTML file, and can view it:. ![image](https://user-images.githubusercontent.com/471813/94210366-5943ae00-fe83-11ea-8931-528607cc5bc8.png). Do you mean it looks just completely empty to you? Can you try a different computer, or tell us which browser version you're using? ( @MariaNattestad FYI. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:98,deployability,version,version,98,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:145,deployability,version,version,145,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:258,deployability,version,version,258,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:98,integrability,version,version,98,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:145,integrability,version,version,145,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:258,integrability,version,version,258,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:98,modifiability,version,version,98,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:145,modifiability,version,version,145,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:258,modifiability,version,version,258,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:19,safety,compl,completely,19,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:218,safety,Compl,Completely,218,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:19,security,compl,completely,19,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:218,security,Compl,Completely,218,"Wow. Yes, it looks completely empty for me. . I tried in both chrome and firefox. google-chrome --version. Google Chrome 85.0.4183.83. firefox --version. Mozilla Firefox 80.0.1. Changed computer and tried on explorer. Completely blank. Internet Explorer 11. version 11.1726.17134.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:505,performance,network,network,505,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:531,safety,compl,complete,531,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:447,security,secur,security,447,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:505,security,network,network,505,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:531,security,compl,complete,531,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/290:563,usability,feedback,feedback,563,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290
https://github.com/google/deepvariant/issues/291:327,deployability,updat,updating,327,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:737,deployability,build,build,737,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:839,deployability,Depend,Depending,839,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1267,deployability,configurat,configuration-file-for-each,1267,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:439,energy efficiency,GPU,GPU,439,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1000,energy efficiency,Current,Currently,1000,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1474,energy efficiency,Cloud,Cloud,1474,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:839,integrability,Depend,Depending,839,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1267,integrability,configur,configuration-file-for-each,1267,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:839,modifiability,Depend,Depending,839,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1267,modifiability,configur,configuration-file-for-each,1267,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:439,performance,GPU,GPU,439,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:399,reliability,Doe,Does,399,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:327,safety,updat,updating,327,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:839,safety,Depend,Depending,839,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:26,security,modif,modifying,26,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:327,security,updat,updating,327,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:896,security,modif,modify,896,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1267,security,configur,configuration-file-for-each,1267,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:420,testability,plan,plan,420,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:839,testability,Depend,Depending,839,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:982,testability,simpl,simplest,982,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:982,usability,simpl,simplest,982,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1059,usability,document,documentation,1059,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/291:1398,usability,document,documentation,1398,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU? * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291
https://github.com/google/deepvariant/issues/292:18,availability,error,error,18,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:24,integrability,messag,message,24,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:24,interoperability,messag,message,24,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:18,performance,error,error,18,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:18,safety,error,error,18,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:18,usability,error,error,18,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:32,usability,indicat,indicates,32,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:268,usability,command,command,268,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:483,availability,error,error,483,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:766,availability,error,error,766,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1752,availability,error,error,1752," with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:458,deployability,version,version,458,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:733,deployability,fail,failed,733,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1465,deployability,Fail,Failed,1465,"s used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3022,deployability,modul,module,3022,"gle_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3050,deployability,fail,failed,3050,"realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledPro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3608,deployability,modul,module,3608,"s_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1768,energy efficiency,Current,Current,1768,". ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:458,integrability,version,version,458,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3904,integrability,sub,subprocess,3904,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3997,integrability,sub,subprocess,3997,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4078,integrability,sub,subprocess,4078,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4158,integrability,buffer,buffer,4158,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:458,modifiability,version,version,458,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3022,modifiability,modul,module,3022,"gle_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3608,modifiability,modul,module,3608,"s_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3668,modifiability,pac,packages,3668,"gle_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3768,modifiability,pac,packages,3768,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:483,performance,error,error,483,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:766,performance,error,error,766,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1752,performance,error,error,1752," with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3031,performance,parallel,parallel,3031,"riant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_cal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4118,performance,time,time,4118,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4133,performance,parallel,parallel,4133,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:733,reliability,fail,failed,733,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1465,reliability,Fail,Failed,1465,"s used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3050,reliability,fail,failed,3050,"realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledPro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:483,safety,error,error,483,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:766,safety,error,error,766,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1752,safety,error,error,1752," with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3022,safety,modul,module,3022,"gle_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3608,safety,modul,module,3608,"s_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3148,security,Worm,Wormbase,3148,"les/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3236,security,Worm,Wormbase,3236,"s. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4255,security,Worm,Wormbase,4255,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4345,security,Worm,Wormbase,4345,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3510,testability,Trace,Traceback,3510,"68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' retur",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:338,usability,guidanc,guidance,338,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:483,usability,error,error,483,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:766,usability,error,error,766,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```. @SQ SN:I LN:15072434. @SQ SN:II LN:15279421. @SQ SN:III LN:13783801. @SQ SN:IV LN:17493829. @SQ SN:V LN:20924180. @SQ SN:X LN:17718942. @SQ SN:MtDNA LN:13794. ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1600,usability,statu,statusor,1600,"ed it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1623,usability,statu,status,1623," the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1639,usability,statu,status,1639,"g **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1752,usability,error,error,1752," with this error:. ```. ... I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]. I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]. I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]. I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]. I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]. I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]. [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?). 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020. Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3411,usability,user,user,3411,"om_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3926,usability,command,command,3926,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4109,usability,Command,Command,4109,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4533,usability,statu,status,4533,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:4760,usability,help,help,4760,"e68bsnf0/runfiles/absl_py/absl/app.py"", line 251 in _run_main. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/absl_py/absl/app.py"", line 300 in run. File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa --reads /scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam --examples /tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz --gvcf /tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz --task 0. real 81m0.387s. user 66m46.794s. sys 0m15.672s. I0402 20:46:31.181916 47643471256256 run_deepvariant.py:321] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/c_elegans.PRJNA13758.WS265.genomic.fa"" --reads ""/scratch/moldach/bin/DEEPVARIANT/Wormbase/maddog_bam_trim_bwaMEM_sort_dedupped.bam"" --examples ""/tmp/tmp5lcsp7ms/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp5lcsp7ms/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 250. ```. Any idea why this could be happening (still wrong reference possibly?). Should we keep debugging this or would the _safest_ thing be to re-run the alignment on a _known_ ref and then try again? Thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:224,availability,error,error,224,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:306,availability,error,error,306,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:727,deployability,fail,fail,727,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:531,interoperability,mismatch,mismatched,531,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:422,modifiability,exten,extend,422,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:657,modifiability,exten,extending,657,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:224,performance,error,error,224,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:306,performance,error,error,306,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:331,reliability,diagno,diagnose,331,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:413,reliability,doe,does,413,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:727,reliability,fail,fail,727,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:224,safety,error,error,224,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:306,safety,error,error,306,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:812,safety,safe,safest,812,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:331,testability,diagno,diagnose,331,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:224,usability,error,error,224,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:306,usability,error,error,306,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:789,usability,behavi,behavior,789,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:421,deployability,modul,module,421,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2404,deployability,modul,module,2404,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2700,integrability,sub,subprocess,2700,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2793,integrability,sub,subprocess,2793,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2874,integrability,sub,subprocess,2874,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2948,integrability,buffer,buffer,2948,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:507,interoperability,platform,platform,507,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:421,modifiability,modul,module,421,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:480,modifiability,pac,packages,480,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2404,modifiability,modul,module,2404,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2464,modifiability,pac,packages,2464,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2564,modifiability,pac,packages,2564,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2914,performance,time,time,2914,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2929,performance,parallel,parallel,2929,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:1833,reliability,doe,doesnt,1833,"_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. ra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:421,safety,modul,module,421,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2404,safety,modul,module,2404,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3013,safety,input,input,3013,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3064,safety,input,input,3064,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:274,testability,Trace,Traceback,274,"Hello, I had a similar issue. since the exit code is 5, I tried to fix with chmod +644 or other codes however it didn't work. My reference is different vertebrate viruses that I merged via ```cat``` And reads are, not mapped reads to human.Then mapped to viral genome. ```. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1461, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1451, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1341, in make_examples_runner. candidates, examples, gvcfs = region_processor.process(region). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1041, in process. for example in self.create_pileup_examples(candidate). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1141, in create_pileup_examples. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_val",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2306,testability,Trace,Traceback,2306,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2275,usability,user,user,2275,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2722,usability,command,command,2722,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:2905,usability,Command,Command,2905,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3013,usability,input,input,3013,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3064,usability,input,input,3064,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:3200,usability,statu,status,3200,"s. pileup_images = self.pic.create_pileup_images(dv_call). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 345, in create_pileup_images. return [_make_one(alts) for alts in self._alt_allele_combinations(variant)]. File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 342, in _make_one. image = self.build_pileup(dv_call, ref, reads, alt_alleles). File ""/tmp/Bazel.runfiles_6B0qe2/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 277, in build_pileup. dv_call.variant). ValueError: ('center of refbases doesnt match variant.refbases', 110, 'C', reference_bases: ""T"". alternate_bases: ""C"". calls {. info {. key: ""AD"". value {. values {. int_value: 0. }. values {. int_value: 3. }. }. }. info {. key: ""DP"". value {. values {. int_value: 3. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 1.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""default"". }. end: 3002. reference_name: ""NC_022518.1"". start: 3001. ). real 0m28.014s. user 1m56.205s. sys 0m16.675s. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>. app.run(main). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run. _run_main(main, args). File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 4 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/03.reference_files/viral_db_v2.fa"" --reads ""/input/CL100135515_L02_unmap.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@5.gz"" --task {}' returned non-zero exit status 5. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:105,safety,compl,complete,105,"Hi @MariaNattestad ,. I found the problem that it was related with "".bam"" file. Alignment process wasn't complete for related file. For other files, process works as expected. Thanks. Bilgehan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:105,security,compl,complete,105,"Hi @MariaNattestad ,. I found the problem that it was related with "".bam"" file. Alignment process wasn't complete for related file. For other files, process works as expected. Thanks. Bilgehan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:43,deployability,updat,update,43,"@bilgehannevruz okay great, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:43,safety,updat,update,43,"@bilgehannevruz okay great, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/292:43,security,updat,update,43,"@bilgehannevruz okay great, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292
https://github.com/google/deepvariant/issues/293:92,modifiability,Pac,PacBio,92,"Hi @gevro . Yes, DeepVariant should not have any problem with smaller insert CCS reads from PacBio.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/293
https://github.com/google/deepvariant/issues/294:99,performance,network,network,99,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:48,security,ident,identify,48,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:99,security,network,network,99,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:224,testability,coverag,coverages,224,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:354,testability,coverag,coverage,354,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:435,testability,coverag,coverages,435,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:153,usability,support,support,153,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/294:282,usability,support,support,282,"Hi @gevro,. DeepVariant uses some heuristics to identify candidate sites for calling by the neural network. These heuristics require at least 2 reads to support a variant for that to be a candidate. As a result, at very low coverages (1x-3x), many of the variants won't have enough support to cross this threshold. I would recommend something like 4x-5x coverage for CCS for DeepVariant. I am not sure how GATK will behave at such low coverages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/294
https://github.com/google/deepvariant/issues/295:21,deployability,build,building,21,"Hi @gevro,. Early in building the PacBio models, we benchmarked on both NGMLR and Minimap2/pbmm2 and found that they gave similar quality of variant calls. We train with data from Minimap2/pbmm2, and I would recommend using pbmm2 for the mapper.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/295
https://github.com/google/deepvariant/issues/295:41,energy efficiency,model,models,41,"Hi @gevro,. Early in building the PacBio models, we benchmarked on both NGMLR and Minimap2/pbmm2 and found that they gave similar quality of variant calls. We train with data from Minimap2/pbmm2, and I would recommend using pbmm2 for the mapper.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/295
https://github.com/google/deepvariant/issues/295:34,modifiability,Pac,PacBio,34,"Hi @gevro,. Early in building the PacBio models, we benchmarked on both NGMLR and Minimap2/pbmm2 and found that they gave similar quality of variant calls. We train with data from Minimap2/pbmm2, and I would recommend using pbmm2 for the mapper.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/295
https://github.com/google/deepvariant/issues/295:41,security,model,models,41,"Hi @gevro,. Early in building the PacBio models, we benchmarked on both NGMLR and Minimap2/pbmm2 and found that they gave similar quality of variant calls. We train with data from Minimap2/pbmm2, and I would recommend using pbmm2 for the mapper.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/295
https://github.com/google/deepvariant/issues/296:91,deployability,contain,containers,91,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:416,deployability,releas,releases,416,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:321,modifiability,interm,intermediate,321,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:271,deployability,log,logs,271,"Hi @meghanasp21 . If you're running with Singularity, I suspect a temp directory has been created directly under your /tmp/. The line of code that created the tempdir is this line:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L234. In the logs as you run, it should give you the name of the corresponding directory under `/tmp/`. You can manually remove the directory after your run is finished.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:271,safety,log,logs,271,"Hi @meghanasp21 . If you're running with Singularity, I suspect a temp directory has been created directly under your /tmp/. The line of code that created the tempdir is this line:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L234. In the logs as you run, it should give you the name of the corresponding directory under `/tmp/`. You can manually remove the directory after your run is finished.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:271,security,log,logs,271,"Hi @meghanasp21 . If you're running with Singularity, I suspect a temp directory has been created directly under your /tmp/. The line of code that created the tempdir is this line:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L234. In the logs as you run, it should give you the name of the corresponding directory under `/tmp/`. You can manually remove the directory after your run is finished.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:271,testability,log,logs,271,"Hi @meghanasp21 . If you're running with Singularity, I suspect a temp directory has been created directly under your /tmp/. The line of code that created the tempdir is this line:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L234. In the logs as you run, it should give you the name of the corresponding directory under `/tmp/`. You can manually remove the directory after your run is finished.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:128,performance,parallel,parallelizing,128,"Perfect. Will do. Is there a way to control the name of the temp directory? I think this would be useful especially because I'm parallelizing my runs and don't want to delete tmp directories from active running jobs. Thanks,. Meghana",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:36,security,control,control,36,"Perfect. Will do. Is there a way to control the name of the temp directory? I think this would be useful especially because I'm parallelizing my runs and don't want to delete tmp directories from active running jobs. Thanks,. Meghana",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:36,testability,control,control,36,"Perfect. Will do. Is there a way to control the name of the temp directory? I think this would be useful especially because I'm parallelizing my runs and don't want to delete tmp directories from active running jobs. Thanks,. Meghana",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:77,interoperability,specif,specify,77,"Yes, You can use the `--intermediate_results_dir` flag in run_deepvariant to specify the directory:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L73",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:199,availability,unavail,unavailable,199,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:100,deployability,stack,stackoverflow,100,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:178,deployability,resourc,resource-temporarily-unavailable,178,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:178,energy efficiency,resourc,resource-temporarily-unavailable,178,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:345,interoperability,bind,bind,345,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:345,modifiability,bind,bind,345,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:472,modifiability,PAC,PACBIO,472,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:178,performance,resourc,resource-temporarily-unavailable,178,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:178,safety,resourc,resource-temporarily-unavailable,178,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:178,testability,resourc,resource-temporarily-unavailable,178,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,usability,command,commands,58,"I am having the same issue with singularity. . Here is my commands:. ```. ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. BIN_VERSION=""1.1.0"". mkdir -p deepvariant_m9. mkdir -p deepvariant_m9/intermediate_results_dir. singularity exec --bind /DATA/transfer:/mnt \. docker://google/deepvariant:${BIN_VERSION} \. /opt/deepvariant/bin/run_deepvariant \. --model_type PACBIO \. --ref /mnt/asm.ctg.fa \. --reads /mnt/BAM/DNA/m9.sorted.bam \. --output_vcf deepvariant_m9/m9.vcf.gz \. --num_shards 16 \. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. As suggested, I set the `--intermediate_results_dir `. Any comments or suggestions? @pichuan Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:70,availability,operat,operating,70,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:107,availability,error,error,107,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,deployability,version,version,58,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:87,deployability,version,version,87,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,integrability,version,version,58,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:87,integrability,version,version,87,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:113,integrability,messag,message,113,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:113,interoperability,messag,message,113,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,modifiability,version,version,58,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:87,modifiability,version,version,87,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:107,performance,error,error,107,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:107,safety,error,error,107,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:181,testability,understand,understand,181,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:107,usability,error,error,107,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:160,usability,help,help,160,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:211,usability,help,helpful,211,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc. Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,availability,Error,Error,58,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:153,availability,Error,Error,153,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1127,availability,error,error,1127,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:12,deployability,version,version,12,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:517,deployability,modul,module,517,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:12,integrability,version,version,12,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:104,integrability,buffer,buffer,104,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:813,integrability,sub,subprocess,813,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:906,integrability,sub,subprocess,906,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:12,modifiability,version,version,12,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:517,modifiability,modul,module,517,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:577,modifiability,pac,packages,577,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:677,modifiability,pac,packages,677,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1162,modifiability,interm,intermediate,1162,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:48,performance,parallel,parallel,48,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,performance,Error,Error,58,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:132,performance,disk,disk,132,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:143,performance,parallel,parallel,143,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:153,performance,Error,Error,153,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1127,performance,error,error,1127,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,safety,Error,Error,58,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:153,safety,Error,Error,153,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:517,safety,modul,module,517,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1127,safety,error,error,1127,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:419,testability,Trace,Traceback,419,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:58,usability,Error,Error,58,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:153,usability,Error,Error,153,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:227,usability,close,close,227,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:322,usability,user,user,322,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:835,usability,command,command,835,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1127,usability,error,error,1127,"singularity version 3.7.0-1.el8. CentOS 8. ```. parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full? parallel: Error: Change $TMPDIR with --tmpdir or use --compress. Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s. user	0m2.686s. sys	0m4.872s. I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:53,interoperability,bind,bind,53,"@simoncchu If you ran with:. ```. singularity exec --bind /DATA/transfer:/mnt \. ```. and. ```. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. I think the intermediate output should be in `/DATA/transfer/deepvariant_tmp` ? If this run succeeded, can you check what you have under /DATA/transfer and /DATA/transfer/deepvariant_tmp ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:53,modifiability,bind,bind,53,"@simoncchu If you ran with:. ```. singularity exec --bind /DATA/transfer:/mnt \. ```. and. ```. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. I think the intermediate output should be in `/DATA/transfer/deepvariant_tmp` ? If this run succeeded, can you check what you have under /DATA/transfer and /DATA/transfer/deepvariant_tmp ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:163,modifiability,interm,intermediate,163,"@simoncchu If you ran with:. ```. singularity exec --bind /DATA/transfer:/mnt \. ```. and. ```. --intermediate_results_dir /mnt/deepvariant_tmp/. ```. I think the intermediate output should be in `/DATA/transfer/deepvariant_tmp` ? If this run succeeded, can you check what you have under /DATA/transfer and /DATA/transfer/deepvariant_tmp ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,availability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,availability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:18,modifiability,interm,intermediate,18,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,performance,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,performance,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,safety,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,safety,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,usability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,usability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,availability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,availability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:18,modifiability,interm,intermediate,18,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,performance,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,performance,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,safety,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,safety,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:115,usability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:168,usability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:51,energy efficiency,current,current,51,"@simoncchu . Is there a ""tmp"" directory under your current working directory then?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1841,availability,down,downloaded,1841,"ute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:86,deployability,version,version,86,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:165,deployability,observ,observed,165,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:609,deployability,version,version,609,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1114,deployability,version,version,1114," tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1170,deployability,releas,release,1170," And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1200,deployability,VERSION,VERSION,1200,"mediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1532,deployability,instal,installed,1532,"ingularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1636,deployability,instal,installation,1636,"I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1654,deployability,instal,installation-on-linux,1654,"ow so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1691,deployability,version,version,1691,"---. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1749,deployability,version,version,1749,"S 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1770,deployability,version,version,1770,"```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:2008,deployability,build,build,2008,"-zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:281,energy efficiency,current,current,281,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:866,energy efficiency,cloud,cloud-platform,866,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:938,energy efficiency,cloud,cloud,938,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:2930,energy efficiency,current,current,2930,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3536,energy efficiency,current,current,3536,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:86,integrability,version,version,86,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:609,integrability,version,version,609,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1114,integrability,version,version,1114," tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1200,integrability,VERSION,VERSION,1200,"mediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1691,integrability,version,version,1691,"---. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1749,integrability,version,version,1749,"S 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1770,integrability,version,version,1770,"```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:422,interoperability,share,share,422,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:647,interoperability,share,share,647,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:872,interoperability,platform,platform,872,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:967,interoperability,standard,standard-,967,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1278,interoperability,platform,platform,1278,"urrent working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:86,modifiability,version,version,86,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:609,modifiability,version,version,609,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1114,modifiability,version,version,1114," tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1200,modifiability,VERSION,VERSION,1200,"mediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1691,modifiability,version,version,1691,"---. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1749,modifiability,version,version,1749,"S 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1770,modifiability,version,version,1770,"```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3684,modifiability,interm,intermediate,3684,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:990,performance,disk,disk-size,990,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:2575,performance,content,content,2575,"instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pic",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:119,safety,test,tests,119,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:728,safety,test,test,728,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1833,safety,test,test,1833,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I wan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3046,safety,test,testdata,3046,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3650,safety,test,testdata,3650,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1036,security,ssh,ssh,1036,"l Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1079,security,ssh,ssh,1079,"d OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singula",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:119,testability,test,tests,119,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:165,testability,observ,observed,165,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:728,testability,test,test,728,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1833,testability,test,test,1833,"copes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I wan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:2246,testability,unit,unittest,2246,"ERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3046,testability,test,testdata,3046,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3251,testability,unit,unittest,3251,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:3650,testability,test,testdata,3650,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir"". call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. ```. Then I ran:. ```. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir tmp/ \. --num_shards=1. ```. Now I check the current directory again:. ```. [pichuan@pichuan-centos8 ~]$ ls -1. deepvariant.sif. quickstart-output. quickstart-testdata. tmp. ```. and I see the intermediate results in the directory:. ```. [pichuan@pichuan-centos8 ~]$ ls -1 tmp/. call_variants_output.tfrecord.gz. gvcf.tfrecord-00000-of-00001.gz. make_examples.tfrecord-00000-of-00001.gz. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:499,usability,help,helpful,499,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:814,usability,USER,USER,814,"Hi @simoncchu ,. I tried to run a small Quick Start using the same Singularity and OS version as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. sing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1085,usability,USER,USER,1085,"ersion as you. . With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1613,usability,guid,guides,1613,") that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1630,usability,guid,guide,1630,"ct this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:1677,usability,Confirm,Confirmed,1677,"e a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:. ```. gcloud compute instances create ""${USER}-centos8"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --image-family ""centos-8"" \. --image-project ""centos-cloud"" \. --machine-type ""e2-standard-16"" \. --boot-disk-size ""200G"" \. --zone ""us-west1-b"". ```. ssh into the machine:. ```. gcloud compute ssh ${USER}-centos8. ```. Check OS version:. ```. [pichuan@pichuan-centos8 ~]$ cat /etc/os-release. NAME=""CentOS Linux"". VERSION=""8"". ID=""centos"". ID_LIKE=""rhel fedora"". VERSION_ID=""8"". PLATFORM_ID=""platform:el8"". PRETTY_NAME=""CentOS Linux 8"". ANSI_COLOR=""0;31"". CPE_NAME=""cpe:/o:centos:centos:8"". HOME_URL=""https://centos.org/"". BUG_REPORT_URL=""https://bugs.centos.org/"". CENTOS_MANTISBT_PROJECT=""CentOS-8"". CENTOS_MANTISBT_PROJECT_VERSION=""8"". ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:. ```. [pichuan@pichuan-centos8 ~]$ singularity --version. singularity version 3.7.0-1.el8. ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:. ```. BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \. deepvariant.sif \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \. --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \. --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \. --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \. --num_shards=1. ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```. [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:152,usability,command,commands,152,"Re-opennig this after running two docker jobs and seeing that the tmpdir files of the first run are still there while running the second run. My docker commands included `--intermediate_results_dir /tmp_dir \` pointing to a local folder. my question is: should I add a `rm tmpdir/*` after each job to clean this place? thanks. ```. docker run \. --rm \. --user ""$(id -u):$(id -u)"" \. -v ""${bamfolder}"":""/inbam"" \. -v ""${reffolder}"":""/inref"" \. -v ""${outfolder}"":""/output"" \. -v ""${tmpdir}"":""/tmp_dir"" \. google/deepvariant:latest \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=/inref/${ref}_ge500.fa \. --regions=/inref/${ref}_ge500.bed \. --reads=/inbam/${bam} \. --output_vcf=/output/${samplename}.vcf \. --output_gvcf=/output/${samplename}.g.vcf \. --num_shards=${nthr} \. --intermediate_results_dir /tmp_dir \. --logging_dir=/output/${samplename}_logs \. --dry_run=false. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:356,usability,user,user,356,"Re-opennig this after running two docker jobs and seeing that the tmpdir files of the first run are still there while running the second run. My docker commands included `--intermediate_results_dir /tmp_dir \` pointing to a local folder. my question is: should I add a `rm tmpdir/*` after each job to clean this place? thanks. ```. docker run \. --rm \. --user ""$(id -u):$(id -u)"" \. -v ""${bamfolder}"":""/inbam"" \. -v ""${reffolder}"":""/inref"" \. -v ""${outfolder}"":""/output"" \. -v ""${tmpdir}"":""/tmp_dir"" \. google/deepvariant:latest \. /opt/deepvariant/bin/run_deepvariant \. --model_type=${model_type} \. --ref=/inref/${ref}_ge500.fa \. --regions=/inref/${ref}_ge500.bed \. --reads=/inbam/${bam} \. --output_vcf=/output/${samplename}.vcf \. --output_gvcf=/output/${samplename}.g.vcf \. --num_shards=${nthr} \. --intermediate_results_dir /tmp_dir \. --logging_dir=/output/${samplename}_logs \. --dry_run=false. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:346,availability,down,downstream,346,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:294,integrability,event,eventually,294,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:14,modifiability,interm,intermediate,14,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:241,performance,perform,performed,241,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:411,safety,compl,completed,411,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:411,security,compl,completed,411,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:241,usability,perform,performed,241,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:276,usability,command,command,276,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:45,usability,command,command,45,"Thanks, in that case it would be nice if the command would end with cleaning them unless the user sets some argument to keep them. They are taking a lot of space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:93,usability,user,user,93,"Thanks, in that case it would be nice if the command would end with cleaning them unless the user sets some argument to keep them. They are taking a lot of space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:52,deployability,automat,automatic,52,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:337,deployability,contain,container,337,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:460,performance,time,time,460,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:393,safety,compl,completes,393,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:393,security,compl,completes,393,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:52,testability,automat,automatic,52,"@splaisan Understood, though what should work as an automatic cleanup in the meantime is if you remove the following two lines:. ```. -v ""${tmpdir}"":""/tmp_dir"" \. --intermediate_results_dir /tmp_dir \. ```. As that triggers a call to `intermediate_results_dir = tempfile.mkdtemp()`, which creates a temporary directory inside the Docker container that cleans itself up after `run_deepvariant` completes. Though Docker itself also creates additional files over time that require periodic cleanups.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:81,deployability,contain,container,81,"great, thanks Paul, . I added these lines because I did not want to overfill the container and was afraid it would write in the /tmp of my host which is very small. I will try this at the next run. cheers",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:234,availability,operat,operating,234,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:57,deployability,contain,containers,57,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:199,deployability,contain,container-based,199,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:432,deployability,contain,container,432,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:477,deployability,continu,continues,477,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:328,interoperability,bind,bind-mounts,328,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:312,modifiability,paramet,parameter,312,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:328,modifiability,bind,bind-mounts,328,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:551,modifiability,layer,layers,551,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:218,reliability,doe,doesn,218,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:270,safety,isol,isolated,270,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:270,security,iso,isolated,270,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:28,testability,understand,understandable,28,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:270,testability,isol,isolated,270,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:646,usability,help,help,646,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:695,usability,help,helps,695,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```. $ docker info | grep Root. Docker Root Dir: /var/lib/docker. $. ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:19,performance,time,time,19,Thank you for your time and great explanation. Makes perfect sense now. Cheers,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:62,interoperability,specif,specify,62,"Hi @splaisan ,. like earlier comments mentioned, if you don't specify intermediate_results_dir, it won't be saved separately. A few more tricks for cleaning up after Docker:. 1. https://docs.docker.com/config/pruning/ has information about how to clean up for Docker. 2. If you run Docker with `--rm` , I believe it'll also clean up after your run completes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:348,safety,compl,completes,348,"Hi @splaisan ,. like earlier comments mentioned, if you don't specify intermediate_results_dir, it won't be saved separately. A few more tricks for cleaning up after Docker:. 1. https://docs.docker.com/config/pruning/ has information about how to clean up for Docker. 2. If you run Docker with `--rm` , I believe it'll also clean up after your run completes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:348,security,compl,completes,348,"Hi @splaisan ,. like earlier comments mentioned, if you don't specify intermediate_results_dir, it won't be saved separately. A few more tricks for cleaning up after Docker:. 1. https://docs.docker.com/config/pruning/ has information about how to clean up for Docker. 2. If you run Docker with `--rm` , I believe it'll also clean up after your run completes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/296:68,usability,tip,tips,68,"I already use the -rm option in all my docker runs, thanks for your tips.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296
https://github.com/google/deepvariant/issues/297:93,usability,user,user,93,"Ah yes you are absolutely right, I was just pointing out that it might be easier for the end user if special characters were not present.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:493,integrability,sub,subsequently,493,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:150,performance,time,time,150,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:535,safety,compl,complicated,535,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:535,security,compl,complicated,535,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:252,testability,simpl,simply,252,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:168,usability,navigat,navigate,168,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:252,usability,simpl,simply,252,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:479,usability,user,user,479,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:322,security,command-lin,command-line,322,"The feature for outputting the realigned reads is mostly meant for looking deeper and troubleshooting issues in small regions, but we will keep this in mind as a feature request. . If you do want to use it more broadly for every locus in your sample, you could use `samtools merge` to merge all the bams together from the command-line right away before working with them. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:322,usability,command,command-line,322,"The feature for outputting the realigned reads is mostly meant for looking deeper and troubleshooting issues in small regions, but we will keep this in mind as a feature request. . If you do want to use it more broadly for every locus in your sample, you could use `samtools merge` to merge all the bams together from the command-line right away before working with them. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/297:383,usability,help,help,383,"The feature for outputting the realigned reads is mostly meant for looking deeper and troubleshooting issues in small regions, but we will keep this in mind as a feature request. . If you do want to use it more broadly for every locus in your sample, you could use `samtools merge` to merge all the bams together from the command-line right away before working with them. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297
https://github.com/google/deepvariant/issues/298:65,energy efficiency,current,current,65,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:85,energy efficiency,model,models,85,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:226,energy efficiency,current,current,226,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:180,modifiability,Pac,PacBio,180,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:635,modifiability,extens,extension,635,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:836,modifiability,Pac,PacificBiosciences,836,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:1029,modifiability,exten,extend,1029,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:473,reliability,doe,doesn,473,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:572,reliability,doe,does,572,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:85,security,model,models,85,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:250,security,ident,identifies,250,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:310,security,sign,signatures,310,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:439,security,sign,signatures,439,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/298:52,usability,stop,stopping,52,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298
https://github.com/google/deepvariant/issues/299:69,interoperability,share,share,69,"Hi @tonyreina . We don't have one, but if you or other users want to share an example, that will be great. I can keep this issue open for another week to see if there's any comments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:55,usability,user,users,55,"Hi @tonyreina . We don't have one, but if you or other users want to share an example, that will be great. I can keep this issue open for another week to see if there's any comments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,availability,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,deployability,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,energy efficiency,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,reliability,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,safety,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:30,testability,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/299:66,usability,close,close,66,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299
https://github.com/google/deepvariant/issues/300:284,availability,consist,consistent,284,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:67,deployability,version,version,67,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:67,integrability,version,version,67,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:67,modifiability,version,version,67,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:303,testability,understand,understanding,303,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:182,usability,indicat,indicate,182,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:284,usability,consist,consistent,284,"Hi @meghanasp21 . DeepVariant is a germline variant caller. In the version on GitHub, it is not designed to call somatic variants. Looking at your numbers in the images, it seems to indicate that DeepVariant calls about 340k germline variants on chromosome 1 on a WGS sample. This is consistent with my understanding of the number of germline variants in human. Is there any reason that you're expecting 10x less than that on average? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:203,deployability,pipelin,pipeline,203,"Thanks for clarifying. I was just curious, because when I used GATK to call germline variants, it gave me 10x less, so I just was curious about the large discrepancy. But, perhaps this is more of a GATK pipeline issue. So, the ""PASS"" variants should be the final filtered germline variants? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:203,integrability,pipelin,pipeline,203,"Thanks for clarifying. I was just curious, because when I used GATK to call germline variants, it gave me 10x less, so I just was curious about the large discrepancy. But, perhaps this is more of a GATK pipeline issue. So, the ""PASS"" variants should be the final filtered germline variants? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:263,integrability,filter,filtered,263,"Thanks for clarifying. I was just curious, because when I used GATK to call germline variants, it gave me 10x less, so I just was curious about the large discrepancy. But, perhaps this is more of a GATK pipeline issue. So, the ""PASS"" variants should be the final filtered germline variants? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:634,energy efficiency,Current,Currently,634,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:811,energy efficiency,model,model,811,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:247,integrability,FILTER,FILTER,247,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:256,integrability,filter,filter,256,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:308,integrability,filter,filters,308,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:612,integrability,filter,filter,612,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:729,security,ident,identifies,729,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:811,security,model,model,811,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:90,usability,indicat,indicate,90,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:263,usability,statu,status,263,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:401,usability,indicat,indicate,401,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:419,usability,tool,tool,419,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:802,usability,learn,learning,802,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position. You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf. ```. FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. . ```. Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:153,availability,consist,consistent,153,"Hi @meghanasp21 . Edit: My apologies, I misread the values in the Venn diagram as 3M, not 300k. 300,000 variant positions on chromosome1 actually sounds consistent with what we would expect. How are you running GATK? Are you restricting its calling to a region? To your second question, are you asking for best practices for tumor+normal calling in general (beyond DeepVariant but to other methods like Mutect or Strelka)? DeepVariant is not intended as a somatic caller and we don't have recommendations for this. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:311,reliability,pra,practices,311,"Hi @meghanasp21 . Edit: My apologies, I misread the values in the Venn diagram as 3M, not 300k. 300,000 variant positions on chromosome1 actually sounds consistent with what we would expect. How are you running GATK? Are you restricting its calling to a region? To your second question, are you asking for best practices for tumor+normal calling in general (beyond DeepVariant but to other methods like Mutect or Strelka)? DeepVariant is not intended as a somatic caller and we don't have recommendations for this. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/300:153,usability,consist,consistent,153,"Hi @meghanasp21 . Edit: My apologies, I misread the values in the Venn diagram as 3M, not 300k. 300,000 variant positions on chromosome1 actually sounds consistent with what we would expect. How are you running GATK? Are you restricting its calling to a region? To your second question, are you asking for best practices for tumor+normal calling in general (beyond DeepVariant but to other methods like Mutect or Strelka)? DeepVariant is not intended as a somatic caller and we don't have recommendations for this. Thanks,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300
https://github.com/google/deepvariant/issues/301:197,deployability,log,logging,197,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:415,deployability,log,logging,415,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:536,deployability,instal,installing,536,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:549,deployability,version,version,549,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:603,deployability,contain,contain,603,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:307,energy efficiency,CPU,CPU-only,307,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:490,energy efficiency,current,current,490,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:86,integrability,messag,message,86,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:549,integrability,version,version,549,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:86,interoperability,messag,message,86,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:549,modifiability,version,version,549,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:581,modifiability,pac,package,581,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:307,performance,CPU,CPU-only,307,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:329,reliability,doe,does,329,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:594,reliability,doe,does,594,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:197,safety,log,logging,197,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:415,safety,log,logging,415,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:197,security,log,logging,197,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:415,security,log,logging,415,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:197,testability,log,logging,197,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:415,testability,log,logging,415,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:54,usability,user,users,54,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:338,usability,support,support,338,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:372,usability,clear,clear,372,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:625,usability,close,close,625,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:66,deployability,version,version,66,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:170,deployability,version,version,170,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:66,integrability,version,version,66,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:170,integrability,version,version,170,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:66,modifiability,version,version,66,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:170,modifiability,version,version,170,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:251,usability,help,helpful,251,"Thanks @gunjanbaid for the quick response. . I'm using the recent version of deepvariant docker image 0.10.0. But still it shows up this issue. Do you have a more recent version docker image than 0.10.0? If yes, then getting that docker image will be helpful for us to get AVX2 or AVX-512 benefits. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:156,deployability,log,logging,156,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:116,integrability,messag,message,116,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:204,integrability,messag,message,204,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:116,interoperability,messag,message,116,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:204,interoperability,messag,message,204,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:156,safety,log,logging,156,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:156,security,log,logging,156,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/301:156,testability,log,logging,156,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301
https://github.com/google/deepvariant/issues/302:279,integrability,standardiz,standardization,279,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:170,interoperability,specif,specifically,170,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:279,interoperability,standard,standardization,279,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:20,usability,document,documentation,20,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:302,usability,tool,tools,302,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:443,usability,feedback,feedback,443,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:190,interoperability,specif,specifically,190,"Hi @AndrewCarroll @mlin . I guess I didn't try hard enough to find the information, and apologize for my laziness. I read the documentation and the issue. Now, I understand the ""half-calls"" specifically. It's a bit complicated, but I think this notation is a reasonable way to describe the information of the overlap. Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:215,safety,compl,complicated,215,"Hi @AndrewCarroll @mlin . I guess I didn't try hard enough to find the information, and apologize for my laziness. I read the documentation and the issue. Now, I understand the ""half-calls"" specifically. It's a bit complicated, but I think this notation is a reasonable way to describe the information of the overlap. Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:215,security,compl,complicated,215,"Hi @AndrewCarroll @mlin . I guess I didn't try hard enough to find the information, and apologize for my laziness. I read the documentation and the issue. Now, I understand the ""half-calls"" specifically. It's a bit complicated, but I think this notation is a reasonable way to describe the information of the overlap. Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:162,testability,understand,understand,162,"Hi @AndrewCarroll @mlin . I guess I didn't try hard enough to find the information, and apologize for my laziness. I read the documentation and the issue. Now, I understand the ""half-calls"" specifically. It's a bit complicated, but I think this notation is a reasonable way to describe the information of the overlap. Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/302:126,usability,document,documentation,126,"Hi @AndrewCarroll @mlin . I guess I didn't try hard enough to find the information, and apologize for my laziness. I read the documentation and the issue. Now, I understand the ""half-calls"" specifically. It's a bit complicated, but I think this notation is a reasonable way to describe the information of the overlap. Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302
https://github.com/google/deepvariant/issues/303:378,testability,coverag,coverage,378,Posting on behalf of @akolesnikov . ---. Candidates are generated when the following criterias are met:. (1) Base quality is greater than the threshold;. (2) Number of reads supporting alt allele are greater than the threshold;. (3) Ration of reads supporting alt allele to all the reads is greater than the threshold (it is 0.12 for SNPs). From the IGV image it looks like the coverage is much greater than what we can see on the picture. It is not possible to pinpoint the exact reason for candidate not generated in the second sample unless we can see the entire picture.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:174,usability,support,supporting,174,Posting on behalf of @akolesnikov . ---. Candidates are generated when the following criterias are met:. (1) Base quality is greater than the threshold;. (2) Number of reads supporting alt allele are greater than the threshold;. (3) Ration of reads supporting alt allele to all the reads is greater than the threshold (it is 0.12 for SNPs). From the IGV image it looks like the coverage is much greater than what we can see on the picture. It is not possible to pinpoint the exact reason for candidate not generated in the second sample unless we can see the entire picture.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:249,usability,support,supporting,249,Posting on behalf of @akolesnikov . ---. Candidates are generated when the following criterias are met:. (1) Base quality is greater than the threshold;. (2) Number of reads supporting alt allele are greater than the threshold;. (3) Ration of reads supporting alt allele to all the reads is greater than the threshold (it is 0.12 for SNPs). From the IGV image it looks like the coverage is much greater than what we can see on the picture. It is not possible to pinpoint the exact reason for candidate not generated in the second sample unless we can see the entire picture.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:171,security,sign,significant,171,"Hello, . thanks for the reply (and sorry for delay getting back in touch. It's indeed a question of coverage, as this sample has a lot of coverage, this somehow create a significant amount of bogus mapping in some regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:101,testability,coverag,coverage,101,"Hello, . thanks for the reply (and sorry for delay getting back in touch. It's indeed a question of coverage, as this sample has a lot of coverage, this somehow create a significant amount of bogus mapping in some regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/303:139,testability,coverag,coverage,139,"Hello, . thanks for the reply (and sorry for delay getting back in touch. It's indeed a question of coverage, as this sample has a lot of coverage, this somehow create a significant amount of bogus mapping in some regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/303
https://github.com/google/deepvariant/issues/304:144,deployability,contain,container,144,Thoughts? I've check all the mentioned paths and can find a python3.6 directory which leads me to believe that is coming from inside the docker container.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:89,deployability,version,version,89,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,deployability,log,log,154,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:282,deployability,fail,fail,282,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:303,deployability,log,logs,303,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:353,deployability,version,version,353,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:89,integrability,version,version,89,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:353,integrability,version,version,353,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:321,interoperability,share,share,321,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:340,interoperability,share,share,340,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:89,modifiability,version,version,89,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:353,modifiability,version,version,353,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:282,reliability,fail,fail,282,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,safety,log,log,154,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:303,safety,log,logs,303,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,security,log,log,154,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:303,security,log,logs,303,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,testability,log,log,154,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:303,testability,log,logs,303,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:29,usability,command,command,29,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:366,usability,command,command,366,"Hi @MorganHow ,. What is the command you used? I assume you're using the latest `0.10.0` version? The latest v0.10.0 is built with Python 3.6. . From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share? Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:577,availability,error,errors,577,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:25,deployability,version,version,25,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:448,deployability,Version,Version,448,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:628,deployability,log,log,628,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:701,deployability,log,log,701,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:25,integrability,version,version,25,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:448,integrability,Version,Version,448,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:25,modifiability,version,version,25,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:448,modifiability,Version,Version,448,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:577,performance,error,errors,577,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:78,safety,input,input,78,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:209,safety,input,input,209,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:250,safety,input,input,250,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:577,safety,error,errors,577,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:628,safety,log,log,628,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:701,safety,log,log,701,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:628,security,log,log,628,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:701,security,log,log,701,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:628,testability,log,log,628,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:701,testability,log,log,701,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:4,usability,command,command,4,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:78,usability,input,input,78,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:209,usability,input,input,209,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:250,usability,input,input,250,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:548,usability,command,command,548,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:577,usability,error,errors,577,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:605,usability,help,help,605,"The command I ran was on version 0.10.0. `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287). Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help! [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:87,deployability,updat,update,87,"I see! @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. . We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:184,interoperability,specif,specific,184,"I see! @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. . We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:87,safety,updat,update,87,"I see! @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. . We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:87,security,updat,update,87,"I see! @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. . We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,availability,error,error,177,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:136,deployability,updat,update,136,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:28,interoperability,distribut,distributed,28,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,performance,error,error,177,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:136,safety,updat,update,136,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,safety,error,error,177,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:10,security,access,access,10,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:136,security,updat,update,136,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,usability,error,error,177,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:114,availability,cluster,cluster,114,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:114,deployability,cluster,cluster,114,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:151,deployability,contain,containers,151,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:19,usability,support,support,19,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:55,usability,experien,experiencing,55,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:83,usability,statu,status,83,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:139,usability,support,support,139,@pichuan . I would support reopening this ticket: I am experiencing the same `exit status 247` problem on a Linux cluster with Singularity support for containers. 22 out of 24 jobs finished w/o any issues using `v0.10.0`.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:98,availability,error,error,98,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:124,deployability,fail,failed,124,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:148,interoperability,specif,specifically,148,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:98,performance,error,error,98,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:124,reliability,fail,failed,124,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:98,safety,error,error,98,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:81,usability,command,command,81,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:98,usability,error,error,98,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:143,usability,help,help,143,"@ptrebert Thank you for reporting. . Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:134,deployability,contain,container,134,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:40,modifiability,layer,layered,40,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:119,performance,memor,memory,119,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:25,safety,compl,complex,25,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:25,security,compl,complex,25,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:119,usability,memor,memory,119,"This is actually quite a complex, multi-layered systems problem. To keep the story short, @ptrebert try increasing the memory to your container be it Docker or Singularity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:50,performance,memor,memory,50,@pgrosu . Thanks for the info. Can I increase the memory as regular user or do I have to notify the sys admin?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:50,usability,memor,memory,50,@pgrosu . Thanks for the info. Can I increase the memory as regular user or do I have to notify the sys admin?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:68,usability,user,user,68,@pgrosu . Thanks for the info. Can I increase the memory as regular user or do I have to notify the sys admin?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:84,availability,cluster,cluster,84,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:84,deployability,cluster,cluster,84,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:58,performance,memor,memory,58,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:26,safety,compl,complicated,26,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:26,security,compl,complicated,26,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:58,usability,memor,memory,58,"Sorry, I was thinking too complicated - I'll increase the memory for the respective cluster job and run it again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:31,availability,operat,operate,31,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:20,deployability,contain,containers,20,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:86,deployability,resourc,resources,86,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:101,deployability,contain,container,101,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:86,energy efficiency,resourc,resources,86,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:79,performance,memor,memory,79,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:86,performance,resourc,resources,86,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:142,performance,memor,memory,142,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:86,safety,resourc,resources,86,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:130,security,access,access,130,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:86,testability,resourc,resources,86,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:79,usability,memor,memory,79,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:142,usability,memor,memory,142,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:63,availability,avail,available,63,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:92,availability,cluster,cluster,92,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:92,deployability,cluster,cluster,92,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:12,integrability,sub,substantial,12,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:56,performance,memor,memory,56,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:63,reliability,availab,available,63,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:63,safety,avail,available,63,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:63,security,availab,available,63,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:56,usability,memor,memory,56,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,usability,help,help,154,"@pgrosu . A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:460,deployability,resourc,resource,460,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:460,energy efficiency,resourc,resource,460,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,integrability,inject,injects,154,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:167,modifiability,layer,layers,167,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:453,performance,memor,memory,453,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:460,performance,resourc,resource,460,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:70,safety,compl,complex,70,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,safety,compl,complexity,177,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:460,safety,resourc,resource,460,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:70,security,compl,complex,70,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:154,security,inject,injects,154,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:177,security,compl,complexity,177,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:200,security,expos,exposed,200,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:460,testability,resourc,resource,460,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:320,usability,interact,interaction,320,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/304:453,usability,memor,memory,453,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304
https://github.com/google/deepvariant/issues/305:327,interoperability,share,share,327,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:171,modifiability,paramet,parameter,171,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:34,reliability,doe,does,34,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:305,reliability,doe,doesn,305,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:99,usability,command,command,99,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:337,usability,command,command,337,"Hi Weiwei. It says `make_examples does not accept positional arguments but some are present on the command line` because it is getting some arguments that don't have a -- parameter name in front of them. It looks like you need to put --regions before the locus: `--regions 3:178952054-178952106`. If that doesn't work, can you share the command you used?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:590,deployability,log,log,590,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:639,deployability,log,logs,639,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:728,deployability,log,logs,728,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:180,safety,input,input,180,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:590,safety,log,log,590,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:639,safety,log,logs,639,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:728,safety,log,logs,728,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:590,security,log,log,590,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:639,security,log,logs,639,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:728,security,log,logs,728,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:590,testability,log,log,590,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:639,testability,log,logs,639,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:728,testability,log,logs,728,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:180,usability,input,input,180,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot! [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:701,availability,error,errors,701,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:736,availability,failur,failure,736,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:123,deployability,log,logs,123,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:736,deployability,fail,failure,736,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:309,integrability,buffer,buffer,309,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:275,performance,time,time,275,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:290,performance,parallel,parallel,290,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:701,performance,error,errors,701,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:736,performance,failur,failure,736,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:736,reliability,fail,failure,736,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:759,reliability,doe,does,759,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:123,safety,log,logs,123,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:374,safety,input,input,374,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:421,safety,input,input,421,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:701,safety,error,errors,701,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:123,security,log,logs,123,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:123,testability,log,logs,123,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:260,usability,command,command,260,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:374,usability,input,input,374,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:421,usability,input,input,421,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:701,usability,error,errors,701,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:715,usability,Command,Command,715,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:824,usability,command,command,824,"Oh, sorry I couldn't see that you already had one `--regions` in there. I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in. ```. ***** Running the command:*****. time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"". ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:70,deployability,build,build,70,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:508,deployability,updat,update,508,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:394,reliability,Doe,Does,394,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:508,safety,updat,update,508,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:508,security,updat,update,508,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:166,usability,command,command,166,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:519,usability,document,documentation,519,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you? Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:148,availability,avail,available,148,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:148,reliability,availab,available,148,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:148,safety,avail,available,148,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:148,security,availab,available,148,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:184,usability,statu,status,184,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:57,availability,error,error,57,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:306,availability,avail,available,306,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:63,integrability,messag,message,63,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:23,interoperability,share,share,23,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:63,interoperability,messag,message,63,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:57,performance,error,error,57,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:306,reliability,availab,available,306,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:57,safety,error,error,57,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:306,safety,avail,available,306,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:306,security,availab,available,306,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:605,security,auth,auth,605,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:33,usability,command,command,33,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:57,usability,error,error,57,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:342,usability,statu,status,342,"Interesting. Could you share the command you ran and the error message? On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'. > works for this case, but when I added more regions, it couldn't give me an. > available output and showed an exit status 247. >. > . > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:60,availability,error,error,60,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:286,availability,avail,available,286,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:72,integrability,sub,subprocess,72,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:152,integrability,buffer,buffer,152,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:60,performance,error,error,60,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:112,performance,time,time,112,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:127,performance,parallel,parallel,127,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:286,reliability,availab,available,286,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:60,safety,error,error,60,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:286,safety,avail,available,286,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:286,security,availab,available,286,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:60,usability,error,error,60,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:103,usability,Command,Command,103,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:248,usability,statu,status,248,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1. For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:25,availability,error,error,25,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:140,interoperability,share,share,140,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:25,performance,error,error,25,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:25,safety,error,error,25,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:31,testability,trace,trace,31,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:25,usability,error,error,25,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:119,usability,statu,status,119,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:187,reliability,doe,does,187,"I'm going to close this since given our follow-up over email, the original issue with --regions is no longer the problem. For anyone else reading this, --regions with more than 2 regions does work, and putting extra quotes exactly like this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""` is the way to ensure that the quotes encasing your space-separated regions don't disappear between your docker run command and actually getting that argument passed into make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:13,usability,close,close,13,"I'm going to close this since given our follow-up over email, the original issue with --regions is no longer the problem. For anyone else reading this, --regions with more than 2 regions does work, and putting extra quotes exactly like this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""` is the way to ensure that the quotes encasing your space-separated regions don't disappear between your docker run command and actually getting that argument passed into make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/305:417,usability,command,command,417,"I'm going to close this since given our follow-up over email, the original issue with --regions is no longer the problem. For anyone else reading this, --regions with more than 2 regions does work, and putting extra quotes exactly like this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""` is the way to ensure that the quotes encasing your space-separated regions don't disappear between your docker run command and actually getting that argument passed into make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305
https://github.com/google/deepvariant/issues/306:329,deployability,observ,observation,329,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:493,deployability,releas,release,493,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:501,deployability,version,version,501,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:349,energy efficiency,current,currently,349,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:501,integrability,version,version,501,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:501,modifiability,version,version,501,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:329,testability,observ,observation,329,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:255,usability,support,supports,255,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/306:56,testability,plan,plan,56,"Thanks for the reply, and it's great to hear about your plan on improving the pileup images of these insertions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306
https://github.com/google/deepvariant/issues/307:13,reliability,Doe,Does,13,Hi @Roj4ck . Does the $REF you provide also have a $REF.fai index file with the same prefix? This is required by DeepVariant. See https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1040,availability,error,errors,1040,"s only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1192,availability,error,errors,1192,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:145,deployability,version,version,145,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:175,deployability,updat,updated,175,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:221,deployability,updat,updated,221,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1072,deployability,Fail,Failed,1072,"long with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1143,deployability,fail,failed,1143,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1217,deployability,fail,fails,1217,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1331,deployability,modul,module,1331,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:115,integrability,discover,discovered,115,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:145,integrability,version,version,145,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1158,integrability,event,eventually,1158,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1627,integrability,sub,subprocess,1627,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1720,integrability,sub,subprocess,1720,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1801,integrability,sub,subprocess,1801,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1882,integrability,buffer,buffer,1882,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:115,interoperability,discover,discovered,115,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:145,modifiability,version,version,145,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:321,modifiability,variab,variables,321,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1331,modifiability,modul,module,1331,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1391,modifiability,pac,packages,1391,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1491,modifiability,pac,packages,1491,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1040,performance,error,errors,1040,"s only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1124,performance,parallel,parallel,1124,"was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1192,performance,error,errors,1192,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1841,performance,time,time,1841,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1857,performance,parallel,parallel,1857,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1072,reliability,Fail,Failed,1072,"long with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1143,reliability,fail,failed,1143,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1217,reliability,fail,fails,1217,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:175,safety,updat,updated,175,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:221,safety,updat,updated,221,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:423,safety,input,input,423,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:709,safety,input,input,709,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:875,safety,input,input,875,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:902,safety,input,input,902,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1040,safety,error,errors,1040,"s only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1192,safety,error,errors,1192,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1331,safety,modul,module,1331,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1948,safety,input,input,1948,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1973,safety,input,input,1973,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:175,security,updat,updated,175,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:221,security,updat,updated,221,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1233,testability,Trace,Traceback,1233,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:115,usability,discov,discovered,115,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:264,usability,command,command,264,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:423,usability,input,input,423,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:638,usability,command,command,638,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:709,usability,input,input,709,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:875,usability,input,input,875,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:902,usability,input,input,902,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1040,usability,error,errors,1040,"s only given the hg19.fa files along with the cram files. . Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1192,usability,error,errors,1192,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1649,usability,command,command,1649,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1832,usability,Command,Command,1832,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1948,usability,input,input,1948,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:1973,usability,input,input,1973,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:2134,usability,statu,status,2134,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". OUTPUT_GVCF=""2009617.bam.g.vcf.gz"". Here is the command to execute deepvariant:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). and here is the new errors I'm seeing:. ValueError: Failed precondition: Cannot query without an index. parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples. --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1. 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:4,availability,error,error,4,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:10,integrability,messag,message,10,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:10,interoperability,messag,message,10,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:4,performance,error,error,4,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:4,safety,error,error,4,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:4,usability,error,error,4,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:186,usability,document,documentation,186,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html. Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:273,performance,time,time,273,You were completely correct after some working through some index issues with cram( which for future reference for anyone dealing with the same issues was done here https://github.com/samtools/htslib/issues/1069 ) all seems to be running smoothly. Thank you again for your time!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:9,safety,compl,completely,9,You were completely correct after some working through some index issues with cram( which for future reference for anyone dealing with the same issues was done here https://github.com/samtools/htslib/issues/1069 ) all seems to be running smoothly. Thank you again for your time!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/307:9,security,compl,completely,9,You were completely correct after some working through some index issues with cram( which for future reference for anyone dealing with the same issues was done here https://github.com/samtools/htslib/issues/1069 ) all seems to be running smoothly. Thank you again for your time!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307
https://github.com/google/deepvariant/issues/308:430,availability,avail,available,430,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:112,energy efficiency,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:527,energy efficiency,frequenc,frequency,527,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:112,integrability,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:696,integrability,transform,transforming,696,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:112,interoperability,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:696,interoperability,transform,transforming,696,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:112,modifiability,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:430,reliability,availab,available,430,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:430,safety,avail,available,430,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:430,security,availab,available,430,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:500,testability,coverag,coverage,500,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:868,usability,guid,guide,868,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:. 1. No, this is a limit within Inception V3 that DeepVariant uses. 2. We don't have any other training tutorials for other systems. 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research. If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:276,availability,error,error,276,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,availability,error,error,989,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:52,deployability,log,logs,52,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:141,deployability,log,logs,141,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:356,deployability,log,log,356,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:813,deployability,log,log,813,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1050,deployability,log,log,1050,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:191,energy efficiency,Cloud,Cloud,191,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:466,energy efficiency,gpu,gpu,466,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:843,energy efficiency,GPU,GPUs,843,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:930,energy efficiency,gpu,gpu,930,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1131,energy efficiency,GPU,GPU,1131,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:276,performance,error,error,276,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:285,performance,time,time,285,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:317,performance,parallel,parallel,317,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:466,performance,gpu,gpu,466,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:843,performance,GPU,GPUs,843,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:930,performance,gpu,gpu,930,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,performance,error,error,989,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1131,performance,GPU,GPU,1131,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:52,safety,log,logs,52,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:141,safety,log,logs,141,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:276,safety,error,error,276,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:356,safety,log,log,356,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:813,safety,log,log,813,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,safety,error,error,989,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1050,safety,log,log,1050,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:52,security,log,logs,52,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:141,security,log,logs,141,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:356,security,log,log,356,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:813,security,log,log,813,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1050,security,log,log,1050,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:52,testability,log,logs,52,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:141,testability,log,logs,141,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:356,testability,log,log,356,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:813,testability,log,log,813,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1050,testability,log,log,1050,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:33,usability,help,help,33,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:255,usability,command,command,255,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:276,usability,error,error,276,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,usability,error,error,989,"Hi Maria,. Thanks a lot for your help! [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error. ( time seq 0 $((N_SHARDS-1)) | \. parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \. sudo nvidia-docker run \. -v ${HOME}:${HOME} \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/make_examples \. --mode training \. --ref ""${REF}"" \. --reads ""${BAM_CHR1}"" \. --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \. --truth_variants ""${TRUTH_VCF}"" \. --confident_regions ""${TRUTH_BED}"" \. --task {} \. --regions ""'chr1'"" \. ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1. Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'. The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot! Best regards,. Weiwei.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:284,availability,error,error,284,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,availability,slo,slow,989,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:566,energy efficiency,current,current,566,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:971,energy efficiency,CPU,CPUs,971,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1124,energy efficiency,CPU,CPUs,1124,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:477,interoperability,specif,specifically,477,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:284,performance,error,error,284,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:971,performance,CPU,CPUs,971,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1124,performance,CPU,CPUs,1124,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:989,reliability,slo,slow,989,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:126,safety,input,input,126,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:284,safety,error,error,284,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:635,security,team,team,635,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1228,security,team,team,1228,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:809,testability,plan,plan,809,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:126,usability,input,input,126,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:284,usability,error,error,284,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:407,usability,confirm,confirm,407,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:675,usability,support,support,675,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:761,usability,tool,tool,761,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:940,usability,experien,experience,940,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1160,usability,close,close,1160,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:1256,usability,support,support,1256,"Hi @WeiweiBian ,. first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --. To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:194,deployability,version,version,194,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:182,energy efficiency,CPU,CPU,182,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:190,energy efficiency,GPU,GPU,190,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:194,integrability,version,version,194,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:194,modifiability,version,version,194,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:182,performance,CPU,CPU,182,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/308:190,performance,GPU,GPU,190,"Hi @WeiweiBian ,. I re-ran the steps of make_examples on : https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. Our instructions work (with both CPU and GPU version) for make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308
https://github.com/google/deepvariant/issues/309:7,availability,error,error,7,Second error (num_shards) has been resolved,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:7,performance,error,error,7,Second error (num_shards) has been resolved,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:7,safety,error,error,7,Second error (num_shards) has been resolved,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:7,usability,error,error,7,Second error (num_shards) has been resolved,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:41,reliability,doe,doesn,41,Case solved! Looks like that deepvariant doesn't like symbolic links for the input files!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:77,safety,input,input,77,Case solved! Looks like that deepvariant doesn't like symbolic links for the input files!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/309:77,usability,input,input,77,Case solved! Looks like that deepvariant doesn't like symbolic links for the input files!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309
https://github.com/google/deepvariant/issues/310:4,availability,error,error,4,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:10,integrability,messag,messages,10,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:10,interoperability,messag,messages,10,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:106,modifiability,variab,variables,106,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:4,performance,error,error,4,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:4,safety,error,error,4,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:132,testability,verif,verifying,132,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:4,usability,error,error,4,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:319,usability,close,close,319,"Thank you @Roj4ck ! @colsen , what @Roj4ck said is exactly right. :) . In your case, you should make sure there is a index file for your hg19.fasta file, under your $INPUT_DIR directory. The name is usually hg19.fasta.fai. If you really don't have a fai file, you can run `samtools faidx hg19.fasta` to create it. I'll close this issue. Feel free to open new ones if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:67,availability,avail,available,67,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:67,reliability,availab,available,67,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:67,safety,avail,available,67,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:67,security,availab,available,67,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:161,usability,command,command,161,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:268,safety,input,input,268,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:361,safety,input,input,361,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:24,usability,confirm,confirming,24,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:89,usability,clear,clear,89,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:177,usability,Confirm,Confirm,177,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:227,usability,command,command,227,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:268,usability,input,input,268,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:361,usability,input,input,361,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:389,usability,confirm,confirm,389,"Hi @colsen . thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:. Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:195,modifiability,variab,variables,195,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:221,modifiability,variab,variables,221,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:932,modifiability,variab,variables,932,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:1033,modifiability,variab,variables,1033,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:332,safety,input,input,332,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:577,safety,input,input,577,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:743,safety,input,input,743,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:770,safety,input,input,770,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:2,usability,person,personally,2,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:34,usability,command,command,34,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:332,usability,input,input,332,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:577,usability,input,input,577,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:743,usability,input,input,743,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:770,usability,input,input,770,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:903,usability,help,helps,903,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:. BIN_VERSION=""0.10.0"". BASE=""${HOME}/deepvariant-run"". INPUT_DIR=""${BASE}/input"". REF=""hg19.fa"". BAM=""2009617s.cram"". OUTPUT_DIR=""${BASE}/output"". DATA_DIR=""${INPUT_DIR}/data"". OUTPUT_VCF=""2009617s.vcf.gz"". OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WES \. --ref=""/input/${REF}"" \. --reads=""/input/${BAM}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:90,safety,input,input,90,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:109,safety,input,input,109,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:115,safety,input,input,115,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:21,usability,command,command,21,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:90,usability,input,input,90,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:109,usability,input,input,109,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:115,usability,input,input,115,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/310:173,usability,help,help,173,@pichuan with the ls command inside the docker I could figure out the problem. Replacing /input/${REF} with /input/input/${REF} in --ref solved the problem. Thanks for your help!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310
https://github.com/google/deepvariant/issues/311:987,availability,consist,consistency,987,"Just realized my code was pointing towards the .vcf.gz files and not to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:25",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1698,availability,consist,consistency,1698,"s. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2636,availability,consist,consistency,2636,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1061,deployability,contain,contained,1061,"ot to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1772,deployability,contain,contained,1772,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2711,deployability,contain,contained,2711,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:226,security,sign,significantly,226,"Just realized my code was pointing towards the .vcf.gz files and not to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:25",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:414,usability,user,username,414,"Just realized my code was pointing towards the .vcf.gz files and not to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:25",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:987,usability,consist,consistency,987,"Just realized my code was pointing towards the .vcf.gz files and not to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:25",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:999,usability,statu,status,999,"st realized my code was pointing towards the .vcf.gz files and not to .g.vcf.gz files. After making that changes my indeterminate consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:2547",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1125,usability,user,username,1125,"te consisentcy dropped to between 5-7% but my violations of Mendelian constraints have increased significantly as well. Are the values listed here fairly normal or is there perhaps something else you would recommend. LIsted below are the results from my latest merge:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 12 non-pass records were skipped. Concordance 2115432: F:261458/309172 (84.57%) M:267986/314085 (85.32%) F+M:219733/301071 (72.98%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1698,usability,consist,consistency,1698,"s. Check for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1710,usability,statu,status,1710,"for incorrect pedigree or sample mislabelling. 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1836,usability,user,username,1836,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2636,usability,consist,consistency,2636,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2648,usability,statu,status,2648,". 7460/338863 (2.20%) records did not conform to expected call ploidy. 323711/338863 (95.53%) records were variant in at least 1 family member and checked for Mendelian constraints. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 86448/323711 (26.71%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 18 non-pass records were skipped. Concordance 2009617: F:244269/268058 (91.13%) M:251439/272790 (92.17%) F+M:219530/260889 (84.15%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 5804/295693 (1.96%) records did not conform to expected call ploidy. 281378/295693 (95.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 16563/281378 (5.89%) records had indeterminate consistency status due to incomplete calls. 45031/281378 (16.00%) records contained a violation of Mendelian constraints. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 24 non-pass records were skipped. Concordance 2009617: F:254730/275763 (92.37%) M:261777/280640 (93.28%) F+M:228534/267907 (85.30%). Sample 2009617 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. Concordance 2115432: F:275692/314566 (87.64%) M:282207/319606 (88.30%) F+M:227816/305838 (74.49%). Sample 2115432 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling. 8113/361421 (2.24%) records did not conform to expected call ploidy. 358380/361421 (99.16%) records were variant in at least 1 family member and checked for Mendelian constraints. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 103177/358380 (28.79%) records contained a violation of Mendelian constraints.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:568,integrability,pub,public,568,"Hi @Roj4ck , . can you provide more detail on what kind of data are you applying DeepVariant on? For example:. What type of sequencing data, what depths, what organism, anything noticeably different from the data we used in case studies, etc. And, my guess is that the ""incomplete calls"" are referring to calls that DeepVariant as `./.`. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are `./.`)? This will help us get a better idea to help answer your question. And, if your data is public, you can point to us and we can take a look. Adding @tedyun for your thoughts (Ted is the first author of our [cohort manuscript](https://doi.org/10.1101/2020.02.10.942086)). Adding @akolesnikov who's on rotation this week as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:671,security,auth,author,671,"Hi @Roj4ck , . can you provide more detail on what kind of data are you applying DeepVariant on? For example:. What type of sequencing data, what depths, what organism, anything noticeably different from the data we used in case studies, etc. And, my guess is that the ""incomplete calls"" are referring to calls that DeepVariant as `./.`. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are `./.`)? This will help us get a better idea to help answer your question. And, if your data is public, you can point to us and we can take a look. Adding @tedyun for your thoughts (Ted is the first author of our [cohort manuscript](https://doi.org/10.1101/2020.02.10.942086)). Adding @akolesnikov who's on rotation this week as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:779,security,rotat,rotation,779,"Hi @Roj4ck , . can you provide more detail on what kind of data are you applying DeepVariant on? For example:. What type of sequencing data, what depths, what organism, anything noticeably different from the data we used in case studies, etc. And, my guess is that the ""incomplete calls"" are referring to calls that DeepVariant as `./.`. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are `./.`)? This will help us get a better idea to help answer your question. And, if your data is public, you can point to us and we can take a look. Adding @tedyun for your thoughts (Ted is the first author of our [cohort manuscript](https://doi.org/10.1101/2020.02.10.942086)). Adding @akolesnikov who's on rotation this week as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:491,usability,help,help,491,"Hi @Roj4ck , . can you provide more detail on what kind of data are you applying DeepVariant on? For example:. What type of sequencing data, what depths, what organism, anything noticeably different from the data we used in case studies, etc. And, my guess is that the ""incomplete calls"" are referring to calls that DeepVariant as `./.`. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are `./.`)? This will help us get a better idea to help answer your question. And, if your data is public, you can point to us and we can take a look. Adding @tedyun for your thoughts (Ted is the first author of our [cohort manuscript](https://doi.org/10.1101/2020.02.10.942086)). Adding @akolesnikov who's on rotation this week as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:520,usability,help,help,520,"Hi @Roj4ck , . can you provide more detail on what kind of data are you applying DeepVariant on? For example:. What type of sequencing data, what depths, what organism, anything noticeably different from the data we used in case studies, etc. And, my guess is that the ""incomplete calls"" are referring to calls that DeepVariant as `./.`. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are `./.`)? This will help us get a better idea to help answer your question. And, if your data is public, you can point to us and we can take a look. Adding @tedyun for your thoughts (Ted is the first author of our [cohort manuscript](https://doi.org/10.1101/2020.02.10.942086)). Adding @akolesnikov who's on rotation this week as an FYI.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1042,availability,avail,available,1042,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:972,integrability,pub,public,972,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1052,integrability,pub,publicly,1052,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1133,integrability,sub,submitted,1133,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1147,integrability,pub,public,1147,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:353,interoperability,format,format,353,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1088,interoperability,share,share,1088,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:191,performance,perform,performed,191,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:728,performance,time,time,728,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:272,reliability,Diagno,Diagnostic,272,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1042,reliability,availab,available,1042,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:170,safety,test,testing,170,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:205,safety,test,tests,205,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:283,safety,Test,Testing,283,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:884,safety,compl,completely,884,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1042,safety,avail,available,1042,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:884,security,compl,completely,884,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1042,security,availab,available,1042,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1115,security,team,team,1115,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:170,testability,test,testing,170,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:205,testability,test,tests,205,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:272,testability,Diagno,Diagnostic,272,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:283,testability,Test,Testing,283,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:467,testability,coverag,coverage,467,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:1242,testability,plan,plan,1242,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:191,usability,perform,performed,191,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:231,usability,tool,tooling,231,"Thank you for the quick response! I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.). What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) . what depths - 152x mean depth of coverage with a quality threshold of 98.6. what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents). Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:271,availability,consist,consistency,271,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:451,availability,consist,consistency,451,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:632,deployability,instal,installer,632,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:642,deployability,resourc,resources,642,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:22,energy efficiency,cloud,cloud,22,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:642,energy efficiency,resourc,resources,642,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:222,interoperability,specif,specific,222,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:642,performance,resourc,resources,642,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:642,safety,resourc,resources,642,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:642,testability,resourc,resources,642,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:31,usability,help,helpful,31,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:271,usability,consist,consistency,271,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:283,usability,statu,status,283,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:451,usability,consist,consistency,451,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:545,usability,Tool,Tools,545,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:560,usability,document,documentation,560,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:619,usability,tool,tools,619,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:652,usability,tool,tools,652,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:283,usability,command,command,283,The amount of values in each cohort are in my earlier post with more details but in summary. (3 in the first (2 parents 1 affected proband) 3 in the second (2 parents and the other affected proband) and 4 in the third (both parents and both affected probands). I'll try running that command and see what type of or results I get and report back ASAP. Thank you both for your assistance.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:294,availability,consist,consistency,294,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:525,availability,consist,consistency,525,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:766,availability,consist,consistency,766,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2,safety,review,reviewed,2,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:102,security,ident,identified,102,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:2,testability,review,reviewed,2,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:54,testability,simpl,simple,54,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:54,usability,simpl,simple,54,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:152,usability,user,username,152,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:294,usability,consist,consistency,294,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:306,usability,statu,status,306,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:383,usability,user,username,383,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:525,usability,consist,consistency,525,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:537,usability,statu,status,537,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:614,usability,user,username,614,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:766,usability,consist,consistency,766,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:778,usability,statu,status,778,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2115432]. 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls. 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz. Family: [2114337 + 2114302] -> [2009617, 2115432]. 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls. 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:156,performance,time,time,156,I'm going to try sifting through this to see if there is anything in here. If I run into any other issues I'll let your teams know. Thank you both for your time (and if you have any recommendations for sifting deep variant called vcfs I'll glady take them :-).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/311:120,security,team,teams,120,I'm going to try sifting through this to see if there is anything in here. If I run into any other issues I'll let your teams know. Thank you both for your time (and if you have any recommendations for sifting deep variant called vcfs I'll glady take them :-).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311
https://github.com/google/deepvariant/issues/312:53,integrability,batch,batch,53,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:53,performance,batch,batch,53,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:445,performance,tune,tune,445,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:151,safety,avoid,avoid,151,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:485,safety,valid,validation,485,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:485,security,validat,validation,485,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:109,usability,help,helps,109,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:216,usability,help,help,216,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)). 2. Shuffling is needed for tune and train data, and not needed for validation data. 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. . 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:135,safety,valid,validation,135,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:135,security,validat,validation,135,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:51,usability,clear,clearer,51,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:254,deployability,releas,release,254,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:533,deployability,log,logic,533,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:262,energy efficiency,model,models,262,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:562,energy efficiency,current,currently,562,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:717,energy efficiency,model,modeling,717,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1578,energy efficiency,model,model,1578,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:2206,energy efficiency,model,model,2206,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1034,integrability,event,eventually,1034,"your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:2104,interoperability,share,share,2104,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:247,modifiability,Pac,PacBio,247,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1468,modifiability,Pac,PacBio,1468,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1834,modifiability,evolv,evolve,1834,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1009,performance,time,times,1009,",. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1846,performance,time,time,1846,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:533,safety,log,logic,533,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:262,security,model,models,262,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:533,security,log,logic,533,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:717,security,model,modeling,717,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1578,security,model,model,1578,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:2206,security,model,model,2206,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:533,testability,log,logic,533,"Hi @mpinese ,. I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1447,usability,experien,experience,1447,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1779,usability,intuit,intuition,1779,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1892,usability,document,documented,1892,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:2289,usability,close,close,2289,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:. For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1178,availability,checkpoint,checkpoint,1178,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1528,deployability,updat,update,1528,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1552,deployability,releas,release,1552,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1172,energy efficiency,model,model,1172,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1915,energy efficiency,model,model,1915,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:430,interoperability,specif,specify,430,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1178,reliability,checkpoint,checkpoint,1178,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1816,reliability,pra,practice,1816,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:137,safety,valid,validation,137,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:265,safety,valid,validation,265,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:485,safety,safe,safer,485,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1056,safety,Valid,Validation,1056,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1413,safety,valid,validation,1413,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1469,safety,accid,accidentally,1469,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1528,safety,updat,update,1528,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1653,safety,valid,validation,1653,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1685,safety,test,test,1685,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1843,safety,test,test,1843,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1984,safety,valid,validation,1984,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:137,security,validat,validation,137,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:265,security,validat,validation,265,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1056,security,Validat,Validation,1056,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1172,security,model,model,1172,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1413,security,validat,validation,1413,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1528,security,updat,update,1528,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1653,security,validat,validation,1653,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1915,security,model,model,1915,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1984,security,validat,validation,1984,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1685,testability,test,test,1685,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1843,testability,test,test,1843,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:53,usability,clear,clearer,53,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:936,usability,learn,learns,936,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1021,usability,learn,learning,1021,"kes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first plac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1132,usability,learn,learns,1132,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1262,usability,learn,learning,1262,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1749,usability,learn,learning,1749,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/312:1797,usability,learn,learning,1797,"s the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset? And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:. Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , . our **training** set are the labeled examples that our classifier actually learns from. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. . This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set. When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release. Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set. When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312
https://github.com/google/deepvariant/issues/313:62,deployability,version,version,62,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:481,energy efficiency,model,model-case-study,481,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:62,integrability,version,version,62,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:520,interoperability,specif,specify,520,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:62,modifiability,version,version,62,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:187,modifiability,PAC,PACBIO,187,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:474,modifiability,pac,pacbio-model-case-study,474,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:736,modifiability,Pac,PacBio,736,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:481,security,model,model-case-study,481,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:573,security,sign,significantly,573,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:54,usability,command,command,54,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:145,usability,command,command,145,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:369,usability,command,command,369,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/313:394,usability,document,documented,394,"Hi @cjain7 . First of all, we recommend using the one command version to run:. https://github.com/google/deepvariant#how-to-run. If you use this command, you'll want to use `--model_type=PACBIO` for HiFi data. In your example, you're running make_examples directly. In that case, you need to add `--norealign_reads --vsc_min_fraction_indels 0.12` to your make_examples command. These flags are documented in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-pacbio-model-case-study.md#running . Once you specify `--norealign_reads`, your run should also be significantly faster. I'm closing this issue now. If you're still encountering any issues, please feel free to report again. (FYI @williamrowell because this is a PacBio related question)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/313
https://github.com/google/deepvariant/issues/314:37,deployability,contain,container,37,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:86,deployability,version,version,86,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:103,deployability,build,build,103,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:118,deployability,contain,container,118,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:86,integrability,version,version,86,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:86,modifiability,version,version,86,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:5,usability,command,command,5,"This command was excuted in a docker container. Host:. macOS Catalina 10.15.5. Docker version 19.03.8, build afacb8b. container. ubuntu:18.04. conda 4.8.3. conda env channnel: bioconda, conda-forge. deepvariant:0.10.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:155,availability,avail,available,155,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:94,deployability,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:293,deployability,contain,container,293,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:364,deployability,contain,container,364,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:94,integrability,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:44,modifiability,pac,package,44,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:94,modifiability,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:155,reliability,availab,available,155,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:94,safety,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:155,safety,avail,available,155,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:155,security,availab,available,155,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:94,testability,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:434,usability,help,helps,434,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:76,availability,echo,echo,76,@chapmanb . Thank you for your reply! I checked the container PATH. ```. ~$ echo $PATH. /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin. ```. and I tried to find out if unzip exists in the bin directory. ```. ~$ ls -l /opt/conda/envs/deepvariant/bin/. . . . -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*. -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*. -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*. lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*. -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*. lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*. -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*. -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*. lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*. . . . ```. The unzip itself doesn't seem to be installed during `conda install` . Is this a problem that only happens to me? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:52,deployability,contain,container,52,@chapmanb . Thank you for your reply! I checked the container PATH. ```. ~$ echo $PATH. /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin. ```. and I tried to find out if unzip exists in the bin directory. ```. ~$ ls -l /opt/conda/envs/deepvariant/bin/. . . . -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*. -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*. -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*. lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*. -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*. lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*. -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*. -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*. lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*. . . . ```. The unzip itself doesn't seem to be installed during `conda install` . Is this a problem that only happens to me? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:946,deployability,instal,installed,946,@chapmanb . Thank you for your reply! I checked the container PATH. ```. ~$ echo $PATH. /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin. ```. and I tried to find out if unzip exists in the bin directory. ```. ~$ ls -l /opt/conda/envs/deepvariant/bin/. . . . -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*. -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*. -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*. lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*. -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*. lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*. -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*. -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*. lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*. . . . ```. The unzip itself doesn't seem to be installed during `conda install` . Is this a problem that only happens to me? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:970,deployability,instal,install,970,@chapmanb . Thank you for your reply! I checked the container PATH. ```. ~$ echo $PATH. /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin. ```. and I tried to find out if unzip exists in the bin directory. ```. ~$ ls -l /opt/conda/envs/deepvariant/bin/. . . . -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*. -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*. -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*. lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*. -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*. lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*. -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*. -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*. lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*. . . . ```. The unzip itself doesn't seem to be installed during `conda install` . Is this a problem that only happens to me? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:927,reliability,doe,doesn,927,@chapmanb . Thank you for your reply! I checked the container PATH. ```. ~$ echo $PATH. /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin. ```. and I tried to find out if unzip exists in the bin directory. ```. ~$ ls -l /opt/conda/envs/deepvariant/bin/. . . . -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*. -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*. -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*. lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*. -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*. lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*. lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*. -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*. -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*. lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*. . . . ```. The unzip itself doesn't seem to be installed during `conda install` . Is this a problem that only happens to me? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:189,deployability,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:478,deployability,instal,install,478,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:511,deployability,build,building,511,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:531,deployability,contain,container,531,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:607,deployability,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:189,integrability,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:607,integrability,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:189,modifiability,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:470,modifiability,pac,package,470,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:607,modifiability,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:124,reliability,doe,doesn,124,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:436,reliability,doe,does,436,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:189,safety,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:405,safety,avoid,avoid,405,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:541,safety,avoid,avoid,541,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:607,safety,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:189,testability,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:607,testability,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:57,usability,help,helpful,57,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:700,usability,help,helps,700,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:176,deployability,instal,installing,176,"@chapmanb . Thank you for your reply. . I have seen the conda recipi. It is indeed strange. Maybe there's something wrong with my running environment. OK, I'll deal with it by installing `unzip` directly in `apt` or `conda`. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/314:208,security,apt,apt,208,"@chapmanb . Thank you for your reply. . I have seen the conda recipi. It is indeed strange. Maybe there's something wrong with my running environment. OK, I'll deal with it by installing `unzip` directly in `apt` or `conda`. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314
https://github.com/google/deepvariant/issues/315:96,deployability,log,logs,96,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:139,deployability,log,logtostderr,139,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:161,deployability,log,logs,161,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:581,deployability,log,logtostderr,581,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:96,safety,log,logs,96,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:139,safety,log,logtostderr,139,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:161,safety,log,logs,161,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:326,safety,input,input,326,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:364,safety,input,input,364,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:581,safety,log,logtostderr,581,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:96,security,log,logs,96,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:139,security,log,logtostderr,139,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:161,security,log,logs,161,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:581,security,log,logtostderr,581,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:96,testability,log,logs,96,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:139,testability,log,logtostderr,139,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:161,testability,log,logs,161,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:581,testability,log,logtostderr,581,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:118,usability,command,command,118,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:253,usability,command,command,253,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:326,usability,input,input,326,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:364,usability,input,input,364,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command. ``` . /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:68,availability,error,errors,68,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:793,deployability,log,logtostderr,793,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:196,integrability,messag,message,196,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:196,interoperability,messag,message,196,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:68,performance,error,errors,68,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:68,safety,error,errors,68,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:75,safety,except,except,75,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:361,safety,input,input,361,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:523,safety,input,input,523,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:564,safety,input,input,564,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:793,safety,log,logtostderr,793,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:793,security,log,logtostderr,793,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:793,testability,log,logtostderr,793,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:16,usability,command,command,16,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:68,usability,error,errors,68,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:361,usability,input,input,361,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:523,usability,input,input,523,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:564,usability,input,input,564,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ilAriAges1.fasta.bgz"" \. --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \. --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \. --norealign_reads \. --vsc_min_fraction_indels ""0.12"" \. --task 54 --logtostderr. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:218,performance,parallel,parallel,218,"hello @akolesnikov,. I was wondering if you might have some additional guidance towards running deepvariant. I have started running deepvariant successfully in another server, but I would like to be run the process in parallel in multiple servers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:71,usability,guidanc,guidance,71,"hello @akolesnikov,. I was wondering if you might have some additional guidance towards running deepvariant. I have started running deepvariant successfully in another server, but I would like to be run the process in parallel in multiple servers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/315:84,interoperability,distribut,distributed,84,"Hello sjin09,. Unfortunately we don't have a tutorial for how to run DeepVariant in distributed environment. Although, there are multiple external solutions that provide good solutions. Please see https://github.com/google/deepvariant#external-solutions for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315
https://github.com/google/deepvariant/issues/316:41,availability,avail,available,41,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/316:41,reliability,availab,available,41,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/316:41,safety,avail,available,41,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/316:41,security,availab,available,41,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/316:181,usability,support,support,181,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/316:43,testability,understand,understand,43,@gunjanbaid Thanks for sharing the link! I understand well.Thank you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316
https://github.com/google/deepvariant/issues/317:249,performance,perform,performs,249,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:799,performance,perform,performed,799,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:1038,reliability,Doe,Does,1038,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:222,testability,context,context,222,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:537,testability,simpl,simple,537,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:183,usability,Learn,Learning,183,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:249,usability,perform,performs,249,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:318,usability,learn,learns,318,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:537,usability,simpl,simple,537,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:634,usability,effectiv,effectively,634,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:685,usability,learn,learn,685,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:799,usability,perform,performed,799,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB. Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:493,availability,error,errors,493,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:584,availability,error,errors,584,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:309,deployability,observ,observed,309,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:827,deployability,stage,stage,827,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:493,performance,error,errors,493,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:584,performance,error,errors,584,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:493,safety,error,errors,493,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:584,safety,error,errors,584,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:663,safety,detect,detected,663,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:730,safety,input,inputting,730,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:663,security,detect,detected,663,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:220,testability,unit,units,220,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:309,testability,observ,observed,309,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:493,usability,error,errors,493,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:584,usability,error,errors,584,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:722,usability,help,help,722,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:730,usability,input,inputting,730,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously. However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not. Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors. I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions? On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:278,availability,error,errors,278,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:782,availability,error,errors,782,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:917,availability,error,errors,917,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:1023,availability,error,errors,1023,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:60,deployability,version,version,60,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:60,integrability,version,version,60,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:510,integrability,contract,contraction,510,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:510,interoperability,contract,contraction,510,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:60,modifiability,version,version,60,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:278,performance,error,errors,278,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:588,performance,content,content,588,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:782,performance,error,errors,782,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:917,performance,error,errors,917,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:1023,performance,error,errors,1023,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:278,safety,error,errors,278,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:782,safety,error,errors,782,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:917,safety,error,errors,917,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:1023,safety,error,errors,1023,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:141,testability,trace,trace,141,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:278,usability,error,errors,278,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:782,usability,error,errors,782,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:917,usability,error,errors,917,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/317:1023,usability,error,errors,1023,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317
https://github.com/google/deepvariant/issues/318:701,availability,down,downstream,701,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:52,deployability,contain,contain,52,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:133,integrability,FILTER,FILTER,133,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:515,integrability,topic,topic,515,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:361,interoperability,specif,specific,361,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:472,interoperability,format,format,472,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:47,reliability,doe,does,47,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:229,security,sign,signal,229,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:488,usability,document,documentation,488,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:593,usability,support,support,593,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:712,usability,tool,tools,712,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1147,deployability,observ,observed,1147,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:613,integrability,filter,filter,613,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:768,integrability,FILTER,FILTER,768,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:240,interoperability,coordinat,coordinates,240,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:784,interoperability,FORMAT,FORMAT,784,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:296,reliability,doe,does,296,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1147,testability,observ,observed,1147,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:73,usability,support,support,73,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:162,usability,clear,clear,162,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1210,usability,support,supporting,1210,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1378,usability,support,supporting,1378,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. 2) . #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31. -- | -- | -- | -- | -- | -- | -- | -- | -- | --. 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:83,availability,avail,available,83,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:604,deployability,observ,observe,604,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1384,deployability,observ,observe,1384,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:74,integrability,pub,publicly,74,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:257,integrability,event,event,257,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:667,integrability,sub,substantial,667,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:970,interoperability,format,format,970,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:83,reliability,availab,available,83,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:314,reliability,doe,does,314,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:83,safety,avail,available,83,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:83,security,availab,available,83,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1044,security,sign,signature,1044,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:604,testability,observ,observe,604,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1384,testability,observ,observe,1384,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:679,usability,support,support,679,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:903,usability,document,documents,903,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1021,usability,learn,learn,1021,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1295,usability,learn,learned,1295,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:97,integrability,pub,public,97,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:67,interoperability,share,shared,67,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:233,reliability,doe,does,233,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:194,usability,clear,clear,194,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:425,integrability,filter,filter,425,"Hi @dhwani2410 . > I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response. > Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it. My prior answer addresses this question - that there are sometimes regions which may have markers of segmental duplication instead of variants, and DeepVariant will sometimes call positions as reference. It makes this decision on a position-by-position basis and as a result, some of these calls may be inconsistent with what appears from a visual phasing of the reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:52,interoperability,coordinat,coordinates,52,"Hi @dhwani2410 . > I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response. > Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it. My prior answer addresses this question - that there are sometimes regions which may have markers of segmental duplication instead of variants, and DeepVariant will sometimes call positions as reference. It makes this decision on a position-by-position basis and as a result, some of these calls may be inconsistent with what appears from a visual phasing of the reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:108,reliability,doe,does,108,"Hi @dhwani2410 . > I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response. > Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it. My prior answer addresses this question - that there are sometimes regions which may have markers of segmental duplication instead of variants, and DeepVariant will sometimes call positions as reference. It makes this decision on a position-by-position basis and as a result, some of these calls may be inconsistent with what appears from a visual phasing of the reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:936,usability,support,supporting,936,"Hi @dhwani2410 . > I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response. > Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it. My prior answer addresses this question - that there are sometimes regions which may have markers of segmental duplication instead of variants, and DeepVariant will sometimes call positions as reference. It makes this decision on a position-by-position basis and as a result, some of these calls may be inconsistent with what appears from a visual phasing of the reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:1292,usability,visual,visual,1292,"Hi @dhwani2410 . > I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp? Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not. Just like GATK has BP resolution option when we run the variant caller. I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response. > Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it. My prior answer addresses this question - that there are sometimes regions which may have markers of segmental duplication instead of variants, and DeepVariant will sometimes call positions as reference. It makes this decision on a position-by-position basis and as a result, some of these calls may be inconsistent with what appears from a visual phasing of the reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:362,availability,down,downstream,362,"@AndrewCarroll . _I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response._. I agree GVCF gives more information, but I need to process the VCF file and downstream processing tools accepts only VCF format. Just like in GATK we can get information for all bases using ""-ERC BP resolution"", is it possible here in deep variant to get information of all variant as well as non-variant sites in VCF format",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:407,interoperability,format,format,407,"@AndrewCarroll . _I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response._. I agree GVCF gives more information, but I need to process the VCF file and downstream processing tools accepts only VCF format. Just like in GATK we can get information for all bases using ""-ERC BP resolution"", is it possible here in deep variant to get information of all variant as well as non-variant sites in VCF format",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:604,interoperability,format,format,604,"@AndrewCarroll . _I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response._. I agree GVCF gives more information, but I need to process the VCF file and downstream processing tools accepts only VCF format. Just like in GATK we can get information for all bases using ""-ERC BP resolution"", is it possible here in deep variant to get information of all variant as well as non-variant sites in VCF format",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:384,usability,tool,tools,384,"@AndrewCarroll . _I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response._. I agree GVCF gives more information, but I need to process the VCF file and downstream processing tools accepts only VCF format. Just like in GATK we can get information for all bases using ""-ERC BP resolution"", is it possible here in deep variant to get information of all variant as well as non-variant sites in VCF format",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:82,energy efficiency,current,currently,82,"Hi @dhwani2410 . No, DeepVariant does not have a BP resolution option. We are not currently planning to add one, but we can keep this in mind in case we see more demand for such a feature in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:33,reliability,doe,does,33,"Hi @dhwani2410 . No, DeepVariant does not have a BP resolution option. We are not currently planning to add one, but we can keep this in mind in case we see more demand for such a feature in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/318:92,testability,plan,planning,92,"Hi @dhwani2410 . No, DeepVariant does not have a BP resolution option. We are not currently planning to add one, but we can keep this in mind in case we see more demand for such a feature in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318
https://github.com/google/deepvariant/issues/319:100,testability,understand,understand,100,"There are several questions here, and I can answer them all individually, but first can you help me understand overall what you are trying to do? For context, when you run run_deepvariant, it already includes make_examples, call_variants, and postprocess_variants. So are you trying to re-run postprocess_variants?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:150,testability,context,context,150,"There are several questions here, and I can answer them all individually, but first can you help me understand overall what you are trying to do? For context, when you run run_deepvariant, it already includes make_examples, call_variants, and postprocess_variants. So are you trying to re-run postprocess_variants?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:92,usability,help,help,92,"There are several questions here, and I can answer them all individually, but first can you help me understand overall what you are trying to do? For context, when you run run_deepvariant, it already includes make_examples, call_variants, and postprocess_variants. So are you trying to re-run postprocess_variants?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:89,interoperability,format,format,89,@dhwani2410 my main aim to get a list of all variant as well as non-variant sites in VCF format and not g.vcf format. I thought running post-process variants may help me this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:110,interoperability,format,format,110,@dhwani2410 my main aim to get a list of all variant as well as non-variant sites in VCF format and not g.vcf format. I thought running post-process variants may help me this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:162,usability,help,help,162,@dhwani2410 my main aim to get a list of all variant as well as non-variant sites in VCF format and not g.vcf format. I thought running post-process variants may help me this.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:117,deployability,stage,stage,117,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:559,deployability,stage,stages,559,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:476,interoperability,specif,specific,476,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1561,interoperability,specif,specific,1561," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1902,interoperability,specif,specific,1902," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:285,modifiability,paramet,parameters,285,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:485,modifiability,paramet,parameters,485,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:664,modifiability,paramet,parameters,664,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:770,modifiability,paramet,parameter,770,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1944,modifiability,paramet,parameters,1944," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1204,safety,valid,valid,1204," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1423,safety,valid,valid,1423," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:967,usability,help,helpshort,967,"Okay, I see. The run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant base",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1075,usability,help,helpshort,1075," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:1794,usability,help,helpshort,1794," run_deepvariant script you ran already includes postprocess_variants as the last step, which is the stage that produced the VCF and optionally gVCF. As I answered in your other issue (https://github.com/google/deepvariant/issues/318), **there unfortunately aren't any parameters in postprocess_variants that will generate a VCF of every base without variants**. I'm including the below for reference in case you or others want to run individual steps or pass specific parameters into the make_examples, call_variants, or postprocess_variants stages. ## How to get usage information for run_deepvariant and other runner scripts. If you need to add parameters for postprocess_variants for another reason, you can add a `--postprocess_variants_extra_args` parameter to run_deepvariant. . See usage for run_deepvariant in the [code for run_deepvariant](https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py) or by running it with --helpshort:. ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --helpshort. # output includes:. --make_examples_extra_args: A comma-separated list of flag_name=flag_value. ""flag_name"" has to be valid flags for make_examples.py. If the flag_value is. boolean, it has to be flag_name=true or flag_name=false. --postprocess_variants_extra_args: A comma-separated list of. flag_name=flag_value. ""flag_name"" has to be valid flags for. postprocess_variants.py. If the flag_value is boolean, it has to be. flag_name=true or flag_name=false. ```. And for the specific flags for [postprocess_variants.py](https://github.com/google/deepvariant/blob/r0.10/deepvariant/postprocess_variants.py). ```. sudo docker run google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --helpshort. ```. Again, I want to make sure to reiterate that I'm including this for reference, but for your specific request, we already know that no parameters will give a VCF with all the non-variant bases in the genome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:99,usability,command,command,99,"@dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. `bcftools convert --gvcf2vcf`. Documentation at http://samtools.github.io/bcftools/bcftools.html#convert",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:155,usability,Document,Documentation,155,"@dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. `bcftools convert --gvcf2vcf`. Documentation at http://samtools.github.io/bcftools/bcftools.html#convert",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:283,reliability,doe,does,283,"> @dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. > . > `bcftools convert --gvcf2vcf`. > . > Documentation at http://samtools.github.io/bcftools/bcftools.html#convert. This is awesome, but are you sure this does what you think it does though? Because the gvcf from deepvariant is different from other gvcfs...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:306,reliability,doe,does,306,"> @dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. > . > `bcftools convert --gvcf2vcf`. > . > Documentation at http://samtools.github.io/bcftools/bcftools.html#convert. This is awesome, but are you sure this does what you think it does though? Because the gvcf from deepvariant is different from other gvcfs...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:101,usability,command,command,101,"> @dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. > . > `bcftools convert --gvcf2vcf`. > . > Documentation at http://samtools.github.io/bcftools/bcftools.html#convert. This is awesome, but are you sure this does what you think it does though? Because the gvcf from deepvariant is different from other gvcfs...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/319:169,usability,Document,Documentation,169,"> @dhwani2410 To do this manually, you can take the gVCF output by DeepVariant and use the following command from `bcftools`. > . > `bcftools convert --gvcf2vcf`. > . > Documentation at http://samtools.github.io/bcftools/bcftools.html#convert. This is awesome, but are you sure this does what you think it does though? Because the gvcf from deepvariant is different from other gvcfs...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/319
https://github.com/google/deepvariant/issues/320:1133,deployability,resourc,resources,1133,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:179,energy efficiency,current,current-and-historical-wgs-accuracy-across-coverage,179,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1100,energy efficiency,model,model,1100,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1133,energy efficiency,resourc,resources,1133,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1316,energy efficiency,model,models,1316,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1703,interoperability,specif,specific,1703,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:264,performance,perform,performance,264,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1119,performance,computational resourc,computational resources,1119,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1133,safety,resourc,resources,1133,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1100,security,model,model,1100,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1316,security,model,models,1316,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:62,testability,coverag,coverages,62,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:222,testability,coverag,coverage,222,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:304,testability,coverag,coverages,304,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:862,testability,understand,understanding,862,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1133,testability,resourc,resources,1133,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:1575,testability,context,context,1575,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/320:264,usability,perform,performance,264,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:. 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) . 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:. 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using. 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail. 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320
https://github.com/google/deepvariant/issues/321:185,energy efficiency,gpu,gpu-image,185,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ? Could you please provide the exact commands that you run? Thank you. Alexey.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:185,performance,gpu,gpu-image,185,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ? Could you please provide the exact commands that you run? Thank you. Alexey.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:28,reliability,doe,doesn,28,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ? Could you please provide the exact commands that you run? Thank you. Alexey.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:88,usability,guid,guide,88,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ? Could you please provide the exact commands that you run? Thank you. Alexey.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:233,usability,command,commands,233,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ? Could you please provide the exact commands that you run? Thank you. Alexey.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:897,deployability,instal,installed,897,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:952,deployability,contain,container,952,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:998,deployability,version,version,998,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1013,deployability,log,logs,1013,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:64,energy efficiency,gpu,gpus,64,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:75,energy efficiency,cpu,cpus,75,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:589,energy efficiency,gpu,gpus,589,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:821,energy efficiency,GPU,GPU,821,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:998,integrability,version,version,998,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:872,interoperability,incompatib,incompatibility,872,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:506,modifiability,paramet,parameters,506,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:998,modifiability,version,version,998,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:64,performance,gpu,gpus,64,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:75,performance,cpu,cpus,75,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:589,performance,gpu,gpus,589,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:821,performance,GPU,GPU,821,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:108,safety,input,input,108,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:297,safety,input,input,297,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:325,safety,input,input,325,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:331,safety,input,input,331,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1013,safety,log,logs,1013,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
