id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/allenai/scispacy/issues/321:375,security,model,model,375,"Tokenizer used for NER in Scipacy model : ""en_ner_bionlp13cg_md""; Helllo,. I'm trying to cluster the embeddings of entities generated by my model. To train an embedding model I'll need the same tokenizer that my scispacy model uses. I tried using scispacy custom tokenizer but it does not give me the exact tokens. Could you please help me identify the tokenizer that my NER model uses to identify tokens?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:389,security,ident,identify,389,"Tokenizer used for NER in Scipacy model : ""en_ner_bionlp13cg_md""; Helllo,. I'm trying to cluster the embeddings of entities generated by my model. To train an embedding model I'll need the same tokenizer that my scispacy model uses. I tried using scispacy custom tokenizer but it does not give me the exact tokens. Could you please help me identify the tokenizer that my NER model uses to identify tokens?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:398,security,token,tokens,398,"Tokenizer used for NER in Scipacy model : ""en_ner_bionlp13cg_md""; Helllo,. I'm trying to cluster the embeddings of entities generated by my model. To train an embedding model I'll need the same tokenizer that my scispacy model uses. I tried using scispacy custom tokenizer but it does not give me the exact tokens. Could you please help me identify the tokenizer that my NER model uses to identify tokens?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:256,usability,custom,custom,256,"Tokenizer used for NER in Scipacy model : ""en_ner_bionlp13cg_md""; Helllo,. I'm trying to cluster the embeddings of entities generated by my model. To train an embedding model I'll need the same tokenizer that my scispacy model uses. I tried using scispacy custom tokenizer but it does not give me the exact tokens. Could you please help me identify the tokenizer that my NER model uses to identify tokens?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:332,usability,help,help,332,"Tokenizer used for NER in Scipacy model : ""en_ner_bionlp13cg_md""; Helllo,. I'm trying to cluster the embeddings of entities generated by my model. To train an embedding model I'll need the same tokenizer that my scispacy model uses. I tried using scispacy custom tokenizer but it does not give me the exact tokens. Could you please help me identify the tokenizer that my NER model uses to identify tokens?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/322:272,availability,error,error,272,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1395,availability,Error,Errors,1395,"0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ne",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2151,availability,Error,Errors,2151,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:91,deployability,upgrad,upgraded,91,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,deployability,version,version,100,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:438,deployability,version,version,438,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:519,deployability,version,versions,519,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:545,deployability,version,versions,545,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:558,deployability,releas,released,558,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:686,deployability,version,version,686,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:939,deployability,modul,module,939,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2459,deployability,version,version,2459,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,energy efficiency,load,load,191,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:244,energy efficiency,load,load,244,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:364,energy efficiency,Model,Model,364,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:580,energy efficiency,model,model,580,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1008,energy efficiency,load,load,1008,"E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path,",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1092,energy efficiency,load,load,1092,"raded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exist",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1121,energy efficiency,load,load,1121,".3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_fil",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,integrability,version,version,100,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:438,integrability,version,version,438,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:519,integrability,version,versions,519,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:545,integrability,version,versions,545,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:686,integrability,version,version,686,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2459,integrability,version,version,2459,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:401,interoperability,specif,specifies,401,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:485,interoperability,compatib,compatibility,485,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:599,interoperability,compatib,compatible,599,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1407,interoperability,format,format,1407," an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2163,interoperability,format,format,2163,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:91,modifiability,upgrad,upgraded,91,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,modifiability,version,version,100,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:316,modifiability,pac,packages,316,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:438,modifiability,version,version,438,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:519,modifiability,version,versions,519,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:545,modifiability,version,versions,545,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:686,modifiability,version,version,686,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:939,modifiability,modul,module,939,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1047,modifiability,pac,packages,1047,"er_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1249,modifiability,pac,packages,1249," get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_pat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1535,modifiability,pac,packages,1535,"spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1967,modifiability,pac,packages,1967,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2384,modifiability,pac,packages,2384,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2459,modifiability,version,version,2459,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,performance,load,load,191,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:244,performance,load,load,244,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:272,performance,error,error,272,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1008,performance,load,load,1008,"E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path,",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1092,performance,load,load,1092,"raded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exist",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1121,performance,load,load,1121,".3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_fil",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1395,performance,Error,Errors,1395,"0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ne",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2151,performance,Error,Errors,2151,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:272,safety,error,error,272,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:912,safety,input,input-,912,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:939,safety,modul,module,939,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1395,safety,Error,Errors,1395,"0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ne",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2151,safety,Error,Errors,2151,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:364,security,Model,Model,364,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:580,security,model,model,580,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:868,testability,Trace,Traceback,868,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:272,usability,error,error,272,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:344,usability,User,UserWarning,344,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:912,usability,input,input-,912,"OSError: [E053] Could not read config.cfg from en_ner_bionlp13cg_md-0.3.0/config.cfg; When upgraded version of spacy to 3.0.3, these libraries (en_ner_bionlp13cg_md, en_ner_bc5cdr_md) cannot load. On . import en_ner_bc5cdr_md. en_ner_bc5cdr_md.load(). I get the following error. . /root/anaconda3/lib/python3.8/site-packages/spacy/util.py:715: UserWarning: [W094] Model 'en_ner_bionlp13cg_md' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:1395,usability,Error,Errors,1395,"0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ne",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:2151,usability,Error,Errors,2151,"Consider changing the ""spacy_version"" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.3,<3.1.0. warnings.warn(warn_msg). ---------------------------------------------------------------------------. OSError Traceback (most recent call last). <ipython-input-45-c2a08dda597e> in <module>. 1 import en_ner_bionlp13cg_md. ----> 2 en_ner_bionlp13cg_md.load(). ~/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/__init__.py in load(**overrides). 8 . 9 def load(**overrides):. ---> 10 nlp = load_model_from_init_py(__file__, **overrides). 11 return nlp. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config). 512 if not model_path.exists():. 513 raise IOError(Errors.E052.format(path=data_path)). --> 514 return load_model_from_path(. 515 data_path,. 516 vocab=vocab,. ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config). 386 meta = get_model_meta(model_path). 387 config_path = model_path / ""config.cfg"". --> 388 config = load_config(config_path, overrides=dict_to_dot(config)). 389 nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude). 390 return nlp.from_disk(model_path, exclude=exclude). ~/anaconda3/lib/python3.8/site-packages/spacy/util.py in load_config(path, overrides, interpolate). 543 else:. 544 if not config_path or not config_path.exists() or not config_path.is_file():. --> 545 raise IOError(Errors.E053.format(path=config_path, name=""config.cfg"")). 546 return config.from_disk(. 547 config_path, overrides=overrides, interpolate=interpolate. OSError: [E053] Could not read config.cfg from /root/anaconda3/lib/python3.8/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.3.0/config.cfg. SpaCy version 2.x works normally. Is truly the problem only the missing config.cfg? PS: Thank you for your excellent work! Life is much easier and productive with scispaCy.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/pull/323:0,deployability,Updat,Update,0,Update index.md;,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/323
https://github.com/allenai/scispacy/pull/323:0,safety,Updat,Update,0,Update index.md;,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/323
https://github.com/allenai/scispacy/pull/323:0,security,Updat,Update,0,Update index.md;,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/323
https://github.com/allenai/scispacy/issues/324:79,deployability,version,version,79,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:272,deployability,updat,update,272,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:310,deployability,Version,Version,310,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:329,deployability,Instal,Install,329,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:347,deployability,instal,install,347,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:376,deployability,instal,install,376,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:435,deployability,releas,releases,435,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2304,deployability,Version,Version,2304,"012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698'",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2323,deployability,Instal,Install,2323,"86399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859'",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2341,deployability,instal,install,2341,"63', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859', 0.84249883890151",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2370,deployability,instal,install,2370,"D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859', 0.8424988389015198), ('C0003060', 0.815065979",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2429,deployability,releas,releases,2429,", 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859', 0.8424988389015198), ('C0003060', 0.8150659799575806), ('C0003044', 0.8055132627487183)], [('C0024530', ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:841,energy efficiency,load,load,841,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2835,energy efficiency,load,load,2835,", ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859', 0.8424988389015198), ('C0003060', 0.8150659799575806), ('C0003044', 0.8055132627487183)], [('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0683368', 1.0), ('C1457887', 1.0), ('C0231221', 0.9386579990386963), ('C1706867', 0.9368797540664673)], [('C0015967', 1.0), ('C4552740', 1.0), ('C0019345', 0.7983872890472412), ('C0034362', 0.7933211922645569), ('C0872996', 0.7933211922645569)], [('C0015672', 0.9999998807907104), ('C4553152', 0.999999",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:79,integrability,version,version,79,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:310,integrability,Version,Version,310,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2304,integrability,Version,Version,2304,"012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698'",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:79,modifiability,version,version,79,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:310,modifiability,Version,Version,310,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2304,modifiability,Version,Version,2304,"012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698'",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:841,performance,load,load,841,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:2835,performance,load,load,2835,", ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036', 0.8386269211769104), ('D020938', 0.7761068940162659), ('D004828', 0.7586985230445862), ('D003294', 0.7213573455810547)], [('D003128', 1.0)], [('D003643', 1.0), ('D009026', 0.8019763231277466), ('D016923', 0.784024178981781), ('D013398', 0.7552506327629089), ('D001926', 0.7274324297904968)]]. ```. ## Version 0.4.0. ### Install. ```. pip install scispacy==0.4.0. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz. ```. ### Script. ```. from scispacy.linking import EntityLinker. import scispacy. import spacy. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""name"": ""mesh""}). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0521831', 1.0)], [('C0086418', 1.0), ('C1300203', 1.0), ('C0318356', 0.7484926581382751), ('C0005660', 0.7458980083465576), ('C4318409', 0.7329899072647095)], [('C0003062', 1.0), ('C3540698', 1.0), ('C4704859', 0.8424988389015198), ('C0003060', 0.8150659799575806), ('C0003044', 0.8055132627487183)], [('C0024530', 1.0), ('C0206255', 1.0), ('C2984572', 1.0), ('C3541306', 1.0), ('C0024536', 0.9396661520004272)], [('C0683368', 1.0), ('C1457887', 1.0), ('C0231221', 0.9386579990386963), ('C1706867', 0.9368797540664673)], [('C0015967', 1.0), ('C4552740', 1.0), ('C0019345', 0.7983872890472412), ('C0034362', 0.7933211922645569), ('C0872996', 0.7933211922645569)], [('C0015672', 0.9999998807907104), ('C4553152', 0.999999",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:272,safety,updat,update,272,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:272,security,updat,update,272,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/issues/324:190,usability,behavi,behaviour,190,"MeSH linker returns UMLS concepts; # Details. It seems that the mesh linker in version 0.4 returns UMLS concepts instead of MeSH. I am providing you some code to reproduce the difference in behaviour between 0.2.5 and 0.4.0. Will investigate the source of the problem and update this issue. # To reproduce. ## Version 0.2.5. ### Install. ```. pip install scispacy==0.2.5. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz. ```. ### Script. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. text = ""Malaria is a mosquito-borne infectious disease that affects humans and other animals. Malaria causes symptoms that typically include fever, tiredness, vomiting, and headaches. In severe cases it can cause yellow skin, seizures, coma, or death"". nlp = spacy.load('en_core_sci_sm'). linker = EntityLinker(name=""mesh""). doc = nlp(text). print([ent._.kb_ents for ent in doc.ents]). ```. ### Results. ```. [[('D008288', 1.0)], [], [('D006801', 1.0)], [('D000818', 1.0), ('D002013', 0.7804350256919861), ('D000829', 0.7701025605201721), ('D000835', 0.7683348655700684), ('D006762', 0.7380481958389282)], [('D008288', 1.0)], [('D000071896', 0.8311232328414917), ('D062706', 0.7586084604263306), ('D009461', 0.7523581385612488), ('D012816', 0.7229775786399841), ('D003863', 0.720658540725708)], [('D005334', 1.0), ('D006255', 0.8281346559524536), ('D011778', 0.8099866509437561), ('D011016', 0.8098501563072205), ('D014435', 0.7505505084991455)], [], [('D014839', 1.0)], [('D006261', 0.8413779735565186), ('D014653', 0.7558636665344238)], [('D018805', 0.7512739300727844)], [], [('D004124', 0.795078694820404), ('D031567', 0.7838744521141052), ('D010128', 0.7816929817199707), ('D001853', 0.7810686230659485), ('D031804', 0.7755565047264099)], [('D012867', 1.0), ('D012880', 0.7630483508110046), ('D017592', 0.7562605738639832), ('D009506', 0.7346692085266113), ('D012882', 0.7077863812446594)], [('D012640', 1.0), ('D013036",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/324
https://github.com/allenai/scispacy/pull/325:0,deployability,Updat,Update,0,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/pull/325:35,deployability,Updat,Updates,35,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/pull/325:0,safety,Updat,Update,0,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/pull/325:35,safety,Updat,Updates,35,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/pull/325:0,security,Updat,Update,0,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/pull/325:35,security,Updat,Updates,35,Update README on EntityLinker 0.4; Updates instructions on using EntityLinker with linkers other than the default. Fixes #324,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/325
https://github.com/allenai/scispacy/issues/326:139,deployability,pipelin,pipelines,139,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:773,deployability,pipelin,pipelines,773,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:139,integrability,pipelin,pipelines,139,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:773,integrability,pipelin,pipelines,773,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:156,reliability,doe,does,156,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:289,safety,input,input,289,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:81,usability,tool,tool,81,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:289,usability,input,input,289,"Chunking seems to not be working properly; Hello and thank you for creating this tool! I have been trying to use the noun_chunks with your pipelines but it does not seem to be working correctly. I have tried with _en_core_sci_sm_, _en_core_sci_md_ and _en_core_sci_lg_. For example when I input the sentence ""CCR5(+) and CXCR3(+) T cells are increased in multiple sclerosis and their ligands MIP-1alpha and IP-10 are expressed in demyelinating brain lesions."" I only get ""CCR5(+"", ""CXCR3(+"" and ""T cells"" as chunks and I would expect more chunks. For example, using spaCys _en_core_web_trf_ I get ""CCR5(+) and CXCR3(+) T cells"", ""multiple sclerosis"", ""their ligands"", ""MIP-1alpha"", ""IP-10"" and ""brain lesions"". . Is the chunking supposed to work in a similar way as spaCys pipelines or have I misinterpreted something? Thank you in advance! Best regards",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/327:80,energy efficiency,load,load,80,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:198,integrability,event,events,198,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:388,integrability,event,events,388,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:580,integrability,event,events,580,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:693,integrability,event,events,693,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:80,performance,load,load,80,"There is something wrong with Sentence segmentation; `import spacy. nlp = spacy.load('en_core_sci_sm'). print('the first entence segmentation:'). doc1 = nlp('Positive for translocation or inversion events involving the ROS1 gene'). for i,sent in enumerate(doc1.sents):. print(i). print(sent). print('the second entence segmentation:'). doc2 = nlp('Negative for translocation or inversion events involving the ALK gene'). for i,sent in enumerate(doc2.sents):. print(i). print(sent)`. The output is as:. > the first entence segmentation:. 0. Positive for translocation or inversion events. 1. involving the ROS1 gene. the second entence segmentation:. 0. Negative for translocation or inversion events involving the ALK gene. The result is obviously wrong!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/328:152,deployability,version,version,152,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:152,integrability,version,version,152,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:29,interoperability,specif,specific,29,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:152,modifiability,version,version,152,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:105,reliability,doe,doesn,105,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:285,usability,user,user-images,285,RuntimeError: [E896] - Colab specific; Hi; I'm trying to run your example code in a Google Colab; but it doesn't appear to work. I'm not sure if it's a version issue. Could you please advise on whether this is an environment issue or if I'm missing the right imports? ![image](https://user-images.githubusercontent.com/31191581/110044501-55973b80-7ced-11eb-8c66-0930d522162b.png).,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/329:166,availability,mask,masked,166,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:69,deployability,build,build,69,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:34,energy efficiency,model,model,34,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:100,energy efficiency,model,model,100,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:182,energy efficiency,model,model,182,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:200,energy efficiency,predict,prediction,200,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:220,energy efficiency,model,model,220,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:16,interoperability,specif,specific,16,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:82,interoperability,specif,specific,82,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:280,modifiability,exten,extended,280,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:200,safety,predict,prediction,200,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:34,security,model,model,34,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:100,security,model,model,100,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:182,security,model,model,182,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/329:220,security,model,model,220,"Training domain specific language model from scratch; Hi,. I want to build domain specific language model for NER task. For example, hugging face allows us to create masked language model or sentence prediction language model in unsupervised way for our own corpus.. which can be extended for NER task. Can we do something like it with spacy?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/330:38,safety,compl,complete,38,User Survey - Please take a moment to complete this if you use scispacy!; https://docs.google.com/forms/d/e/1FAIpQLScS6BjlLLdF584RO0WlMwaSZP7iFRPyLdSxaPJuD54sfDtz8w/viewform?usp=sf_link,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/330
https://github.com/allenai/scispacy/issues/330:38,security,compl,complete,38,User Survey - Please take a moment to complete this if you use scispacy!; https://docs.google.com/forms/d/e/1FAIpQLScS6BjlLLdF584RO0WlMwaSZP7iFRPyLdSxaPJuD54sfDtz8w/viewform?usp=sf_link,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/330
https://github.com/allenai/scispacy/issues/330:0,usability,User,User,0,User Survey - Please take a moment to complete this if you use scispacy!; https://docs.google.com/forms/d/e/1FAIpQLScS6BjlLLdF584RO0WlMwaSZP7iFRPyLdSxaPJuD54sfDtz8w/viewform?usp=sf_link,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/330
https://github.com/allenai/scispacy/issues/331:287,deployability,build,building,287,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:169,integrability,transform,transformer,169,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:25,interoperability,ontolog,ontologies,25,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:169,interoperability,transform,transformer,169,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:272,interoperability,ontolog,ontologies,272,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:370,interoperability,ontolog,ontology,370,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:380,interoperability,specif,specifically,380,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:435,interoperability,ontolog,ontology,435,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:413,testability,Plan,Planteome,413,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:423,testability,plan,plant-trait-ontology,423,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:214,usability,experien,experience,214,"Entity-linking for other ontologies; Hey there! Firstly, thank you so much for all the hard work you all have put into scispacy! I really appreciate the addition of the transformer and entity-linking. While I have experience using spacy for ner and rel, I am quite new to ontologies and building spacy entity-linkers/knowledge bases. Would it be possible to get another ontology (specifically, https://github.com/Planteome/plant-trait-ontology) added? Or a tutorial/description of how I could do this myself? Thank you again,. Blake",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/332:147,availability,error,error,147,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:173,deployability,Fail,Failed,173,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:40,energy efficiency,load,loading,40,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:66,energy efficiency,load,loading,66,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:52,modifiability,pac,package,52,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:107,modifiability,pac,package,107,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:40,performance,load,loading,40,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:66,performance,load,loading,66,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:147,performance,error,error,147,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:173,reliability,Fail,Failed,173,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:147,safety,error,error,147,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:147,usability,error,error,147,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:332,usability,help,helpful,332,"""en_ner_bionlp13cg_md-0.4.0"" issue with loading the package; I am loading the ""en_ner_bionlp13cg_md-0.4.0"" package which is throwing the following error. ValueError: [E912] Failed to initialize lemmatizer. Missing lemmatizer table(s) found for mode 'rule'. Required tables: ['lemma_rules']. Found: []. Any Solution for this will be helpful.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/333:371,availability,down,downloads,371,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:0,deployability,Instal,Install,0,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:54,deployability,instal,install,54,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:85,deployability,version,version,85,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:119,deployability,instal,install,119,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:227,deployability,version,version,227,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:245,deployability,instal,install,245,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:304,deployability,releas,releases,304,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:385,deployability,instal,installs,385,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:464,deployability,version,version,464,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:511,deployability,version,version,511,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:17,energy efficiency,model,models,17,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:189,energy efficiency,model,model,189,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:412,energy efficiency,model,model,412,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:85,integrability,version,version,85,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:227,integrability,version,version,227,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:464,integrability,version,version,464,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:511,integrability,version,version,511,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:201,interoperability,compatib,compatible,201,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:85,modifiability,version,version,85,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:227,modifiability,version,version,227,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:464,modifiability,version,version,464,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:511,modifiability,version,version,511,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:17,security,model,models,17,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:189,security,model,model,189,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/333:412,security,model,model,412,"Install scispacy models with spacy 2.X; When I try to install scispacy with a forced version, everything is fine:. pip install scispacy==0.3.0 . But when I try to get the scispacy language model large compatible with the 0.3.0 version via:. pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz#egg=en_core_sci_lg. It downloads and installs 0.4.0 of both the model and scispacy which also forces spacy to go to version 3.X. Is there any way to get the 0.3.0 version of en_core_sci_lg ?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/334:80,availability,error,error,80,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:277,availability,down,download,277,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:437,availability,down,downloads,437,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1252,availability,Operat,Operating,1252,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:39,deployability,instal,installation,39,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:97,deployability,instal,installation,97,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:593,deployability,pipelin,pipeline,593,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1297,deployability,Version,Version,1297,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1331,deployability,Version,Version,1331,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:204,energy efficiency,load,load,204,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:303,energy efficiency,load,load,303,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1073,energy efficiency,current,currently,1073,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:593,integrability,pipelin,pipeline,593,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1297,integrability,Version,Version,1297,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1331,integrability,Version,Version,1331,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:491,modifiability,paramet,parameter,491,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1297,modifiability,Version,Version,1297,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1331,modifiability,Version,Version,1331,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:80,performance,error,error,80,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:204,performance,load,load,204,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:303,performance,load,load,303,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:407,performance,time,time,407,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:451,performance,cach,cached,451,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:716,performance,perform,performed,716,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:80,safety,error,error,80,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:862,safety,test,test,862,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:903,safety,test,test,903,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:862,testability,test,test,862,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:903,testability,test,test,903,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:80,usability,error,error,80,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:716,usability,perform,performed,716,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1405,usability,user,user-images,1405,"Entity linker execution demands NMSLIB installation from sources and results in error even after installation; Code:. import spacy. import scispacy. from scispacy.linking import EntityLinker. nlp = spacy.load(""en_core_sci_lg""). """""". this line takes a while, because we have to download ~1GB of data and load a large JSON file (the knowledge base). Be patient! Thankfully it should be faster after the first time you use it, because. the downloads are cached. NOTE: The resolve_abbreviations parameter is optional, and requires that. the AbbreviationDetector pipe has already been added to the pipeline. Adding. the AbbreviationDetector pipe and setting resolve_abbreviations to True means. that linking will only be performed on the long form of abbreviations. """""". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). test = open(""xloutput.txt"",""r"") . text = test.readlines(). doc = nlp(str(text)). //Let's look at a random entity! entity = doc.ents[1]. print(""Name: "", entity). //Each entity is linked to UMLS with a score. // (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). Environment used:. Operating System: Ubuntu 20.04.2 LTS. Python Version Used: Python 3.8.5. spaCy Version Used: Spacy 3.0.4. ![Screenshot from 2021-03-11 10-48-54](https://user-images.githubusercontent.com/78722507/110740392-00ad7500-8259-11eb-9411-a61b4f70a052.png).",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/335:848,availability,error,errors,848,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:431,deployability,manag,managed,431,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:37,energy efficiency,model,model,37,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:84,energy efficiency,model,model,84,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:431,energy efficiency,manag,managed,431,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:442,energy efficiency,adapt,adapt,442,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:188,integrability,compon,component,188,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:303,integrability,compon,component,303,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:331,integrability,compon,component,331,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:442,integrability,adapt,adapt,442,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:188,interoperability,compon,component,188,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:303,interoperability,compon,component,303,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:331,interoperability,compon,component,331,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:442,interoperability,adapt,adapt,442,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:188,modifiability,compon,component,188,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:303,modifiability,compon,component,303,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:331,modifiability,compon,component,331,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:442,modifiability,adapt,adapt,442,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:1004,modifiability,reu,reuse,1004,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:1036,modifiability,layer,layer,1036,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:563,performance,time,time,563,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:848,performance,error,errors,848,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:431,safety,manag,managed,431,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:848,safety,error,errors,848,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:1021,safety,except,except,1021,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:37,security,model,model,37,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:84,security,model,model,84,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:902,testability,understand,understand,902,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:9,usability,learn,learning,9,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:60,usability,learn,learn,60,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:174,usability,learn,learn,174,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:363,usability,resum,resume,363,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:505,usability,command,command,505,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:775,usability,resum,resume,775,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:848,usability,error,errors,848,"Transfer learning of specialised NER model; Hi, I'd like to learn a specialised NER model on my own corpus starting from en_core_sci_md. I'd like to try several things :. 1. learn the NER component from scratch (so I would keep tok2vec, parser, tagger, attribute_ruler, lemmatizer, and create a new ner component). 1. keep the NER component in en_core_sci_md and resume training on my data. I have some python code to do 2. that I managed to adapt to spaCy v3, but I'd like to use the whole config file + command line way of doing things. Well, I'm having a hard time ^^"". I enclose my config file [base_ner.cfg.txt](https://github.com/allenai/scispacy/files/6129705/base_ner.cfg.txt) (inspired by your own base_ner.cfg) that I hope will let me do 1. (I have now idea how to resume training using config files at the moment), but that is returning errors when using spacy debug. In particular, I don't understand why it asks for vectors, init_tok2vec, vocab_data, lookups in [initialize] since I want to reuse everything except the NER layer. Thank you.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/336:34,availability,robust,robustness,34,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:231,deployability,contain,containing,231,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:804,deployability,version,version,804,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:214,integrability,Pub,PubMed,214,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:221,integrability,abstract,abstracts,221,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:804,integrability,version,version,804,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1061,integrability,pub,published,1061,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:221,modifiability,abstract,abstracts,221,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:804,modifiability,version,version,804,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1051,performance,time,time,1051,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1483,performance,time,times,1483,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:34,reliability,robust,robustness,34,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:34,safety,robust,robustness,34,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1125,safety,detect,detected,1125,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1430,safety,detect,detected,1430,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1125,security,detect,detected,1125,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1327,security,Authoriz,Authorization,1327,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:1430,security,detect,detected,1430,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:470,usability,user,user-images,470,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:677,usability,user,user-images,677,"Explore data augmentation for NER robustness; Hi, I am working on a covid-19 antiviral and was spot checking antivirals in scispacy and was surprised that remdesivir is not tagged as a chemical in any of the 1,338 PubMed abstracts containing it. I'm using en_ner_bc5cdr_md to extract CHEMICAL and DISEASE entities; spacy: '3.0.4', scispacy: '0.4.0'. As you see below, remdesivir is not tagged as a CHEMICAL when I run en_ner_bc5cdr_md in Jupyter Lab. . ![image](https://user-images.githubusercontent.com/70850242/111051780-9eed3680-8423-11eb-97a2-870f4ee12280.png). However, when I put the same text into your demo, I was surprised that remdesivir is found. . ![image](https://user-images.githubusercontent.com/70850242/111051846-04412780-8424-11eb-9229-02df44ef98e8.png). **Questions**. - Wonder if the version running on demo is the same one that I used in my notebook (spacy: '3.0.4', scispacy: '0.4.0')? - Maybe remdesivir isn't found since it wasn't present in earlier training sets? . - Can we expect new chemicals to be recognized (e.g., first time ever published)? - It's especially surprising that remdesivir wasn't detected as a CHEMICAL even in the following line where it's called a 'drug' from the text used in my example: . . `Though the drug remdesivir (RDV) is not approved by the FDA, still the ""Emergency Use Authorization"" (EUA) for compassionate use in severe cases is endorsed. `. - In the demo remdesivir is detected but only once while it is mentioned several times in that passage. Is that expected? . Thanks, . vikram",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/337:94,deployability,updat,updating,94,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:179,deployability,version,versions,179,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:270,deployability,version,version,270,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:530,deployability,version,version,530,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:179,integrability,version,versions,179,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:270,integrability,version,version,270,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:530,integrability,version,version,530,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:12,interoperability,specif,specify,12,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:179,modifiability,version,versions,179,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:270,modifiability,version,version,270,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:530,modifiability,version,version,530,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:94,safety,updat,updating,94,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:94,security,updat,updating,94,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:392,testability,simpl,simply,392,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:54,usability,custom,custom,54,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:263,usability,custom,custom,263,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/337:392,usability,simpl,simply,392,"Spacy 3.0 - specify my own candidate generator to use custom UMLS path; I'm in the process of updating my python project from using Spacy 2 to use Spacy 3. My project uses my own versions of Scispacy candidate_generation.py and linking_utils.py in order to use a custom version of concept_aliases.json and other UMLS data files. How do I accomplish this in Spacy 3 (Scispacy 0.4.0)? In V2, I simply made copies of candidate_generation.py and linking_utils.py which referenced local copies of the KB data files and instantiated my version of candidate_generation.py and passed the object to the EntitlyLinker constructor like so:. ```. _candidate_generator = CandidateGenerator(). _el = EntityLinker(resolve_abbreviations=True, name=""umls"",. candidate_generator=_candidate_generator). ```. My candidate_generation.py, in turn, references my linking_utils.py.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337
https://github.com/allenai/scispacy/issues/338:720,availability,error,error,720,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3291,availability,error,error,3291,"_Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[k",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3483,availability,error,error,3483,"lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:234,deployability,pipelin,pipeline,234,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:992,deployability,modul,module,992,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1875,deployability,modul,module,1875,"bbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3104,deployability,modul,module,3104,"ipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:5151,deployability,version,version,5151,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:169,energy efficiency,load,load,169,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2911,energy efficiency,current,current,2911,"cess=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:234,integrability,pipelin,pipeline,234,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:726,integrability,messag,message,726,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3297,integrability,messag,message,3297," return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = g",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:5151,integrability,version,version,5151,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:726,interoperability,messag,message,726,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3297,interoperability,messag,message,3297," return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = g",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:340,modifiability,inherit,inherited,340,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:992,modifiability,modul,module,992,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1875,modifiability,modul,module,1875,"bbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1952,modifiability,pac,packages,1952,"ast):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrappin",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2046,modifiability,pac,packages,2046,"line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and yo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3104,modifiability,modul,module,3104,"ipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3827,modifiability,pac,packages,3827,"'''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_pack",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3958,modifiability,pac,packages,3958,"e. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packe",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4234,modifiability,pac,packages,4234,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4419,modifiability,pac,packages,4419,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4553,modifiability,pac,packages,4553,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4602,modifiability,pac,packb,4602,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4616,modifiability,Pac,Packer,4616,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4633,modifiability,pac,pack,4633,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4711,modifiability,Pac,Packer,4711,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4718,modifiability,pac,pack,4718,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4793,modifiability,Pac,Packer,4793,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4800,modifiability,pac,pack,4800,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4875,modifiability,Pac,Packer,4875,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4882,modifiability,pac,pack,4882,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4957,modifiability,Pac,Packer,4957,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:5040,modifiability,Pac,Packer,5040,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:5151,modifiability,version,version,5151,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:169,performance,load,load,169,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:720,performance,error,error,720,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3291,performance,error,error,3291,"_Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[k",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3483,performance,error,error,3483,"lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:734,reliability,doe,doesn,734,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:283,safety,test,test,283,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:556,safety,test,test,556,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:720,safety,error,error,720,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:992,safety,modul,module,992,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1852,safety,test,test,1852,"lves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1875,safety,modul,module,1875,"bbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1904,safety,test,test,1904,"eviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3104,safety,modul,module,3104,"ipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3291,safety,error,error,3291,"_Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[k",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3483,safety,error,error,3483,"lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4072,security,token,tokens,4072,"he proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not se",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4109,security,token,tokens,4109," if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' obje",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4146,security,token,tokens,4146,"upport(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the lates",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4183,security,token,tokens,4183,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4322,security,token,tokens,4322,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:4359,security,token,tokens,4359,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:5090,security,token,tokens,5090,"port()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\msgpack\__init__.py"", line 55, in packb. return Packer(**kwargs).pack(o). File ""srsly\msgpack\_packer.pyx"", line 285, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 291, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 288, in srsly.msgpack._packer.Packer.pack. File ""srsly\msgpack\_packer.pyx"", line 264, in srsly.msgpack._packer.Packer._pack. File ""srsly\msgpack\_packer.pyx"", line 282, in srsly.msgpack._packer.Packer._pack. TypeError: can not serialize 'spacy.tokens.span.Span' object. ```. Running spacy 3.0, the latest version, and on Windows 10.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:283,testability,test,test,283,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:556,testability,test,test,556,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:927,testability,Trace,Traceback,927,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1852,testability,test,test,1852,"lves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1904,testability,test,test,1904,"eviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2268,testability,context,context,2268,"prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:2407,testability,context,context,2407,"_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3564,testability,Trace,Traceback,3564,"get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return msgpack.dumps(data, use_bin_type=True). File ""C:\Python38\lib\site-packages\srsly\m",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:720,usability,error,error,720,"Span is not serializable in abbreviations - figure out a better workaround; ```python. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). test = [""Spinal and bulbar muscular atrophy (SBMA) is an inherited motor neuron disease caused by the expansion of a polyglutamine tract within the androgen receptor (AR). SBMA can be caused by this easily.""]. print(""Abbreviation"", ""\t"", ""Definition""). for doc in nlp.pipe(test, n_process=4):. for abrv in doc._.abbreviations:. print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ```. Running that code leads to this. The error message doesn't make a lot of sense, It could be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:1768,usability,User,Users,1768," be because there are more processes than entries. If you remove `n_process` the solves the problem. ```. Abbreviation Definition. Abbreviation Definition. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""C:\Python38\lib\multiprocessing\spawn.py"", line 116, in spawn_main. exitcode = _main(fd, parent_sentinel). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 125, in _main. prepare(preparation_data). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 236, in prepare. _fixup_main_from_path(data['init_main_from_path']). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 287, in _fixup_main_from_path. main_content = runpy.run_path(main_path,. File ""C:\Python38\lib\runpy.py"", line 265, in run_path. return _run_module_code(code, init_globals, run_name,. File ""C:\Python38\lib\runpy.py"", line 97, in _run_module_code. _run_code(code, mod_globals, init_globals,. File ""C:\Python38\lib\runpy.py"", line 87, in _run_code. exec(code, run_globals). File ""C:\Users\alexd\Dropbox (UFL)\UFII_COVID19_RESEARCH_TOPICS\cord19\text_parsing_pipeline\test.py"", line 13, in <module>. for doc in nlp.pipe(test, n_process=4):. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1475, in pipe. for doc in docs:. File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1511, in _multiprocessing_pipe. proc.start(). File ""C:\Python38\lib\multiprocessing\process.py"", line 121, in start. self._popen = self._Popen(self). File ""C:\Python38\lib\multiprocessing\context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"",",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3291,usability,error,error,3291,"_Popen. return _default_context.get_context().Process._Popen(process_obj). File ""C:\Python38\lib\multiprocessing\context.py"", line 327, in _Popen. return Popen(process_obj). File ""C:\Python38\lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[k",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/338:3483,usability,error,error,3483,"lib\multiprocessing\popen_spawn_win32.py"", line 45, in __init__. prep_data = spawn.get_preparation_data(process_obj._name). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 154, in get_preparation_data. _check_not_importing_main(). File ""C:\Python38\lib\multiprocessing\spawn.py"", line 134, in _check_not_importing_main. raise RuntimeError('''. RuntimeError:. An attempt has been made to start a new process before the. current process has finished its bootstrapping phase. This probably means that you are not using fork to start your. child processes and you have forgotten to use the proper idiom. in the main module:. if __name__ == '__main__':. freeze_support(). ... The ""freeze_support()"" line can be omitted if the program. is not going to be frozen to produce an executable. ```. This is the error message from my main piece of code with more data. It sort of makes more sense. I think it has to something to do with how the multiprocess pipe collects the results of the workers. The error pops up after a while so it's definitely running. ```. Process Process-1:. Traceback (most recent call last):. File ""C:\Python38\lib\multiprocessing\process.py"", line 315, in _bootstrap. self.run(). File ""C:\Python38\lib\multiprocessing\process.py"", line 108, in run. self._target(*self._args, **self._kwargs). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in _apply_pipes. sender.send([doc.to_bytes() for doc in docs]). File ""C:\Python38\lib\site-packages\spacy\language.py"", line 1995, in <listcomp>. sender.send([doc.to_bytes() for doc in docs]). File ""spacy\tokens\doc.pyx"", line 1237, in spacy.tokens.doc.Doc.to_bytes. File ""spacy\tokens\doc.pyx"", line 1296, in spacy.tokens.doc.Doc.to_dict. File ""C:\Python38\lib\site-packages\spacy\util.py"", line 1134, in to_dict. serialized[key] = getter(). File ""spacy\tokens\doc.pyx"", line 1293, in spacy.tokens.doc.Doc.to_dict.lambda18. File ""C:\Python38\lib\site-packages\srsly\_msgpack_api.py"", line 14, in msgpack_dumps. return ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338
https://github.com/allenai/scispacy/issues/339:291,modifiability,variab,variable,291,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:398,security,token,token,398,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:405,security,Token,Token,405,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:438,security,token,token,438,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:515,security,token,token,515,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:644,security,token,token,644,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:476,testability,simpl,simple,476,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/339:476,usability,simpl,simple,476,"Potential IndexError in hyponym_detector.py; I've noticed there's potentially unbounded index increment/decrement inside while cycles of the expand_to_noun_compound method of [HyponymDetector](https://github.com/allenai/scispacy/blob/master/scispacy/hyponym_detector.py). Also I believe the variable name inside the second cycle should be next, not previous. ```. def expand_to_noun_compound(self, token: Token, doc: Doc):. """""". Expand a token to it's noun phrase based. on a simple POS tag heuristic. """""". start = token.i. while True:. previous = doc[start - 1]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. start -= 1. else:. break. end = token.i + 1. while True:. previous = doc[end]. if previous.pos_ in {""PROPN"", ""NOUN"", ""PRON""}:. end += 1. else:. break. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/339
https://github.com/allenai/scispacy/issues/340:257,deployability,deploy,deploy,257,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:360,deployability,deploy,deploy,360,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:29,energy efficiency,model,model,29,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:482,energy efficiency,load,load,482,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:569,energy efficiency,load,load,569,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:85,modifiability,pac,package,85,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:482,performance,load,load,482,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:569,performance,load,load,569,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:57,reliability,doe,doesn,57,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:98,safety,valid,valid,98,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:498,safety,except,exception,498,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/issues/340:29,security,model,model,29,"OSError at [E050] Can't find model 'en_core_sci_sm '. It doesn't seem to be a Python package or a valid path to a data directory.; This is somewhat related to issue# 337. When I run my code in the debugger, everything works perfectly. When I use Jenkins to deploy/dockerize my application on an Ubuntu server, everything works perfectly. When I use Jenkins to deploy/dockerize my application on my Windows 10 laptop, everything was working perfectly until it wasn't. The line spacy.load throws the exception:. ```. _scispacy_data_model = ""en_core_sci_sm"". _nlp = spacy.load(_scispacy_data_model). ```. Thoughts or suggestions?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/340
https://github.com/allenai/scispacy/pull/341:23,availability,error,error,23,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/pull/341:23,performance,error,error,23,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/pull/341:8,safety,detect,detector,8,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/pull/341:23,safety,error,error,23,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/pull/341:8,security,detect,detector,8,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/pull/341:23,usability,error,error,23,Hyponym detector index error fix; Fix to a problem described in this issue: https://github.com/allenai/scispacy/issues/339,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/341
https://github.com/allenai/scispacy/issues/342:0,deployability,Updat,Update,0,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:16,deployability,version,version,16,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:837,deployability,depend,dependencies,837,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:960,deployability,releas,releases,960,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:523,energy efficiency,model,model,523,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:563,energy efficiency,load,load,563,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:568,energy efficiency,model,model,568,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:16,integrability,version,version,16,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:837,integrability,depend,dependencies,837,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:252,interoperability,cot,cotransporter,252,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:636,interoperability,cot,cotransporter,636,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:1442,interoperability,specif,specific,1442,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:1498,interoperability,cooperat,cooperative,1498,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:1514,interoperability,bind,binding,1514,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:16,modifiability,version,version,16,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:837,modifiability,depend,dependencies,837,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:1514,modifiability,bind,binding,1514,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:563,performance,load,load,563,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:0,safety,Updat,Update,0,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:87,safety,input,input,87,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:837,safety,depend,dependencies,837,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:0,security,Updat,Update,0,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:523,security,model,model,523,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:568,security,model,model,568,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:218,testability,unit,unit,218,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:602,testability,unit,unit,602,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:837,testability,depend,dependencies,837,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:87,usability,input,input,87,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/342:372,usability,user,user-images,372,"Update scispacy version on streamlit demo; I am getting different results for the same input text when I use the streamlit demo vs. when I run the code locally. The text in question:. ```python. text = ""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer."". ```. NER results using `""en_ner_jnlpba_md""` on streamlit demo. ![image](https://user-images.githubusercontent.com/8917831/112166886-6d643000-8bc6-11eb-9030-d2b23c69768d.png). Then, running things locally:. ```python. import spacy. model = ""en_ner_jnlpba_md"". nlp = spacy.load(model). doc = nlp(""The structural unit of the secretory Na+-K+-2Cl- cotransporter (NKCC1) is a homodimer.""). for ent in doc.ents:. print(f""{ent.text}\t{ent.label_}""). """""". The above prints:. NKCC1	DNA. homodimer	PROTEIN. """""". ```. My `pyproject.toml` has the following dependencies:. ```toml. scispacy = ""^0.4.0"". en_ner_jnlpba_md = {url = ""https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_jnlpba_md-0.4.0.tar.gz""}. ```. Any idea what might be causing this? I consider the streamlit demo response to be more correct, and am interesting in getting the same result locally! ---. Also, I am only showing one example here, but I found I could quickly come up with other examples where the streamlit demo specialized NER results were better (IMO) than the results I got locally. A second example is:. ```python. text = ""Fourteen residues of the U1 snRNP-specific U1A protein are required for homodimerization, cooperative RNA binding, and inhibition of polyadenylation."". ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342
https://github.com/allenai/scispacy/issues/343:29,availability,down,down,29,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:213,availability,down,downloading,213,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:465,availability,down,download,465,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:34,deployability,contain,container,34,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:94,deployability,contain,container,94,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:350,deployability,fail,fails,350,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:374,deployability,contain,container,374,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:404,integrability,configur,configure,404,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:64,modifiability,pac,package,64,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:404,modifiability,configur,configure,404,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:22,performance,lock,locked-down,22,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:137,performance,network,network,137,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:165,performance,network,network,165,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:229,performance,cach,cache,229,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:442,performance,cach,cached,442,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:350,reliability,fail,fails,350,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:181,safety,test,test,181,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:22,security,lock,locked-down,22,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:123,security,access,access,123,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:137,security,network,network,137,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:165,security,network,network,165,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:404,security,configur,configure,404,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/343:181,testability,test,test,181,"Running scispacy in a locked-down container; Hi,. I'm trying to package and run scispacy on a container which may not have access to the network/Internet (passing --network=none to test). It seems that even after downloading the cache files and seeding ~/.scispacy/datasets, it still attempts to pull the files (tfidf_vectors, etc) from S3 (and thus fails on a disconnected container). Is there a way to configure scispacy to use the locally cached files and _not_ download them on every initialization? Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/343
https://github.com/allenai/scispacy/issues/344:794,deployability,stack,stackoverflow,794,what controls the order of UMLS linked entities from scispacy if the scores are all 1; i'm using Scispacy (which is awesome!) but when I type 'tau' into the app found here https://scispacy.apps.allenai.org/ the UMLS entity gives me the canonical name of 'MAPT gene' which is what I want. But when I do the exact same thing in my python code based on the app code (see here https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42) the first canonical name on the list is 'uridine triacetate' (the second is 'MAPT gene'). in the app code there is the call 'if show_only_top:break' so I assume somehow their app implementation orders the linked entities differently. if someone can explain the difference in ordering and how to fix that would be great thanks!! PS this is also posted on stackoverflow - before I thought to post here!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/344:5,security,control,controls,5,what controls the order of UMLS linked entities from scispacy if the scores are all 1; i'm using Scispacy (which is awesome!) but when I type 'tau' into the app found here https://scispacy.apps.allenai.org/ the UMLS entity gives me the canonical name of 'MAPT gene' which is what I want. But when I do the exact same thing in my python code based on the app code (see here https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42) the first canonical name on the list is 'uridine triacetate' (the second is 'MAPT gene'). in the app code there is the call 'if show_only_top:break' so I assume somehow their app implementation orders the linked entities differently. if someone can explain the difference in ordering and how to fix that would be great thanks!! PS this is also posted on stackoverflow - before I thought to post here!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/344:5,testability,control,controls,5,what controls the order of UMLS linked entities from scispacy if the scores are all 1; i'm using Scispacy (which is awesome!) but when I type 'tau' into the app found here https://scispacy.apps.allenai.org/ the UMLS entity gives me the canonical name of 'MAPT gene' which is what I want. But when I do the exact same thing in my python code based on the app code (see here https://gist.github.com/DeNeutoy/b20860b40b9fa9d33675893c56afde42) the first canonical name on the list is 'uridine triacetate' (the second is 'MAPT gene'). in the app code there is the call 'if show_only_top:break' so I assume somehow their app implementation orders the linked entities differently. if someone can explain the difference in ordering and how to fix that would be great thanks!! PS this is also posted on stackoverflow - before I thought to post here!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/344
https://github.com/allenai/scispacy/issues/345:69,availability,error,error,69,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1908,availability,error,error,1908," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:2010,availability,error,error,2010," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:804,deployability,modul,module,804,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:510,energy efficiency,load,load,510,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1681,energy efficiency,reduc,reduction,1681," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1750,energy efficiency,reduc,reduction,1750," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1804,integrability,protocol,protocol,1804," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1804,interoperability,protocol,protocol,1804," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:804,modifiability,modul,module,804,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:880,modifiability,pac,packages,880,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:972,modifiability,pac,packages,972,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:69,performance,error,error,69,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:510,performance,load,load,510,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1908,performance,error,error,1908," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:2010,performance,error,error,2010," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:14,reliability,doe,doesn,14,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:176,reliability,doe,does,176,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:288,reliability,doe,does,288,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:69,safety,error,error,69,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:804,safety,modul,module,804,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1908,safety,error,error,1908," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:2010,safety,error,error,2010," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:734,testability,Trace,Traceback,734,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1190,testability,context,context,1190," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1327,testability,context,context,1327," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:69,usability,error,error,69,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:196,usability,interact,interacts,196,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:315,usability,Minim,Minimal,315,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:434,usability,document,document,434,"rxnorm linker doesn't work with multiprocessing?; Hi, I'm getting an error trying to run `nlp.pipe` with `n_processes > 1`, I think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:1908,usability,error,error,1908," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/345:2010,usability,error,error,2010," think because the pickling that `multiprocessing` does under the hood interacts poorly with `nmslib.dist.FloatIndex`, which the rxnorm entity linker requires and does not seem picklable. . Minimal code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. TEXTS = [""Hello! This is document 1."", ""And here's doc 2.""]. if __name__ == '__main__':. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""rxnorm""}). for doc in nlp.pipe(TEXTS, n_process=2):. print(doc). ```. Running with Python 3.8.5 gives me:. ```. Traceback (most recent call last):. File ""./mwerror.py"", line 13, in <module>. for doc in nlp.pipe(TEXTS, n_process=2):. File "".../python3.8/site-packages/spacy/language.py"", line 1479, in pipe. for doc in docs:. File "".../python3.8/site-packages/spacy/language.py"", line 1515, in _multiprocessing_pipe. proc.start(). File "".../python3.8/multiprocessing/process.py"", line 121, in start. self._popen = self._Popen(self). File "".../python3.8/multiprocessing/context.py"", line 224, in _Popen. return _default_context.get_context().Process._Popen(process_obj). File "".../python3.8/multiprocessing/context.py"", line 284, in _Popen. return Popen(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 32, in __init__. super().__init__(process_obj). File "".../python3.8/multiprocessing/popen_fork.py"", line 19, in __init__. self._launch(process_obj). File "".../python3.8/multiprocessing/popen_spawn_posix.py"", line 47, in _launch. reduction.dump(process_obj, fp). File "".../python3.8/multiprocessing/reduction.py"", line 60, in dump. ForkingPickler(file, protocol).dump(obj). TypeError: cannot pickle 'nmslib.dist.FloatIndex' object. ```. Note I don't get an error with `n_process=1`, presumably because `multiprocessing` is not invoked. I also do not get this error if I don't include the linker pipe (i.e. comment out the `add_pipe()` line above). Thanks! This lib is great!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/345
https://github.com/allenai/scispacy/issues/346:248,deployability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:564,energy efficiency,cool,cool,564,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,integrability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:711,integrability,sub,subset,711,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:197,interoperability,ontolog,ontology,197,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,interoperability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,modifiability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:98,performance,time,time,98,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:106,performance,time,time,106,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,reliability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,security,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:248,testability,integr,integration,248,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/346:766,usability,help,help,766,"Add Wikidata as a possible EntityLinker?; Hello, . SciSpacy is awesome, I have been using it from time to time. I use [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) as a single go-to ontology, and it would be awesome to have Wikidata integration in the EntityLinker in sciSpacy. It would be super useful to have wikidata IDs as long as the UMLS IDs, for example. . I do not know the implementation details, so I do not know how much of a challenge it would be. Wikidata has +100M concepts, but only a fraction are relevant. It would already be super cool to have the 6.2M that are linked to English Wikipedia. . If there is something in the side of Wikidata that could be useful (e.g. making some subset of terms), just let me know and I would love to help! Best,. Tiago",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/346
https://github.com/allenai/scispacy/issues/347:8,availability,error,error,8,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:81,availability,error,error,81,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:396,availability,error,error,396,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:886,availability,Avail,Available,886,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:329,deployability,pipelin,pipeline,329,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:688,deployability,instal,install,688,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:264,energy efficiency,load,load,264,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:620,energy efficiency,current,current,620,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:329,integrability,pipelin,pipeline,329,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:576,integrability,compon,component,576,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:662,integrability,Transform,Transformer,662,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:703,integrability,transform,transformers,703,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:743,integrability,compon,component,743,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:802,integrability,compon,component,802,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:827,integrability,compon,components,827,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:873,integrability,compon,components,873,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:576,interoperability,compon,component,576,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:662,interoperability,Transform,Transformer,662,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:703,interoperability,transform,transformers,703,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:743,interoperability,compon,component,743,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:802,interoperability,compon,component,802,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:827,interoperability,compon,components,827,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:873,interoperability,compon,components,873,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:576,modifiability,compon,component,576,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:743,modifiability,compon,component,743,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:781,modifiability,deco,decorator,781,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:802,modifiability,compon,component,802,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:827,modifiability,compon,components,827,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:873,modifiability,compon,components,873,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:8,performance,error,error,8,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:81,performance,error,error,81,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:264,performance,load,load,264,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:396,performance,error,error,396,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:886,reliability,Availab,Available,886,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:8,safety,error,error,8,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:81,safety,error,error,81,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:396,safety,error,error,396,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:886,safety,Avail,Available,886,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:886,security,Availab,Available,886,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:8,usability,error,error,8,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:81,usability,error,error,81,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:396,usability,error,error,396,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:569,usability,custom,custom,569,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/347:736,usability,custom,custom,736,"Getting error on the use of 'abbreviation_detector' ; . Hello, . I am getting an error when I try to go through code snippets on abbreviations from the README file of the repo. ```. import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline. nlp.add_pipe(""abbreviation_detector""). ```. Below is the error i get: . . ValueError: [E002] Can't find factory for 'abbreviation_detector' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, en.lemmatizer",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/347
https://github.com/allenai/scispacy/issues/348:0,security,token,token,0,"token probabilities are not included; It seems the config isn't quite right to include token probabilities. I'm not 100% sure of the solution, but this issue should help (https://github.com/explosion/spaCy/discussions/6388#discussioncomment-330606). The vocab files include the probabilities, but I guess that isn't enough.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/348:87,security,token,token,87,"token probabilities are not included; It seems the config isn't quite right to include token probabilities. I'm not 100% sure of the solution, but this issue should help (https://github.com/explosion/spaCy/discussions/6388#discussioncomment-330606). The vocab files include the probabilities, but I guess that isn't enough.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/348:165,usability,help,help,165,"token probabilities are not included; It seems the config isn't quite right to include token probabilities. I'm not 100% sure of the solution, but this issue should help (https://github.com/explosion/spaCy/discussions/6388#discussioncomment-330606). The vocab files include the probabilities, but I guess that isn't enough.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/348
https://github.com/allenai/scispacy/issues/349:0,deployability,Instal,Installing,0,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:17,deployability,version,versions,17,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:139,deployability,version,version,139,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:170,deployability,instal,install,170,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:184,deployability,version,versions,184,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:62,energy efficiency,model,models,62,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:233,energy efficiency,model,models,233,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:17,integrability,version,versions,17,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:139,integrability,version,version,139,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:184,integrability,version,versions,184,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:253,interoperability,compatib,compatible,253,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:17,modifiability,version,versions,17,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:139,modifiability,version,version,139,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:184,modifiability,version,versions,184,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:62,security,model,models,62,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/349:233,security,model,models,233,"Installing older versions of scispacy and associated language models?; I'm interested in using scispacy with prodigy, which requires spaCy version 2.2. Is there a way to install older versions of scispacy and the assocaited language models that will be compatible?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/349
https://github.com/allenai/scispacy/issues/350:602,availability,error,error,602,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:973,availability,Error,Errors,973,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1137,availability,error,error,1137,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1166,availability,error,error,1166,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1202,availability,sli,slice,1202,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1293,availability,error,error,1293,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1342,availability,error,error,1342,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:62,deployability,upgrad,upgraded,62,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:258,deployability,instal,install,258,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:296,deployability,version,version,296,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:370,deployability,instal,install,370,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:394,deployability,instal,install,394,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:453,deployability,releas,releases,453,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1088,deployability,contain,contained,1088,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:167,energy efficiency,model,model,167,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:201,energy efficiency,model,model,201,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:536,energy efficiency,load,load,536,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:659,energy efficiency,model,model,659,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:804,energy efficiency,model,model,804,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:866,energy efficiency,model,model,866,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:881,energy efficiency,model,model,881,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:296,integrability,version,version,296,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:586,integrability,filter,filterwarnings,586,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:62,modifiability,upgrad,upgraded,62,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:296,modifiability,version,version,296,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:744,modifiability,pac,packages,744,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:766,modifiability,layer,layers,766,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:536,performance,load,load,536,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:602,performance,error,error,602,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:973,performance,Error,Errors,973,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1137,performance,error,error,1137,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1166,performance,error,error,1166,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1293,performance,error,error,1293,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1342,performance,error,error,1342,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1202,reliability,sli,slice,1202,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:602,safety,error,error,602,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:640,safety,input,input,640,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:931,safety,except,except,931,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:973,safety,Error,Errors,973,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1137,safety,error,error,1137,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1166,safety,error,error,1166,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1293,safety,error,error,1293,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1342,safety,error,error,1342,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:167,security,model,model,167,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:201,security,model,model,201,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:659,security,model,model,659,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:804,security,model,model,804,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:866,security,model,model,866,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:881,security,model,model,881,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1279,testability,simpl,simplefilter,1279,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:602,usability,error,error,602,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:640,usability,input,input,640,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:973,usability,Error,Errors,973,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1137,usability,error,error,1137,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1166,usability,error,error,1166,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1279,usability,simpl,simplefilter,1279,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1293,usability,error,error,1293,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/350:1342,usability,error,error,1342,"DeprecationWarning from `spacy_legacy`; Hi there,. I recently upgraded to spacy 3 and scispacy 0.4, but I am now getting a warning whenever I use the small `scispacy` model (I have not tried any other model). . I am getting a `DeprecationWarning` on a fresh install in python 3.8 with the latest version of `scispacy` and `en_core_sci_sm`. ### Steps to reproduce:. `pip install scispacy`. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz`. ```python. import spacy. nlp = spacy.load(""en_core_sci_sm""). import warnings. warnings.filterwarnings(""error""). nlp(""Hello World""). ```. Any input to the `nlp` model triggers the same warning:. ```. /opt/miniconda3/envs/clean/lib/python3.8/site-packages/spacy_legacy/layers/staticvectors_v1.py in forward(model, docs, is_train). 43 ). 44 try:. ---> 45 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 46 except ValueError:. 47 raise RuntimeError(Errors.E896). DeprecationWarning: Out of bound index found. . This was previously ignored when the indexing result contained no elements. . In the future the index error will be raised. . This error occurs either due to an empty slice, or if an array has zero elements even before indexing. (Use `warnings.simplefilter('error')` to turn this DeprecationWarning into an error and get more details on the invalid index.). ```. Any ideas as to how to resolve this without manually ignoring the warning?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/350
https://github.com/allenai/scispacy/issues/351:394,availability,down,downloading,394,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:908,availability,down,downloading,908,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:58,deployability,instal,installed,58,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:80,deployability,contain,container,80,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:94,deployability,instal,install,94,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:128,deployability,instal,install,128,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:187,deployability,releas,releases,187,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:285,deployability,instal,installed,285,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:104,energy efficiency,model,model,104,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:901,performance,cach,cache,901,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:759,safety,avoid,avoid,759,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:104,security,model,model,104,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/351:384,security,access,access,384,"How can all the Entity linkers (RXNORM, UMLS, etc) be pre-installed in a Docker container; To install a model I can do `RUN pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_md-0.4.0.tar.gz` inside of a `Dockerfile`. Can the files below be pre-installed in any manner (e.g. `wget http://XYZ -O ABC`)? I will have a runtime environment without access to downloading linkers at runtime. So it would be great to have all things in place in `Dockerfile` to execute something like `nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True,. ""linker_name"": ""umls""})`. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43-L49. i am attempting to avoid the following -. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz not found in cache, downloading to /tmp/ `",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351
https://github.com/allenai/scispacy/issues/352:402,availability,servic,service,402,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:402,deployability,servic,service,402,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:774,deployability,observ,observable,774,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1377,deployability,observ,observable,1377,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:0,integrability,Filter,Filtering,0,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:402,integrability,servic,service,402,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:2106,interoperability,share,share,2106,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:402,modifiability,servic,service,402,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:2117,safety,input,inputs,2117,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1225,security,Soc,Socially-constructed,1225,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1246,security,ident,identity,1246,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:186,testability,context,context,186,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:293,testability,context,context,293,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:774,testability,observ,observable,774,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1377,testability,observ,observable,1377,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:305,usability,help,help,305,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:910,usability,Behavi,Behavior,910,"Filtering Entity Linking candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.999999940395355",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1020,usability,behavi,behaviour,1020,"candidates based on sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1038,usability,behavi,behaviors,1038,"n sentence similarity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was t",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1056,usability,behavi,behavior,1056,"rity; Hi, thanks for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of con",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1073,usability,Behavi,Behavior,1073,"for developing such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1087,usability,behavi,behavior,1087," such an awesome library. I wanted to know if there is way to generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be grea",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:1148,usability,behavi,behaviour,1148,"generate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! V",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/352:2117,usability,input,inputs,2117,"erate candidates based on the context information. In certain cases, the distance seems to be same for all possible candidates where the context may help describing the nearest candidate. For Example-. ```. for x in nlp(""date of birth: sex: male service: micu and then ..."").ents[1]._.kb_ents:. print(linker.kb.cui_to_entity[x[0]],x[1],sep=""\n\n\n""). print(). ```. Gives output:. ```. CUI: C0009253, Name: Coitus. Definition: The sexual union of a male and a female, a term used for human only. TUI(s): T040. Aliases (abbreviated, total: 21): . 	 vaginal intercourse, Sexual act, NOS, SEXUAL INTERCOURSE, Intercourse (observable entity), making love, Sex, sexual act, sexual intercourse, Coitus, NOS, coitus. 0.9999999403953552. CUI: C0036864, Name: Sex Behavior. Definition: Sexual activities of humans. TUI(s): T053. Aliases (abbreviated, total: 29): . 	 sexual behaviour, sexual behaviors, Sexual behavior, Sexual Behavior, sex behavior, sexuality, Sexually, Activity, Sexual, Sex, Sexual behaviour, NOS. 0.9999999403953552. CUI: C0079399, Name: Gender. Definition: Socially-constructed identity of male or female. TUI(s): T032. Aliases: (total: 7): . 	 gender, Gender, individual sex, sex (gender), gendered, Gender (observable entity), sex. 0.9999999403953552. CUI: C1314687, Name: Sexual intercourse - finding. Definition: None. TUI(s): T033. Aliases (abbreviated, total: 12): . 	 Sexual act, Biological sex (property), Biological sex, Coitus, Finding of sexual intercourse (finding), Sexual intercourse - finding, Finding of sexual intercourse, Sex, Vaginal intercourse, SI - Sexual intercourse. 0.9999999403953552. CUI: C1418662, Name: PLXNA3 gene. Definition: None. TUI(s): T028. Aliases (abbreviated, total: 11): . 	 XAP-6, Plxn3, PLXN4, PLXNA3, plexin A3, SEX, 6.3, TRANSMEMBRANE PROTEIN SEX, PLXNA3 gene, PLEXIN 4. 0.9999999403953552. ```. One possible way I thought of was to make use of concept definitions. Would be great if you could share your inputs on this! Thanks in Advance! Vasu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352
https://github.com/allenai/scispacy/issues/353:143,interoperability,specif,specifically,143,"Word Embedding Implementation; Hi,. Thanks for this post. Does the ""doc.similarity(...)"" function use the word vector from Spacy (which is not specifically trained on biomedical text at all)? In other words, is there any difference between calling ""doc.similarity(...)"" via Spacy or SciSpacy? Thanks a lot for your attention,. Yifu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/353:58,reliability,Doe,Does,58,"Word Embedding Implementation; Hi,. Thanks for this post. Does the ""doc.similarity(...)"" function use the word vector from Spacy (which is not specifically trained on biomedical text at all)? In other words, is there any difference between calling ""doc.similarity(...)"" via Spacy or SciSpacy? Thanks a lot for your attention,. Yifu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/353
https://github.com/allenai/scispacy/issues/354:275,deployability,instal,installation,275,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:466,deployability,instal,installation,466,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:406,testability,simpl,simple,406,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:406,usability,simpl,simple,406,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:556,usability,help,help,556,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:648,usability,user,user-images,648,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/354:818,usability,user,user-images,818,"The online demo and local results are different in Specialized NER; Hello. I have entered the following sentence:. ""Endohydrolysis of (14)--D-linkages between D-galactose 4-sulfate and 3,6-anhydro-D-galactose in -carrageenans"". in the online demo and tried it on my local installation (both with ""en_ner_bionlp13cg_md""). But the results are different for ""-carrageenans"". The online demo labels it as ""simple chemical"" (Figure 1), which is correct. But my local installation labels it as ""GENE_OR_GENE_PRODUCT"", which is not correct (Figure 2). Please help! Thank you. <img width=""1210"" alt=""Bildschirmfoto 2021-05-16 um 01 00 56"" src=""https://user-images.githubusercontent.com/1873196/118380486-5fd59000-b5e2-11eb-9f62-49f480245a91.png"">. <img width=""945"" alt=""Bildschirmfoto 2021-05-16 um 01 01 43"" src=""https://user-images.githubusercontent.com/1873196/118380489-65cb7100-b5e2-11eb-88c0-7f1d66d18262.png"">.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/354
https://github.com/allenai/scispacy/issues/355:319,energy efficiency,load,load,319,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:578,integrability,inject,injection,578,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:684,integrability,inject,injection,684,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:802,interoperability,specif,specific,802,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:319,performance,load,load,319,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:578,security,inject,injection,578,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:684,security,inject,injection,684,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/355:867,testability,understand,understand,867,"EntityLinker knowledge base returns CUIs not MeSH IDs when 'mesh' is selected; I'm using scispaCy entity linker using this snippet:. ```python. from scispacy.linking import EntityLinker. import spacy, scispacy. config = {. ""resolve_abbreviations"": True, . ""name"": ""mesh"", . ""max_entities_per_mention"":1. }. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""scispacy_linker"", config=config) . linker = nlp.get_pipe(""scispacy_linker""). def mesh_extractor(text):. doc = nlp(text). for e in doc.ents:. if e._.kb_ents:. cui = e._.kb_ents[0][0]. print(e, cui). text = ""Give him three injection of paracetamol"". ```. Then when I use it:. ```jupyter. >> mesh_extractor(text). Give C1947971. injection C0021485. ```. But, in the README of scispaCy, I see that for MeSH, it should not return UMLS CUIs, but the specific MeSH IDs (for example, D003435). How to fix this? Did I understand something badly?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/355
https://github.com/allenai/scispacy/issues/356:58,availability,error,error,58,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:177,availability,down,download,177,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:3,deployability,modul,module,3,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:114,deployability,Modul,ModuleNotFoundError,114,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:138,deployability,modul,module,138,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:3,modifiability,modul,module,3,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:114,modifiability,Modul,ModuleNotFoundError,114,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:138,modifiability,modul,module,138,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:58,performance,error,error,58,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:3,safety,modul,module,3,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:58,safety,error,error,58,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:114,safety,Modul,ModuleNotFoundError,114,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:138,safety,modul,module,138,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:58,usability,error,error,58,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/356:94,usability,help,help,94,"No module named 'scispacy.umls_utils; Hello,. I have this error and I don't know why, can you help me please ? . ""ModuleNotFoundError: No module named 'scispacy.umls_utils'"". I download everything, and I have the umls_utils.py on scispacy. Thanks,. Cheers",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/356
https://github.com/allenai/scispacy/issues/357:2531,availability,error,error,2531,"ne 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2728,availability,error,error,2728,"p_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_pat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5123,availability,down,downloading,5123,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:754,deployability,instal,install,754,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:787,deployability,instal,install,787,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3191,deployability,modul,module,3191,"he above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. retu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4162,deployability,api,api,4162," ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/li",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4303,deployability,api,api,4303,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:376,energy efficiency,load,load,376,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:624,energy efficiency,CPU,CPU,624,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2353,energy efficiency,adapt,adapters,2353,"ges/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4651,energy efficiency,adapt,adapter,4651,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4758,energy efficiency,adapt,adapters,4758,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2150,integrability,protocol,protocol,2150,"xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call la",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2353,integrability,adapt,adapters,2353,"ges/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3027,integrability,protocol,protocol,3027,"hon3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4162,integrability,api,api,4162," ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/li",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4303,integrability,api,api,4303,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4651,integrability,adapt,adapter,4651,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4758,integrability,adapt,adapters,4758,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5090,integrability,protocol,protocol,5090,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:28,interoperability,prox,proxy,28,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:288,interoperability,prox,proxy,288,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:343,interoperability,prox,proxy,343,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:523,interoperability,Prox,Proxy,523,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:561,interoperability,Prox,Proxy,561,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2150,interoperability,protocol,protocol,2150,"xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call la",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2353,interoperability,adapt,adapters,2353,"ges/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3027,interoperability,protocol,protocol,3027,"hon3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4162,interoperability,api,api,4162," ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/li",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4303,interoperability,api,api,4303,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4651,interoperability,adapt,adapter,4651,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4758,interoperability,adapt,adapters,4758,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5090,interoperability,protocol,protocol,5090,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5238,interoperability,Prox,Proxy,5238,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:913,modifiability,pac,packages,913,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1055,modifiability,pac,packages,1055,"r UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1193,modifiability,pac,packages,1193,"ispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1352,modifiability,pac,packages,1352,"5"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/ada",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1500,modifiability,pac,packages,1500,"e is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1678,modifiability,pac,packages,1678,"d to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2335,modifiability,pac,packages,2335,"thon3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2353,modifiability,adapt,adapters,2353,"ges/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2459,modifiability,pac,packages,2459,"/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candida",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2639,modifiability,pac,packages,2639,"cispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_genera",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3191,modifiability,modul,module,3191,"he above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. retu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3311,modifiability,pac,packages,3311,"/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", lin",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3438,modifiability,pac,packages,3438,"b/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3609,modifiability,pac,packages,3609,"ci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3806,modifiability,pac,packages,3806,"ool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, r",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3972,modifiability,pac,packages,3972,"Error(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4144,modifiability,pac,packages,4144,"all last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4285,modifiability,pac,packages,4285,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4447,modifiability,pac,packages,4447,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4596,modifiability,pac,packages,4596,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4651,modifiability,adapt,adapter,4651,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4740,modifiability,pac,packages,4740,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4758,modifiability,adapt,adapters,4758,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:376,performance,load,load,376,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:624,performance,CPU,CPU,624,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:733,performance,perform,performance,733,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2386,performance,time,timeout,2386,"506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/d",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2394,performance,time,timeout,2394,"_connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2531,performance,error,error,2531,"ne 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2728,performance,error,error,2728,"p_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_pat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2203,safety,except,exception,2203,"3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker =",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2222,safety,except,exception,2222,"ine 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(r",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2386,safety,timeout,timeout,2386,"506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/d",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2394,safety,timeout,timeout,2394,"_connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2531,safety,error,error,2531,"ne 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2728,safety,error,error,2728,"p_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_pat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2768,safety,except,exceptions,2768,"ket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/sci",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3084,safety,except,exception,3084,"o_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3103,safety,except,exception,3103,"SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3191,safety,modul,module,3191,"he above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. retu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4836,safety,except,exceptions,4836,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1562,security,soc,sock,1562,"xy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1777,security,soc,sock,1777,"es. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1856,security,ssl,ssl,1856,"ile ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1891,security,session,session,1891,"ib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1899,security,session,session,1899,"n3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scis",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1947,security,ssl,ssl,1947," line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vector",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2038,security,ssl,ssl,2038,"python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ss",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2104,security,ssl,ssl,2104,"prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exc",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2108,security,SSL,SSLEOFError,2108,"proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception o",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2973,security,SSL,SSLError,2973," self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packa",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2982,security,SSL,SSLEOFError,2982,"ndshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4340,security,session,session,4340,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4465,security,session,sessions,4465,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4614,security,session,sessions,4614,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4797,security,SSL,SSLError,4797,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:4847,security,SSL,SSLError,4847,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5036,security,SSL,SSLError,5036,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:5045,security,SSL,SSLEOFError,5045,"e/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 104, in head. return request('head', url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/api.py"", line 61, in request. return session.request(method=method, url=url, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 542, in request. resp = self.send(prep, **send_kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/sessions.py"", line 655, in send. r = adapter.send(request, **kwargs). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 514, in send. raise SSLError(e, request=request). requests.exceptions.SSLError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). ```. downloading the ""/ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz"" using pip and the defined Proxy is done without any issue.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:821,testability,Trace,Traceback,821,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:1568,testability,context,context,1568,"nds for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2243,testability,Trace,Traceback,2243,"conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:3124,testability,Trace,Traceback,3124,"rred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). File ""/data/home/xx/scispacy/sci-env/lib/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:567,usability,command,commands,567,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:628,usability,support,supports,628,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:733,usability,perform,performance,733,"UmlsEntityLinker issue with proxy?; I am using this code for UmlsEntityLinker:. ```. import scispacy. import spacy. #import displacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. import os. os.environ['http_proxy'] = ""http://proxy.xxx:5555"" . os.environ['https_proxy'] = ""https://proxy.xxx:5555"" . #. nlp = spacy.load('en_core_sci_sm'). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). ```. However, it seems there is an issue with Proxy even though I am using the same Proxy commands for other tasks without any problem:. ```. Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2. For maximum performance, you can install NMSLIB from sources. pip install --no-binary :all: nmslib. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 696, in urlopen. self._prepare_proxy(conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 964, in _prepare_proxy. conn.connect(). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 359, in connect. conn = self._connect_tls_proxy(hostname, conn). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connection.py"", line 506, in _connect_tls_proxy. ssl_context=ssl_context,. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). F",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2531,usability,error,error,2531,"ne 429, in ssl_wrap_socket. sock, context, tls_in_tls, server_hostname=server_hostname. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/ssl_.py"", line 472, in _ssl_wrap_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/357:2728,usability,error,error,2728,"p_socket_impl. return ssl_context.wrap_socket(sock, server_hostname=server_hostname). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 412, in wrap_socket. session=session. File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 850, in _create. self.do_handshake(). File ""/home/xx/anaconda3/lib/python3.7/ssl.py"", line 1108, in do_handshake. self._sslobj.do_handshake(). ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:1045). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send. timeout=timeout. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 756, in urlopen. method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/urllib3/util/retry.py"", line 574, in increment. raise MaxRetryError(_pool, url, error or ResponseError(cause)). urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/linkers/2020-10-09/umls/tfidf_vectors_sparse.npz (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1045)'))). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""linker.py"", line 12, in <module>. linker = UmlsEntityLinker(resolve_abbreviations=True). File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/linking.py"", line 85, in __init__. name=linker_name. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 223, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/data/home/xx/scispacy/sci-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 133, in load_approximate_nearest_neighbours_index. cached_path(linker_pat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357
https://github.com/allenai/scispacy/issues/358:188,performance,perform,performance,188,"Is there any difference between the returned entities with the same score?; I just noticed that if I shuffle the returned entities by UMLS entity linker and then choose the max score, the performance will be significantly decreased based on MedMentions dataset. . I've been thinking if you're considering any constraints/rules/... when ordering the entities with equally high score? For example, ""22q11.2 Deletion Syndrome"" being mapped to [**('C0012236', 1.0), ('C0220704', 1.0), ('C1321551', 1.0)**, ('C4518343', 0.8799166679382324), ('C2936346', 0.8731739521026611)], I just noticed that in most cases the first item is more relevant than others with equal score.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/358:208,security,sign,significantly,208,"Is there any difference between the returned entities with the same score?; I just noticed that if I shuffle the returned entities by UMLS entity linker and then choose the max score, the performance will be significantly decreased based on MedMentions dataset. . I've been thinking if you're considering any constraints/rules/... when ordering the entities with equally high score? For example, ""22q11.2 Deletion Syndrome"" being mapped to [**('C0012236', 1.0), ('C0220704', 1.0), ('C1321551', 1.0)**, ('C4518343', 0.8799166679382324), ('C2936346', 0.8731739521026611)], I just noticed that in most cases the first item is more relevant than others with equal score.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/358:188,usability,perform,performance,188,"Is there any difference between the returned entities with the same score?; I just noticed that if I shuffle the returned entities by UMLS entity linker and then choose the max score, the performance will be significantly decreased based on MedMentions dataset. . I've been thinking if you're considering any constraints/rules/... when ordering the entities with equally high score? For example, ""22q11.2 Deletion Syndrome"" being mapped to [**('C0012236', 1.0), ('C0220704', 1.0), ('C1321551', 1.0)**, ('C4518343', 0.8799166679382324), ('C2936346', 0.8731739521026611)], I just noticed that in most cases the first item is more relevant than others with equal score.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/358
https://github.com/allenai/scispacy/issues/359:191,availability,avail,available,191,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:111,deployability,releas,release,111,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:257,deployability,releas,release,257,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:191,reliability,availab,available,191,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:191,safety,avail,available,191,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:191,security,availab,available,191,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/359:133,usability,support,supports,133,"SciBERT; May I ask you to provide more information about ""en_core_sci_scibert"" please? According to the latest release note, it only supports parsing and POS tagging for now. When it will be available for NER and doc.vector, etc. Is there any branch or pre-release that I can follow?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/359
https://github.com/allenai/scispacy/issues/360:93,energy efficiency,model,model,93,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:60,performance,perform,performing,60,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:136,reliability,doe,does,136,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:93,security,model,model,93,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:112,security,ident,identifies,112,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:60,usability,perform,performing,60,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/360:251,usability,user,user-images,251,"SciBERT not returning the proper entity label for NER; When performing NER using the SciBERT model, it properly identifies entities but does not apply the proper label (CANCER, GENE, etc). All entities are marked as ENTITY. Example:. ![image](https://user-images.githubusercontent.com/11846453/119068075-73ea0a80-b9b1-11eb-9ce6-09c351cd0726.png)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/360
https://github.com/allenai/scispacy/issues/361:94,deployability,version,version,94,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:231,deployability,modul,module,231,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:343,deployability,version,version,343,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:593,deployability,version,version,593,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:632,deployability,version,version,632,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:364,energy efficiency,load,loads,364,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:384,energy efficiency,cloud,cloud,384,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:579,energy efficiency,load,load,579,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:624,energy efficiency,current,current,624,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:94,integrability,version,version,94,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:343,integrability,version,version,343,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:593,integrability,version,version,593,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:632,integrability,version,version,632,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:94,modifiability,version,version,94,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:231,modifiability,modul,module,231,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:343,modifiability,version,version,343,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:593,modifiability,version,version,593,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:632,modifiability,version,version,632,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:364,performance,load,loads,364,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:579,performance,load,load,579,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:567,reliability,pra,practice,567,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:231,safety,modul,module,231,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:540,testability,understand,understand,540,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:16,usability,custom,custom,16,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:158,usability,help,help,158,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:529,usability,help,help,529,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/361:586,usability,custom,custom,586,How to define a custom knowledge base for Entity Linker ?; I'm going to leverage an augmented version of UMLS (by adding new aliases from external sources to help the algorithm to match better). It seems that [candidate generation module](https://github.com/allenai/scispacy/blob/main/scispacy/candidate_generation.py) leverages a pre-indexed version of UMLS and [loads them from the cloud](https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L43). May I ask you to help me to understand what's the best practice to load a custom version of UMLS or augment the current version with new aliases please?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/361
https://github.com/allenai/scispacy/issues/362:14,interoperability,specif,specific,14,"Selecting for specific UMLS sources (vocabularies); The UMLS knowledge base used in the example scispacy snippet seems to be using multiple UMLS vocabularies. What would be the best method to select for and use only a specific UMLS vocabulary? One idea I've had would be by providing a list of sources similar to what is defined here (https://github.com/allenai/scispacy/blob/main/scispacy/umls_utils.py#L7), but still have not figured out how to make the change to the EntityLinker class object for 'scispacy_linker'.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/362:218,interoperability,specif,specific,218,"Selecting for specific UMLS sources (vocabularies); The UMLS knowledge base used in the example scispacy snippet seems to be using multiple UMLS vocabularies. What would be the best method to select for and use only a specific UMLS vocabulary? One idea I've had would be by providing a list of sources similar to what is defined here (https://github.com/allenai/scispacy/blob/main/scispacy/umls_utils.py#L7), but still have not figured out how to make the change to the EntityLinker class object for 'scispacy_linker'.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/362
https://github.com/allenai/scispacy/issues/363:18,deployability,version,version,18,"Demo and offiline version of Scispacy gives different results .; For example (en_core_sci_lg) gives a different result for text ""I am looking for Surgery of Appendicitis by allopathy treatment."" with UMLS linker",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/363
https://github.com/allenai/scispacy/issues/363:18,integrability,version,version,18,"Demo and offiline version of Scispacy gives different results .; For example (en_core_sci_lg) gives a different result for text ""I am looking for Surgery of Appendicitis by allopathy treatment."" with UMLS linker",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/363
https://github.com/allenai/scispacy/issues/363:18,modifiability,version,version,18,"Demo and offiline version of Scispacy gives different results .; For example (en_core_sci_lg) gives a different result for text ""I am looking for Surgery of Appendicitis by allopathy treatment."" with UMLS linker",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/363
https://github.com/allenai/scispacy/issues/364:946,reliability,Pra,Practice,946,"If candidate generation is case-sensitive?; The generated candidates for a sample term (""gaps""--in lowercase) are as follow:. ```. el = nlp.get_pipe(""scispacy_linker""). el.candidate_generator([""gaps""], 5). [[MentionCandidate(concept_id='C0003074', aliases=['Anion Gaps'], similarities=[0.8052887320518494]), . MentionCandidate(concept_id='C0061928', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C0245355', aliases=['ras GAPs'], similarities=[0.8358908891677856]), . MentionCandidate(concept_id='C0392343', aliases=['Generation Gaps'], similarities=[0.727967381477356]), . MentionCandidate(concept_id='C1419277', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984552', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984553', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C4277599', aliases=['Practice Gaps'], similarities=[0.7284183502197266])]]. ```. It seems that SciSpaCy is normalising lower/upper cases when generating candidates; could you point me to the part of the code that normalising terms or is it possible to control this feature?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/364
https://github.com/allenai/scispacy/issues/364:1177,security,control,control,1177,"If candidate generation is case-sensitive?; The generated candidates for a sample term (""gaps""--in lowercase) are as follow:. ```. el = nlp.get_pipe(""scispacy_linker""). el.candidate_generator([""gaps""], 5). [[MentionCandidate(concept_id='C0003074', aliases=['Anion Gaps'], similarities=[0.8052887320518494]), . MentionCandidate(concept_id='C0061928', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C0245355', aliases=['ras GAPs'], similarities=[0.8358908891677856]), . MentionCandidate(concept_id='C0392343', aliases=['Generation Gaps'], similarities=[0.727967381477356]), . MentionCandidate(concept_id='C1419277', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984552', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984553', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C4277599', aliases=['Practice Gaps'], similarities=[0.7284183502197266])]]. ```. It seems that SciSpaCy is normalising lower/upper cases when generating candidates; could you point me to the part of the code that normalising terms or is it possible to control this feature?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/364
https://github.com/allenai/scispacy/issues/364:1177,testability,control,control,1177,"If candidate generation is case-sensitive?; The generated candidates for a sample term (""gaps""--in lowercase) are as follow:. ```. el = nlp.get_pipe(""scispacy_linker""). el.candidate_generator([""gaps""], 5). [[MentionCandidate(concept_id='C0003074', aliases=['Anion Gaps'], similarities=[0.8052887320518494]), . MentionCandidate(concept_id='C0061928', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C0245355', aliases=['ras GAPs'], similarities=[0.8358908891677856]), . MentionCandidate(concept_id='C0392343', aliases=['Generation Gaps'], similarities=[0.727967381477356]), . MentionCandidate(concept_id='C1419277', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984552', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C2984553', aliases=['GAP'], similarities=[0.6076238751411438]), . MentionCandidate(concept_id='C4277599', aliases=['Practice Gaps'], similarities=[0.7284183502197266])]]. ```. It seems that SciSpaCy is normalising lower/upper cases when generating candidates; could you point me to the part of the code that normalising terms or is it possible to control this feature?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/364
https://github.com/allenai/scispacy/issues/365:12074,availability,servic,service-,12074,da.anaconda.org/conda-forge/osx-64/cffi-1.14.5-py38ha97d567_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/chardet-4.0.0-py38h50d1736_1.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cycler-0.10.0-py38_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.cond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:527,deployability,instal,installed,527,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12074,deployability,servic,service-,12074,da.anaconda.org/conda-forge/osx-64/cffi-1.14.5-py38ha97d567_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/chardet-4.0.0-py38h50d1736_1.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cycler-0.10.0-py38_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.cond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12892,deployability,modul,modules-,12892,64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://re,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:17689,deployability,api,api-core-,17689,naconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-p,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:5204,energy efficiency,cloud,cloudpickle-,5204,da. https://repo.anaconda.com/pkgs/main/osx-64/qt-5.9.7-h468cd18_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/appnope-0.1.2-py38hecd8cb5_1001.conda. https://conda.anaconda.org/conda-forge/noarch/async-timeout-3.0.1-py_1000.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/async_generator-1.10-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/attrs-21.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/dbus-1.13.18-h18a8e69_0.conda. https://repo.anaconda.com/pkgs/main/noarch/decorator-4.4.2-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/defusedxml-0.7.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dictdiffer-0.8.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dill-0.3.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/docutils-0.17.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/entrypoints-0.3-py38_0.conda. https://repo.anaconda.com/pkgs/main/noarch/et_xmlfile-1.0.1-py_1001.conda. https://repo.anaconda.com/pkgs/main/noarch/fsspec-0.9.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-fo,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:14495,energy efficiency,core,core-,14495,a.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/zict-2.0.0-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/anyqt-0.0.13-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/argon2-cffi-20.1.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/bleach-3.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/brotlipy-0.7.0-py38h5406a74_1001.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/commonmark-0.9.1-py_0.conda. https://conda.anaconda.org/conda-forge/osx-64/cryptography-3.4.7-py38h1fa4640_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/dask-core-2021.4.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/google-crc32c-1.1.2-py38haea9d43_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/grpcio-1.37.1-py38ha263829_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/httpcore-0.13.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/importlib_metadata-3.10.0-hd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/jinja2-3.0.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/joblib-1.0.1-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/jupyter_core-4.7.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-base-1.20.1-py38h585ceec_0.conda. https://conda.anaconda.org/anaconda/osx-64/pango-1.42.4-h060686c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pip-21.0.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/plotly/noarch/plotly-4.14.3-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/preshed-3.0.5-py38h91a8764_0.tar.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:17693,energy efficiency,core,core-,17693,conda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyh,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:18103,energy efficiency,cloud,cloud-core-,18103,/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-co,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:18279,energy efficiency,core,core-,18279,otocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:18356,energy efficiency,cloud,cloud-storage-,18356,nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cytho,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19006,energy efficiency,core,core-components-,19006,boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-lou,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:21845,energy efficiency,core,core-,21845,conda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numba-0.53.0-py38hb2f4e1b_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyerfa-1.7.3-py38h9ed2024_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pywavelets-1.1.1-py38haf1e3a3_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/scipy-1.6.2-py38hd5f7400_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/tifffile-2020.10.1-py38h0cf3a3e_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/astropy-4.2.1-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/baycomp-1.0.2-py_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/gensim-3.8.3-py38h23ab428_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyldavis-3.3.1-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/scikit-image-0.18.1-py38hb2f4e1b_0.conda. https://conda.anaconda.org/anaconda/osx-64/scikit-learn-0.23.2-py38h959d312_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/seaborn-0.11.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/glue-core-0.15.6-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/py-xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/pynndescent-0.5.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/glue-vispy-viewers-0.12.2-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/opentsne-0.3.11-py38h1de35cc_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/glueviz-1.0.0-0.conda. https://repo.anaconda.com/pkgs/main/osx-64/orange3-3.26.0-py38hb1e8313_0.conda. ```. Please help. . Thanks.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12074,integrability,servic,service-,12074,da.anaconda.org/conda-forge/osx-64/cffi-1.14.5-py38ha97d567_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/chardet-4.0.0-py38h50d1736_1.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cycler-0.10.0-py38_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.cond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:17689,integrability,api,api-core-,17689,naconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-p,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19011,integrability,compon,components-,19011,o3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvai,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19107,integrability,compon,components-,19107,-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:672,interoperability,platform,platform,672,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:17689,interoperability,api,api-core-,17689,naconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-p,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19011,interoperability,compon,components-,19011,o3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvai,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19107,interoperability,compon,components-,19107,-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:20246,interoperability,distribut,distributed-,20246,/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bottleneck-1.3.2-py38hf1fa96c_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/distributed-2021.4.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dask-2021.4.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numba-0.53.0-py38hb2f4e1b_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyerfa-1.7.3-py38h9ed2024_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pywavelets-1.1.1-py38haf1e3a3_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/scipy-1.6.2-py38hd5f7400_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/tifffile-2020.10.1-py38h0cf3a3e_2.conda. https://,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:518,modifiability,pac,packages,518,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:5606,modifiability,deco,decorator-,5606,https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/dbus-1.13.18-h18a8e69_0.conda. https://repo.anaconda.com/pkgs/main/noarch/decorator-4.4.2-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/defusedxml-0.7.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dictdiffer-0.8.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dill-0.3.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/docutils-0.17.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/entrypoints-0.3-py38_0.conda. https://repo.anaconda.com/pkgs/main/noarch/et_xmlfile-1.0.1-py_1001.conda. https://repo.anaconda.com/pkgs/main/noarch/fsspec-0.9.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/funcy-1.16-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/h11-0.12.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/heapdict-1.0.1-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hpack-3.0.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hyperframe-5.2.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/idna-2.10-pyh9f0ad1d_0.tar.b,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12074,modifiability,servic,service-,12074,da.anaconda.org/conda-forge/osx-64/cffi-1.14.5-py38ha97d567_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/chardet-4.0.0-py38h50d1736_1.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cycler-0.10.0-py38_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.cond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12485,modifiability,pac,packaging-,12485,.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12892,modifiability,modul,modules-,12892,64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://re,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:13773,modifiability,extens,extensions-,13773,/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/zict-2.0.0-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/anyqt-0.0.13-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/argon2-cffi-20.1.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/bleach-3.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/brotlipy-0.7.0-py38h5406a74_1001.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/commonmark-0.9.1-py_0.conda. https://conda.anaconda.org/conda-forge/osx-64/cryptography-3.4.7-py38h1fa4640_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/dask-core-2021.4.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/google-crc32c-1.1.2-py38haea9d43_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/grpcio-1.37.1-py38ha263829_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/httpcore-0.13.3-pyhd3eb1b0_0.cond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19011,modifiability,compon,components-,19011,o3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvai,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19107,modifiability,compon,components-,19107,-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.1-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pathy-0.5.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-3.3.1-0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/matplotlib-base-3.3.1-py38h181983e_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:4418,performance,time,timeout-,4418,freetype-2.10.4-ha233b18_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libxml2-2.9.10-h7cdb67c_3.conda. https://repo.anaconda.com/pkgs/main/osx-64/readline-8.1-h9ed2024_0.conda. https://conda.anaconda.org/anaconda/osx-64/zstd-1.4.4-h1990bb4_3.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/fontconfig-2.13.0-h5d5b041_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/gettext-0.21.0-h7535e17_0.conda. https://conda.anaconda.org/anaconda/osx-64/libtiff-4.1.0-hcb84e12_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sqlite-3.35.4-hce871da_0.conda. https://conda.anaconda.org/anaconda/osx-64/glib-2.56.2-hd9629dc_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/lcms2-2.11-h92f6f08_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/python-3.8.8-h88f2d9e_5.conda. https://repo.anaconda.com/pkgs/main/osx-64/qt-5.9.7-h468cd18_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/appnope-0.1.2-py38hecd8cb5_1001.conda. https://conda.anaconda.org/conda-forge/noarch/async-timeout-3.0.1-py_1000.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/async_generator-1.10-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/attrs-21.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.ana,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:4882,performance,cach,cachetools-,4882,f-4.1.0-hcb84e12_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sqlite-3.35.4-hce871da_0.conda. https://conda.anaconda.org/anaconda/osx-64/glib-2.56.2-hd9629dc_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/lcms2-2.11-h92f6f08_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/python-3.8.8-h88f2d9e_5.conda. https://repo.anaconda.com/pkgs/main/osx-64/qt-5.9.7-h468cd18_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/appnope-0.1.2-py38hecd8cb5_1001.conda. https://conda.anaconda.org/conda-forge/noarch/async-timeout-3.0.1-py_1000.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/async_generator-1.10-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/attrs-21.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/dbus-1.13.18-h18a8e69_0.conda. https://repo.anaconda.com/pkgs/main/noarch/decorator-4.4.2-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/defusedxml-0.7.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dictdiffer-0.8.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dill-0.3.3-pyhd3eb1b0_0.conda. https://repo.anaco,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:7294,performance,lock,locket-,7294,11-0.12.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/heapdict-1.0.1-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hpack-3.0.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hyperframe-5.2.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/idna-2.10-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/ipython_genutils-0.2.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/itsdangerous-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jdcal-1.4.1-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/jmespath-0.10.0-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/json5-0.9.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyring-22.3.0-py38hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/kiwisolver-1.2.0-py38h04f5b5a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/llvmlite-0.36.0-py38he4411ff_4.conda. https://repo.anaconda.com/pkgs/main/osx-64/locket-0.2.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/lockfile-0.12.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/mistune-0.8.4-py38h1de35cc_1001.conda. https://repo.anaconda.com/pkgs/main/osx-64/msgpack-python-1.0.2-py38hf7b0b51_1.conda. https://repo.anaconda.com/pkgs/main/noarch/nest-asyncio-1.5.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/anaconda/noarch/olefile-0.46-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandocfilters-1.4.3-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/parso-0.8.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pickleshare-0.7.5-pyhd3eb1b0_1003.conda. https://repo.anaconda.com/pkgs/main/noarch/prometheus_client-0.10.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/psutil-5.8.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/ptyprocess-0.7.0-pyhd3eb1b0_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2. https,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:7372,performance,lock,lockfile-,7372,ct-1.0.1-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hpack-3.0.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hyperframe-5.2.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/idna-2.10-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/ipython_genutils-0.2.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/itsdangerous-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jdcal-1.4.1-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/jmespath-0.10.0-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/json5-0.9.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyring-22.3.0-py38hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/kiwisolver-1.2.0-py38h04f5b5a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/llvmlite-0.36.0-py38he4411ff_4.conda. https://repo.anaconda.com/pkgs/main/osx-64/locket-0.2.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/lockfile-0.12.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/mistune-0.8.4-py38h1de35cc_1001.conda. https://repo.anaconda.com/pkgs/main/osx-64/msgpack-python-1.0.2-py38hf7b0b51_1.conda. https://repo.anaconda.com/pkgs/main/noarch/nest-asyncio-1.5.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/anaconda/noarch/olefile-0.46-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandocfilters-1.4.3-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/parso-0.8.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pickleshare-0.7.5-pyhd3eb1b0_1003.conda. https://repo.anaconda.com/pkgs/main/noarch/prometheus_client-0.10.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/psutil-5.8.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/ptyprocess-0.7.0-pyhd3eb1b0_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pycparser-2.20-pyh9f0ad1d_2.tar.bz2. h,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12326,performance,network,networkx-,12326,g/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:17604,performance,cach,cachecontrol-,17604,pkgs/main/noarch/jupyterlab_pygments-0.1.2-py_0.conda. https://conda.anaconda.org/anaconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/boto3-1.17.72-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-cloud-core-1.5.0-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/notebook-6.3.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-canvas-core-0.1.15-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-cloud-storage-1.19.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_server-1.2.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab-2.3.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/smart_open-2.2.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:20164,performance,bottleneck,bottleneck-,20164,rg/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bottleneck-1.3.2-py38hf1fa96c_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/distributed-2021.4.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dask-2021.4.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numba-0.53.0-py38hb2f4e1b_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyerfa-1.7.3-py38h9ed2024_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pywavelets-1.1.1-py38haf1e3a3_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/scipy-1.6.2-py38hd5f7400_1.conda. https://re,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:255,reliability,doe,does,255,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:4418,safety,timeout,timeout-,4418,freetype-2.10.4-ha233b18_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libxml2-2.9.10-h7cdb67c_3.conda. https://repo.anaconda.com/pkgs/main/osx-64/readline-8.1-h9ed2024_0.conda. https://conda.anaconda.org/anaconda/osx-64/zstd-1.4.4-h1990bb4_3.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/fontconfig-2.13.0-h5d5b041_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/gettext-0.21.0-h7535e17_0.conda. https://conda.anaconda.org/anaconda/osx-64/libtiff-4.1.0-hcb84e12_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sqlite-3.35.4-hce871da_0.conda. https://conda.anaconda.org/anaconda/osx-64/glib-2.56.2-hd9629dc_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/lcms2-2.11-h92f6f08_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/python-3.8.8-h88f2d9e_5.conda. https://repo.anaconda.com/pkgs/main/osx-64/qt-5.9.7-h468cd18_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/appnope-0.1.2-py38hecd8cb5_1001.conda. https://conda.anaconda.org/conda-forge/noarch/async-timeout-3.0.1-py_1000.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/async_generator-1.10-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/attrs-21.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.ana,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:9918,safety,test,testpath-,9918,d2024_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyzmq-20.0.0-py38h23ab428_1.conda. https://repo.anaconda.com/pkgs/main/noarch/qtpy-1.9.0-py_0.conda. https://conda.anaconda.org/anaconda/osx-64/regex-2020.10.15-py38haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/rfc3986-1.4.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/send2trash-1.5.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/shellingham-1.4.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sip-4.19.8-py38h0a44026_0.conda. https://conda.anaconda.org/conda-forge/noarch/six-1.16.0-pyh6c4a22f_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sniffio-1.2.0-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/sortedcontainers-2.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/spacy-legacy-3.0.5-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/tblib-1.7.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/testpath-0.4.4-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/threadpoolctl-2.1.0-pyh5ca1d4c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/toolz-0.11.1-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/tornado-6.1-py38h9ed2024_0.conda. https://conda.anaconda.org/conda-forge/noarch/tqdm-4.60.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing_extensions-3.7.4.3-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/wasabi-0.8.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/wcwidth-0.2.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/webencodings-0.5.1-py38_1.conda. https://repo.anaconda.com/pkgs/main/noarch/wheel-0.36.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlrd-2.0.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlsxwriter-1.3.8-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/zipp-3.4.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12892,safety,modul,modules-,12892,64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://re,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:975,security,certif,certificates-,975,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:5041,security,certif,certifi-,5041,-hd9629dc_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/lcms2-2.11-h92f6f08_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/python-3.8.8-h88f2d9e_5.conda. https://repo.anaconda.com/pkgs/main/osx-64/qt-5.9.7-h468cd18_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/appnope-0.1.2-py38hecd8cb5_1001.conda. https://conda.anaconda.org/conda-forge/noarch/async-timeout-3.0.1-py_1000.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/async_generator-1.10-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/attrs-21.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/backcall-0.2.0-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/boto-2.49.0-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/bz2file-0.98-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/cachetools-4.2.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/cairo-1.14.12-hc4e6be7_4.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/certifi-2021.5.30-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/noarch/click-7.1.2-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cloudpickle-1.6.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/colorama-0.4.4-pyh9f0ad1d_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-renderer-1.9.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dataclasses-0.8-pyhc8e2a94_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/dbus-1.13.18-h18a8e69_0.conda. https://repo.anaconda.com/pkgs/main/noarch/decorator-4.4.2-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/defusedxml-0.7.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dictdiffer-0.8.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dill-0.3.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/docutils-0.17.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/entrypoints-0.3-py38_0.conda. https://repo.anaconda,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:7294,security,lock,locket-,7294,11-0.12.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/heapdict-1.0.1-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hpack-3.0.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hyperframe-5.2.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/idna-2.10-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/ipython_genutils-0.2.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/itsdangerous-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jdcal-1.4.1-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/jmespath-0.10.0-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/json5-0.9.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyring-22.3.0-py38hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/kiwisolver-1.2.0-py38h04f5b5a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/llvmlite-0.36.0-py38he4411ff_4.conda. https://repo.anaconda.com/pkgs/main/osx-64/locket-0.2.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/lockfile-0.12.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/mistune-0.8.4-py38h1de35cc_1001.conda. https://repo.anaconda.com/pkgs/main/osx-64/msgpack-python-1.0.2-py38hf7b0b51_1.conda. https://repo.anaconda.com/pkgs/main/noarch/nest-asyncio-1.5.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/anaconda/noarch/olefile-0.46-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandocfilters-1.4.3-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/parso-0.8.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pickleshare-0.7.5-pyhd3eb1b0_1003.conda. https://repo.anaconda.com/pkgs/main/noarch/prometheus_client-0.10.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/psutil-5.8.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/ptyprocess-0.7.0-pyhd3eb1b0_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2. https,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:7372,security,lock,lockfile-,7372,ct-1.0.1-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hpack-3.0.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/hyperframe-5.2.0-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/idna-2.10-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/ipython_genutils-0.2.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/itsdangerous-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/jdcal-1.4.1-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/jmespath-0.10.0-pyh9f0ad1d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/json5-0.9.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyring-22.3.0-py38hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/kiwisolver-1.2.0-py38h04f5b5a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/llvmlite-0.36.0-py38he4411ff_4.conda. https://repo.anaconda.com/pkgs/main/osx-64/locket-0.2.1-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/lockfile-0.12.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/mistune-0.8.4-py38h1de35cc_1001.conda. https://repo.anaconda.com/pkgs/main/osx-64/msgpack-python-1.0.2-py38hf7b0b51_1.conda. https://repo.anaconda.com/pkgs/main/noarch/nest-asyncio-1.5.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/anaconda/noarch/olefile-0.46-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandocfilters-1.4.3-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/parso-0.8.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pickleshare-0.7.5-pyhd3eb1b0_1003.conda. https://repo.anaconda.com/pkgs/main/noarch/prometheus_client-0.10.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/psutil-5.8.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/ptyprocess-0.7.0-pyhd3eb1b0_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-0.4.8-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pycparser-2.20-pyh9f0ad1d_2.tar.bz2. h,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12326,security,network,networkx-,12326,g/conda-forge/osx-64/cymem-2.0.5-py38h91a8764_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/cytoolz-0.11.0-py38haf1e3a3_0.conda. https://conda.anaconda.org/conda-forge/osx-64/future-0.18.2-py38h50d1736_3.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/h2-3.2.0-py38_1.conda. https://conda.anaconda.org/anaconda/osx-64/harfbuzz-1.8.8-hb8d4a28_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/importlib-metadata-3.10.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:13363,security,rsa,rsa-,13363,s://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/zict-2.0.0-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/anyqt-0.0.13-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/argon2-cffi-20.1.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/bleach-3.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/brotlipy-0.7.0-py38h5406a74_1001.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/commonmark-0.9.1-py_0.conda. https:/,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:14404,security,cryptograph,cryptography-,14404,conda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/werkzeug-2.0.0-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/zict-2.0.0-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/anyqt-0.0.13-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/argon2-cffi-20.1.0-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/bleach-3.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/brotlipy-0.7.0-py38h5406a74_1001.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/commonmark-0.9.1-py_0.conda. https://conda.anaconda.org/conda-forge/osx-64/cryptography-3.4.7-py38h1fa4640_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/dask-core-2021.4.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/google-crc32c-1.1.2-py38haea9d43_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/grpcio-1.37.1-py38ha263829_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/httpcore-0.13.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/importlib_metadata-3.10.0-hd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/jinja2-3.0.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/joblib-1.0.1-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/jupyter_core-4.7.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-base-1.20.1-py38h585ceec_0.conda. https://conda.anaconda.org/anaconda/osx-64/pango-1.42.4-h060686c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pip-21.0.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/plotly/noarch/plotly-4.14.3-py_0.t,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:16959,security,auth,auth-,16959,ask-2.0.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-resumable-media-1.2.0-pyhd3deb0d_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/googleapis-common-protos-1.53.0-py38h50d1736_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/graphviz-2.40.1-hefbbd9a_2.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/httpx-0.18.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/ipython-7.22.0-py38h01d92e1_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jsonschema-3.2.0-py_2.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyter_client-6.1.12-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_pygments-0.1.2-py_0.conda. https://conda.anaconda.org/anaconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py38h5ca1d4c_0.conda. https://repo.anaconda.com/pkgs/main/noarch/nbformat-5.1.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/urllib3-1.26.4-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/botocore-1.20.72-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/nbclient-0.5.3-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/qtconsole-5.0.3-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/requests-2.25.1-pyhd3deb0d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/cachecontrol-0.12.6-py_0.conda. https://conda.anaconda.org/conda-forge/noarch/google-api-core-1.26.3-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/nbconvert-6.0.7-py38_0.conda. https://conda.anaconda.org/conda-forge/noarch/s3transfer-0.4.2-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/serverfiles-0.3.1-py38hecd8cb5_0.c,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:9918,testability,test,testpath-,9918,d2024_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyzmq-20.0.0-py38h23ab428_1.conda. https://repo.anaconda.com/pkgs/main/noarch/qtpy-1.9.0-py_0.conda. https://conda.anaconda.org/anaconda/osx-64/regex-2020.10.15-py38haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/rfc3986-1.4.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/send2trash-1.5.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/shellingham-1.4.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sip-4.19.8-py38h0a44026_0.conda. https://conda.anaconda.org/conda-forge/noarch/six-1.16.0-pyh6c4a22f_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sniffio-1.2.0-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/sortedcontainers-2.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/spacy-legacy-3.0.5-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/tblib-1.7.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/testpath-0.4.4-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/threadpoolctl-2.1.0-pyh5ca1d4c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/toolz-0.11.1-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/tornado-6.1-py38h9ed2024_0.conda. https://conda.anaconda.org/conda-forge/noarch/tqdm-4.60.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing_extensions-3.7.4.3-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/wasabi-0.8.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/wcwidth-0.2.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/webencodings-0.5.1-py38_1.conda. https://repo.anaconda.com/pkgs/main/noarch/wheel-0.36.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlrd-2.0.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlsxwriter-1.3.8-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/zipp-3.4.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:401,usability,user,user-images,401,"EntityLinker import not working; Hi everyone, . I'm unable to import the scispacy EntityLinker using the code:. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. ```. When I run this piece of code in the Jupyter lab, the cell does not run and probably the kernel dies or restarts. I'm not sure what exactly is happening. . Details which might be useful:. ![image](https://user-images.githubusercontent.com/40318353/121766015-acbe6f00-cb6c-11eb-8597-0594f5262ba7.png). Please also find the packages installed in the environment:. ```. # This file may be used to create an environment using:. # $ conda create --name <env> --file <this file>. # platform: osx-64. @EXPLICIT. https://conda.anaconda.org/conda-forge/osx-64/_py-xgboost-mutex-2.0-cpu_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/blas-1.0-mkl.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/c-ares-1.17.1-h0d85af4_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ca-certificates-2021.5.25-hecd8cb5_1.conda. https://conda.anaconda.org/anaconda/osx-64/fribidi-1.0.10-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/intel-openmp-2021.2.0-hecd8cb5_564.conda. https://conda.anaconda.org/anaconda/osx-64/jpeg-9b-he5867d9_2.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/libcxx-11.1.0-habf9029_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/libgfortran-3.0.1-h93005f0_2.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/libiconv-1.16-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/libsodium-1.0.18-h1de35cc_0.conda. https://conda.anaconda.org/conda-forge/osx-64/llvm-openmp-11.1.0-hda6cdc1_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pandoc-2.12-hecd8cb5_0.conda. https://conda.anaconda.org/anaconda/osx-64/pixman-0.40.0-haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/xz-5.2.5-h1de35cc_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/yaml-0.2.5-haf1e3a3_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/zlib-1.2.11-h1de",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:10083,usability,tool,toolz-,10083,s://conda.anaconda.org/anaconda/osx-64/regex-2020.10.15-py38haf1e3a3_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/rfc3986-1.4.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/send2trash-1.5.0-pyhd3eb1b0_1.conda. https://conda.anaconda.org/conda-forge/noarch/shellingham-1.4.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sip-4.19.8-py38h0a44026_0.conda. https://conda.anaconda.org/conda-forge/noarch/six-1.16.0-pyh6c4a22f_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/sniffio-1.2.0-py38hecd8cb5_1.conda. https://repo.anaconda.com/pkgs/main/noarch/sortedcontainers-2.3.0-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/spacy-legacy-3.0.5-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/tblib-1.7.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/testpath-0.4.4-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/threadpoolctl-2.1.0-pyh5ca1d4c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/toolz-0.11.1-pyhd3eb1b0_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/tornado-6.1-py38h9ed2024_0.conda. https://conda.anaconda.org/conda-forge/noarch/tqdm-4.60.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing_extensions-3.7.4.3-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/wasabi-0.8.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/wcwidth-0.2.5-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/webencodings-0.5.1-py38_1.conda. https://repo.anaconda.com/pkgs/main/noarch/wheel-0.36.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlrd-2.0.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/xlsxwriter-1.3.8-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/zipp-3.4.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/brotli-python-1.0.9-py38h0a5c65b_4.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/catalogue-2.0.4-py38h50d1736_0.tar.bz2. https://conda.anaco,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:12804,usability,tool,toolkit-,12804,.com/pkgs/main/osx-64/jedi-0.17.0-py38_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/keyrings.alt-4.0.2-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/markupsafe-2.0.0-py38h96a0964_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/mkl-service-2.3.0-py38h9ed2024_1.conda. https://conda.anaconda.org/conda-forge/osx-64/multidict-5.1.0-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/murmurhash-1.0.5-py38h91a8764_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/networkx-2.5.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/openpyxl-3.0.7-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/noarch/packaging-20.9-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/partd-1.2.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/pexpect-4.8.0-pyhd3eb1b0_3.conda. https://conda.anaconda.org/anaconda/osx-64/pillow-8.0.0-py38h1a82f1a_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/prompt-toolkit-3.0.17-pyh06a4308_0.conda. https://conda.anaconda.org/conda-forge/noarch/pyasn1-modules-0.2.7-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/pydantic-1.7.3-py38h96a0964_1.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pyqt-5.9.2-py38h655552a_2.conda. https://conda.anaconda.org/conda-forge/osx-64/pysocks-1.7.1-py38h50d1736_3.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/python-dateutil-2.8.1-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/retrying-1.3.3-py_2.conda. https://conda.anaconda.org/conda-forge/noarch/rsa-4.7.2-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/setuptools-52.0.0-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/terminado-0.9.4-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/traitlets-5.0.5-pyhd3eb1b0_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typer-0.3.2-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/typing-extensions-3.7.4.3-0.tar.bz2. https,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:16047,usability,resum,resumable-media-,16047,m/pkgs/main/osx-64/jupyter_core-4.7.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-base-1.20.1-py38h585ceec_0.conda. https://conda.anaconda.org/anaconda/osx-64/pango-1.42.4-h060686c_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/pip-21.0.1-py38hecd8cb5_0.conda. https://conda.anaconda.org/plotly/noarch/plotly-4.14.3-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/preshed-3.0.5-py38h91a8764_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/protobuf-3.16.0-py38ha048514_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/pygments-2.8.1-pyhd3eb1b0_0.conda. https://conda.anaconda.org/conda-forge/osx-64/srsly-2.4.1-py38ha048514_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/yarl-1.6.3-py38h5406a74_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/aiohttp-3.7.4-py38h96a0964_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-2.0.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-resumable-media-1.2.0-pyhd3deb0d_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/googleapis-common-protos-1.53.0-py38h50d1736_0.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/graphviz-2.40.1-hefbbd9a_2.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/httpx-0.18.1-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/ipython-7.22.0-py38h01d92e1_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jsonschema-3.2.0-py_2.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyter_client-6.1.12-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/jupyterlab_pygments-0.1.2-py_0.conda. https://conda.anaconda.org/anaconda/noarch/nltk-3.5-py_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/pyopenssl-20.0.1-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/flask-compress-1.9.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/google-auth-1.30.0-pyh44b312d_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/ipykernel-5.3.4-py3,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:19859,usability,widget,widget-base-,19859,.tar.bz2. https://conda.anaconda.org/anaconda/osx-64/pandas-1.1.3-py38hb1e8313_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-core-components-1.16.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-html-components-1.1.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-1.20.0-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/noarch/dash-table-4.11.3-pyhd8ed1ab_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/cython-blis-0.7.4-py38ha1b04c9_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/numexpr-2.7.3-py38h1588c1c_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/thinc-8.0.3-py38he35c9cc_1.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/spacy-3.0.6-py38he35c9cc_0.tar.bz2. https://conda.anaconda.org/conda-forge/osx-64/xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/imageio-2.9.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/orange-widget-base-4.7.0-py_1.conda. https://repo.anaconda.com/pkgs/main/noarch/pyqtgraph-0.11.0-py_0.conda. https://repo.anaconda.com/pkgs/main/noarch/python-louvain-0.15-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bokeh-2.3.2-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/bottleneck-1.3.2-py38hf1fa96c_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/distributed-2021.4.1-py38hecd8cb5_0.conda. https://repo.anaconda.com/pkgs/main/noarch/dask-2021.4.0-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anacond,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:21688,usability,learn,learn-,21688,conda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numba-0.53.0-py38hb2f4e1b_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyerfa-1.7.3-py38h9ed2024_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pywavelets-1.1.1-py38haf1e3a3_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/scipy-1.6.2-py38hd5f7400_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/tifffile-2020.10.1-py38h0cf3a3e_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/astropy-4.2.1-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/baycomp-1.0.2-py_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/gensim-3.8.3-py38h23ab428_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyldavis-3.3.1-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/scikit-image-0.18.1-py38hb2f4e1b_0.conda. https://conda.anaconda.org/anaconda/osx-64/scikit-learn-0.23.2-py38h959d312_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/seaborn-0.11.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/glue-core-0.15.6-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/py-xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/pynndescent-0.5.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/glue-vispy-viewers-0.12.2-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/opentsne-0.3.11-py38h1de35cc_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/glueviz-1.0.0-0.conda. https://repo.anaconda.com/pkgs/main/osx-64/orange3-3.26.0-py38hb1e8313_0.conda. ```. Please help. . Thanks.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/365:22367,usability,help,help,22367,conda.com/pkgs/main/osx-64/fast-histogram-0.9-py38he3068b8_0.conda. https://repo.anaconda.com/pkgs/main/noarch/mpl-scatter-density-0.7-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/h5py-2.10.0-py38h0601b69_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_fft-1.3.0-py38h4a7008c_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/mkl_random-1.2.1-py38hb2f4e1b_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/numpy-1.20.1-py38hd6e1bb9_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/numba-0.53.0-py38hb2f4e1b_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pyerfa-1.7.3-py38h9ed2024_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/pywavelets-1.1.1-py38haf1e3a3_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/scipy-1.6.2-py38hd5f7400_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/tifffile-2020.10.1-py38h0cf3a3e_2.conda. https://repo.anaconda.com/pkgs/main/osx-64/astropy-4.2.1-py38h9ed2024_1.conda. https://repo.anaconda.com/pkgs/main/noarch/baycomp-1.0.2-py_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/gensim-3.8.3-py38h23ab428_2.conda. https://conda.anaconda.org/conda-forge/noarch/pyldavis-3.3.1-pyhd8ed1ab_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/scikit-image-0.18.1-py38hb2f4e1b_0.conda. https://conda.anaconda.org/anaconda/osx-64/scikit-learn-0.23.2-py38h959d312_0.tar.bz2. https://conda.anaconda.org/anaconda/noarch/seaborn-0.11.0-py_0.tar.bz2. https://repo.anaconda.com/pkgs/main/osx-64/glue-core-0.15.6-py38hecd8cb5_0.conda. https://conda.anaconda.org/conda-forge/osx-64/py-xgboost-1.4.0-py38h50d1736_0.tar.bz2. https://repo.anaconda.com/pkgs/main/noarch/pynndescent-0.5.2-pyhd3eb1b0_0.conda. https://repo.anaconda.com/pkgs/main/noarch/glue-vispy-viewers-0.12.2-py_0.conda. https://repo.anaconda.com/pkgs/main/osx-64/opentsne-0.3.11-py38h1de35cc_1.conda. https://repo.anaconda.com/pkgs/main/osx-64/glueviz-1.0.0-0.conda. https://repo.anaconda.com/pkgs/main/osx-64/orange3-3.26.0-py38hb1e8313_0.conda. ```. Please help. . Thanks.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/365
https://github.com/allenai/scispacy/issues/366:693,availability,cluster,clusters,693,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:730,availability,cluster,cluster,730,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:693,deployability,cluster,clusters,693,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:730,deployability,cluster,cluster,730,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1205,deployability,contain,contains,1205,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:832,interoperability,specif,specific,832,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:22,modifiability,paramet,parameter,22,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:430,modifiability,paramet,parameter,430,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:841,modifiability,paramet,parameters,841,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:874,modifiability,paramet,parameter,874,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:953,modifiability,layer,layers,953,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1030,modifiability,layer,layer,1030,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1165,modifiability,layer,layers,1165,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1199,modifiability,layer,layer,1199,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1278,modifiability,layer,layers,1278,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1303,modifiability,paramet,parameter,1303,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:609,testability,understand,understanding,609,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:328,usability,user,user-images,328,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/issues/366:1049,usability,behavi,behavior,1049,"Explanation for the M parameter in CreateIndex in nmslib; I was digging through code on creating the entity linker and found this TODO:. https://github.com/allenai/scispacy/blob/2290a80cfe0948e48d8ecfbd60064019d57a6874/scispacy/candidate_generation.py#L387. So i went on a deeper dive and found:. ![M param explanation](https://user-images.githubusercontent.com/12971408/122602503-ee558b00-d040-11eb-9cb1-30d930c90521.png). The M parameter seems to be the size of the neighbourhood in the underlying graph structure when creating the HNSW ( https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf) graph. . My understanding is that putting this number higher increases the connectivity between clusters allowing for easier between-cluster jumping. . The way they describe it in NMSLIB repo is:. """""". In what follows, we discuss HNSW-specific parameters. First, for HNSW, the parameter M defines the maximum number of neighbors in the zero and above-zero layers. However, the actual default maximum number of neighbors for the zero layer is 2*M. This behavior can be overriden by explicitly setting the maximum number of neighbors for the zero (maxM0) and above-zero layers (maxM). Note that the zero layer contains all the data points, but the number of points included in other layers is defined by the parameter mult (see the HNSW paper for details). """"""",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/366
https://github.com/allenai/scispacy/pull/367:6,modifiability,paramet,parameter,6,Add m parameter documentation; Closes #366,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:16,usability,document,documentation,16,Add m parameter documentation; Closes #366,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/367:31,usability,Close,Closes,31,Add m parameter documentation; Closes #366,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/367
https://github.com/allenai/scispacy/pull/369:0,safety,Test,Test,0,Test ci;,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/369
https://github.com/allenai/scispacy/pull/369:0,testability,Test,Test,0,Test ci;,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/369
https://github.com/allenai/scispacy/issues/370:5,availability,down,downloading,5,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:346,availability,error,error,346,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1274,availability,servic,service,1274,"nction. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2705,availability,servic,service,2705,"aconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3333,availability,error,error,3333,"onn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3510,availability,error,error,3510,"iled to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPAC",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3612,availability,error,error,3612,"ction object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3963,availability,servic,service,3963,"rt, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6352,availability,error,error,6352,".resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8022,availability,down,downloading,8022,"andidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, m",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10105,availability,servic,service,10105,", proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz. b7aad",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10245,availability,down,downloading,10245,"44 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_spa",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12239,availability,down,downloading,12239,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12304,availability,down,downloaded,12304,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12399,availability,down,download,12399,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1274,deployability,servic,service,1274,"nction. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2511,deployability,Fail,Failed,2511,"ethod, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, erro",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2648,deployability,Fail,Failed,2648,"05 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . M",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2705,deployability,servic,service,2705,"aconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3906,deployability,Fail,Failed,3906,"ters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3963,deployability,servic,service,3963,"rt, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4142,deployability,modul,module,4142,"od, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/langua",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6417,deployability,fail,fails,6417,"g"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: U",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8518,deployability,api,api,8518,"/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8726,deployability,api,api,8726,"_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9179,deployability,updat,update,9179,"onda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. =================================================",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10048,deployability,Fail,Failed,10048,"a, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc605811",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10105,deployability,servic,service,10105,", proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz. b7aad",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2905,energy efficiency,adapt,adapters,2905,"n). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Fai",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9418,energy efficiency,adapt,adapter,9418,"ode != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9577,energy efficiency,adapt,adapters,9577,"ow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12648,energy efficiency,load,load,12648,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1274,integrability,servic,service,1274,"nction. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2705,integrability,servic,service,2705,"aconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2905,integrability,adapt,adapters,2905,"n). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Fai",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3963,integrability,servic,service,3963,"rt, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5594,integrability,schema,schema,5594,"""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-pack",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5696,integrability,schema,schema,5696,"id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, t",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5703,integrability,schema,schema,5703,"numerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshol",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5885,integrability,schema,schema,5885,", source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 nam",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6026,integrability,schema,schema,6026,"self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/ca",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6248,integrability,schema,schema,6248,"rnal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8518,integrability,api,api,8518,"/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8726,integrability,api,api,8726,"_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9418,integrability,adapt,adapter,9418,"ode != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9577,integrability,adapt,adapters,9577,"ow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10105,integrability,servic,service,10105,", proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz. b7aad",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:887,interoperability,socket,socket,887,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:926,interoperability,socket,socket,926,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1033,interoperability,socket,socket,1033,"LS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpoo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1925,interoperability,Socket,SocketTimeout,1925,".SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, r",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2905,interoperability,adapt,adapters,2905,"n). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Fai",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2971,interoperability,prox,proxies,2971,"vs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5347,interoperability,registr,registry,5347,"d(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5402,interoperability,registr,registry,5402,"ct_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if th",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:7668,interoperability,Semant,SemanticTypeNode,7668,"lve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/e",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8518,interoperability,api,api,8518,"/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8726,interoperability,api,api,8726,"_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9111,interoperability,prox,proxies,9111," os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service no",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9418,interoperability,adapt,adapter,9418,"ode != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9577,interoperability,adapt,adapters,9577,"ow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9643,interoperability,prox,proxies,9643,"s). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca0",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:547,modifiability,pac,packages,547,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:755,modifiability,pac,packages,755,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1274,modifiability,servic,service,1274,"nction. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1461,modifiability,pac,packages,1461,"r Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1750,modifiability,pac,packages,1750,"-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above e",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2006,modifiability,pac,packages,2006,"scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2225,modifiability,pac,packages,2225,"name, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2414,modifiability,pac,packages,2414,"anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2705,modifiability,servic,service,2705,"aconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2887,modifiability,pac,packages,2887,"_validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2905,modifiability,adapt,adapters,2905,"n). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Fai",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3089,modifiability,pac,packages,3089,"onn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3437,modifiability,pac,packages,3437,".py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, o",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3963,modifiability,servic,service,3963,"rt, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4142,modifiability,modul,module,4142,"od, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/langua",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4797,modifiability,pac,packages,4797,"e_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anacon",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5124,modifiability,pac,packages,5124,"620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, se",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5545,modifiability,pac,packages,5545,"---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5838,modifiability,pac,packages,5838,"factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6201,modifiability,pac,packages,6201,"onfig, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWL",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6593,modifiability,pac,packages,6593,"ema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.p",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:7009,modifiability,pac,packages,7009," 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from th",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:7304,modifiability,pac,packages,7304,"t want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cach",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:7554,modifiability,pac,packages,7554,"3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:7867,modifiability,pac,packages,7867,"tor(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 retu",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8221,modifiability,pac,packages,8221,"233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(pre",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8500,modifiability,pac,packages,8500,"94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approx",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8708,modifiability,pac,packages,8708,"e in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8980,modifiability,pac,packages,8980,". 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError(",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9312,modifiability,pac,packages,9312,"xist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a101",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9418,modifiability,adapt,adapter,9418,"ode != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9559,modifiability,pac,packages,9559,"gs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9577,modifiability,adapt,adapters,9577,"ow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b3",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10105,modifiability,servic,service,10105,", proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz. b7aad",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:346,performance,error,error,346,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:682,performance,time,timeout,682,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:821,performance,time,timeout,821,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1578,performance,time,timeout,1578,"new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionErro",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1827,performance,time,timeout,1827,"ource_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent c",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2948,performance,time,timeout,2948,") as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3020,performance,time,timeout,3020,"onnectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3028,performance,time,timeout,3028,"npool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, anothe",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3206,performance,time,timeout,3206,"python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3333,performance,error,error,3333,"onn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3510,performance,error,error,3510,"iled to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPAC",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3612,performance,error,error,3612,"ction object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6352,performance,error,error,6352,".resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8015,performance,cach,cache,8015,"scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: UmlsSemanticTypeTree = construct_umls_tree_from_tsv(. ---> 92 types_file_path. 93 ). 94 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/umls_semantic_type_tree.py in construct_umls_tree_from_tsv(filepath). 98 . 99 node_stack: Deque[SemanticTypeNode] = deque(). --> 100 for line in open(cached_path(filepath), ""r""):. 101 name, type_id, level = line.split(""\t""). 102 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in reque",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8796,performance,memor,memory,8796,"2 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. Connectio",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9085,performance,time,timeout,9085,"ename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Er",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9477,performance,time,time,9477,"y/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c53",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9620,performance,time,timeout,9620,"st('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.t",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12648,performance,load,load,12648,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2511,reliability,Fail,Failed,2511,"ethod, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, erro",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2648,reliability,Fail,Failed,2648,"05 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . M",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3906,reliability,Fail,Failed,3906,"ters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6417,reliability,fail,fails,6417,"g"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 self.semantic_type_tree: U",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:10048,reliability,Fail,Failed,10048,"a, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vectorizer.joblib.json. b7aadedb8a0dc19e86aca4e866e8767a419889c39f92f96e9458ee264bc13783.d670f5df5e7ad62f30ba05df2f48cafc605811",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:346,safety,error,error,346,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:682,safety,timeout,timeout,682,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:821,safety,timeout,timeout,821,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1322,safety,except,exception,1322,"opped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = se",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1341,safety,except,exception,1341,"error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1578,safety,timeout,timeout,1578,"new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionErro",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1827,safety,timeout,timeout,1827,"ource_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent c",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1917,safety,except,except,1917,"mily, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2753,safety,except,exception,2753,"/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with u",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2772,safety,except,exception,2772,"pool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispac",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2948,safety,timeout,timeout,2948,") as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3020,safety,timeout,timeout,3020,"onnectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3028,safety,timeout,timeout,3028,"npool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, anothe",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3206,safety,timeout,timeout,3206,"python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3333,safety,error,error,3333,"onn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3510,safety,error,error,3510,"iled to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPAC",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3612,safety,error,error,3612,"ction object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4015,safety,except,exception,4015,"timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4034,safety,except,exception,4034,"0 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4115,safety,input,input-,4115,"ol.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4142,safety,modul,module,4142,"od, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/langua",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4376,safety,input,input-,4376,")[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=v",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4918,safety,valid,validate,4918,"ish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4985,safety,valid,validate,4985,"ing handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, re",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4994,safety,valid,validate,4994,"ing of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5212,safety,valid,validate,5212," 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5270,safety,avoid,avoid,5270,"= pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5369,safety,valid,validate,5369,"on-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, be",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5378,safety,valid,validate,5378,"4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5444,safety,valid,validate,5444,"er, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5453,safety,valid,validate,5453,"refix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*a",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5613,safety,valid,validate,5613,"ns"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linkin",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5732,safety,valid,validate,5732,"p[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, fi",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5741,safety,valid,validate,5741,"()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5913,safety,valid,validate,5913," validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6034,safety,valid,validate,6034,"_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_g",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6043,safety,valid,validate,6043,"ex(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6256,safety,valid,validate,6256,"l here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6322,safety,except,except,6322,"e. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/lin",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6352,safety,error,error,6352,".resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9085,safety,timeout,timeout,9085,"ename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Er",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9179,safety,updat,update,9179,"onda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. =================================================",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9620,safety,timeout,timeout,9620,"st('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.t",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9760,safety,except,except,9760,"gs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:887,security,soc,socket,887,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:926,security,soc,socket,926,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:955,security,soc,socktype,955,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1033,security,soc,socket,1033,"LS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpoo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1207,security,soc,socktype,1207,"d database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/p",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1925,security,Soc,SocketTimeout,1925,".SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, r",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2099,security,soc,sock,2099,"4 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/url",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2142,security,soc,sock,2142,".getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, met",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2284,security,certif,certificate,2284,"n. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4918,security,validat,validate,4918,"ish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4985,security,validat,validate,4985,"ing handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, re",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4994,security,validat,validate,4994,"ing of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5212,security,validat,validate,5212," 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5369,security,validat,validate,5369,"on-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, be",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5378,security,validat,validate,5378,"4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5444,security,validat,validate,5444,"er, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5453,security,validat,validate,5453,"refix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*a",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5613,security,validat,validate,5613,"ns"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linkin",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5732,security,validat,validate,5732,"p[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, fi",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5741,security,validat,validate,5741,"()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:5913,security,validat,validate,5913," validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6034,security,validat,validate,6034,"_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_g",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6043,security,validat,validate,6043,"ex(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6256,security,validat,validate,6256,"l here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8827,security,session,sessions,8827,"a3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8836,security,Session,Session,8836,"cispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-w",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8849,security,session,session,8849,"ython3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazona",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8874,security,session,session,8874,"ispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max r",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8998,security,session,sessions,8998," it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connecti",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9063,security,cookie,cookies,9063,"_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9079,security,auth,auth,9079,"l_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connect",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9179,security,updat,update,9179,"onda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. =================================================",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9330,security,session,sessions,9330," . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9663,security,SSL,SSLError,9663,"aconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12276,security,ident,identify,12276,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:467,testability,Trace,Traceback,467,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:1381,testability,Trace,Traceback,1381,"===========================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2296,testability,verif,verification,2296,"ndling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.in",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2807,testability,Trace,Traceback,2807,", method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:2957,testability,verif,verify,2957," ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn). 1009 if not getattr(conn, ""sock"", None): # AppEngine might not have `.sock`. -> 1010 conn.connect(). 1011 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in connect(self). 352 # Add certificate verification. --> 353 conn = self._new_conn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4071,testability,Trace,Traceback,4071,"thon3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6391,testability,trace,traceback,6391,"led = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self, file_path, types_file_path). 90 . 91 se",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9120,testability,hook,hooks,9120,".exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known'",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9135,testability,verif,verify,9135,"filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. =====",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9629,testability,verif,verify,9629,"', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12946,testability,trace,trace,12946,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:325,usability,stop,stopped,325,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:346,usability,error,error,346,"Keep downloading files related to UMLS from amazon website everytime while entity linking; Hi, I tried to extract entities related to those in UMLS from the EHR data, and I used the Scispacy with en_ner_bc5cdr_md database. Although I ran the chunks of EHR data using by below function. It can iterate the 10-12 loop and then stopped by the below error : . =========================================================================================. ```python. gaierror Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 169 conn = connection.create_connection(. --> 170 (self._dns_host, self.port), self.timeout, **extra_kw. 171 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options). 72 . ---> 73 for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):. 74 af, socktype, proto, canonname, sa = res. ~/anaconda3/envs/scispacy/lib/python3.6/socket.py in getaddrinfo(host, port, family, type, proto, flags). 744 addrlist = []. --> 745 for res in _socket.getaddrinfo(host, port, family, type, proto, flags):. 746 af, socktype, proto, canonname, sa = res. gaierror: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. NewConnectionError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 705 headers=headers,. --> 706 chunked=chunked,. 707 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw). 381 try:. --> 382 self._validate_conn(conn). 383 except (SocketTimeout, BaseSSLError) as e:. ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3333,usability,error,error,3333,"onn(). 354 hostname = self.host. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self). 181 raise NewConnectionError(. --> 182 self, ""Failed to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3510,usability,error,error,3510,"iled to establish a new connection: %s"" % e. 183 ). NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPAC",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:3612,usability,error,error,3612,"ction object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known. During handling of the above exception, another exception occurred:. MaxRetryError Traceback (most recent call last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 448 retries=self.max_retries,. --> 449 timeout=timeout. 450 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviat",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4115,usability,input,input-,4115,"ol.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw). 755 retries = retries.increment(. --> 756 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:4376,usability,input,input-,4376,")[2]. 757 ). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace). 573 if new_retry.is_exhausted():. --> 574 raise MaxRetryError(_pool, url, error or ResponseError(cause)). 575 . MaxRetryError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). During handling of the above exception, another exception occurred:. ConnectionError Traceback (most recent call last). <ipython-input-15-230f620b9f72> in <module>. 4 df_ner_processed = extract_entity(arg_list[i]). 5 else :. ----> 6 tmp = extract_entity(arg_list[i]). 7 df_ner_processed = pd.concat([df_ner_processed, tmp], ignore_index=True). 8 arg_list_done.append(arg_list[i]). <ipython-input-4-ba3e01ebd0e6> in extract_entity(patient_line_dates_map, out_folder, out_prefix). 9 #nlp.add_pipe(linker). 10 #os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". ---> 11 nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). 12 result = pd.DataFrame(). 13 for i, patient_id in enumerate(patient_line_dates_map[0].keys()) :. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in add_pipe(self, factory_name, name, before, after, first, last, source, config, raw_config, validate). 776 config=config,. 777 raw_config=raw_config,. --> 778 validate=validate,. 779 ). 780 pipe_index = self._get_pipe_index(before, after, first, last). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/language.py in create_pipe(self, factory_name, name, config, raw_config, validate). 657 # We're calling the internal _fill here to avoid constructing the. 658 # registered functions twice. --> 659 resolved = registry.resolve(cfg, validate=v",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:6352,usability,error,error,6352,".resolve(cfg, validate=validate). 660 filled = registry.fill({""cfg"": cfg[factory_name]}, validate=validate)[""cfg""]. 661 filled = Config(filled). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in resolve(cls, config, schema, overrides, validate). 726 ) -> Dict[str, Any]:. 727 resolved, _ = cls._make(. --> 728 config, schema=schema, overrides=overrides, validate=validate, resolve=True. 729 ). 730 return resolved. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _make(cls, config, schema, overrides, resolve, validate). 775 config = Config(orig_config).interpolate(). 776 filled, _, resolved = cls._fill(. --> 777 config, schema, validate=validate, overrides=overrides, resolve=resolve. 778 ). 779 filled = Config(filled, section_order=section_order). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/thinc/config.py in _fill(cls, config, schema, validate, resolve, parent, overrides). 846 # We don't want to try/except this and raise our own error. 847 # here, because we want the traceback if the function fails. --> 848 getter_result = getter(*args, **kwargs). 849 else:. 850 # We're not resolving and calling the function, so replace. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking.py in __init__(self, nlp, name, candidate_generator, resolve_abbreviations, k, threshold, no_definition_threshold, filter_for_definitions, max_entities_per_mention, linker_name). 83 . 84 self.candidate_generator = candidate_generator or CandidateGenerator(. ---> 85 name=linker_name. 86 ). 87 self.resolve_abbreviations = resolve_abbreviations. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/candidate_generation.py in __init__(self, ann_index, tfidf_vectorizer, ann_concept_aliases_list, kb, verbose, ef_search, name). 230 ). 231 . --> 232 self.kb = kb or DEFAULT_KNOWLEDGE_BASES[name](). 233 self.verbose = verbose. 234 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/linking_utils.py in __init__(self",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:8796,usability,memor,memory,8796,"2 name = name.strip(). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in cached_path(url_or_filename, cache_dir). 34 if parsed.scheme in (""http"", ""https""):. 35 # URL, so get it from the cache (downloading if necessary). ---> 36 return get_from_cache(url_or_filename, cache_dir). 37 elif os.path.exists(url_or_filename):. 38 # File, and it exists. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/scispacy/file_cache.py in get_from_cache(url, cache_dir). 110 os.makedirs(cache_dir, exist_ok=True). 111 . --> 112 response = requests.head(url, allow_redirects=True). 113 if response.status_code != 200:. 114 raise IOError(. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs). 102 . 103 kwargs.setdefault('allow_redirects', False). --> 104 return request('head', url, **kwargs). 105 . 106 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs). 59 # cases, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. Connectio",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:9767,usability,Close,ClosedPoolError,9767,"ses, and look like a memory leak in others. 60 with sessions.Session() as session:. ---> 61 return session.request(method=method, url=url, **kwargs). 62 . 63 . ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json). 540 }. 541 send_kwargs.update(settings). --> 542 resp = self.send(prep, **send_kwargs). 543 . 544 return resp. ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs). 653 . 654 # Send the request. --> 655 r = adapter.send(request, **kwargs). 656 . 657 # Total elapsed time of the request (approximately). ~/anaconda3/envs/scispacy/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies). 514 raise SSLError(e, request=request). 515 . --> 516 raise ConnectionError(e, request=request). 517 . 518 except ClosedPoolError as e:. ConnectionError: HTTPSConnectionPool(host='s3-us-west-2.amazonaws.com', port=443): Max retries exceeded with url: /ai2-s2-scispacy/data/umls_semantic_type_tree.tsv (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fbb483d36a0>: Failed to establish a new connection: [Errno -2] Name or service not known',)). ```. ===============================================================================. I guess that the scispacy keep downloading several files related to UMLS such as below ones, . '21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv. 21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv.json. 2d16de7bdaeca09492930b4065e44c63c649377e583a60baedc48b21160e6ffe.d22e20f8a82d5590c728adb94b7cf2d9bc3c953bfb63e06e356fd8b13d7b4d77.tfidf_vector",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/370:12351,usability,user,user,12351,"f48cafc6058117f2e352385f289872ac8c95028.tfidf_vectors_sparse.npz.json. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl. cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json. d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json.json. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin. dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin.json'. I think this is the issue of sha256 while downloading because Scispacy did not identify the prefix name of downloaded files in the .scispacy/datasets/ in user's home directory, so it endlessly tried to download the mandatory files for extracting entity of EHR. How can I fix this issue? . =================================================================================================. ```python. def extract_entity(patient_dates_map):. nlp = spacy.load('en_ner_bc5cdr_md'). os.environ[""SCISPACY_CACHE""]=""/home/hahnyi/.scispacy"". nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). for i, patient_id in enumerate(patient_dates_map[0].keys()) :. . dates_map = patient_dates_map[0][patient_id]. for line, trace in dates_map.items():. doc = nlp(line.lower()). for ent in doc.ents :. if ent.label_ == ""DISEASE"":. if len(ent._.kb_ents) > 0:. linked_ent = ent._.kb_ents[0]. else:. linked_ent = None. ent_text = linked_ent[0] if linked_ent else ent.text. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/370
https://github.com/allenai/scispacy/issues/371:131,interoperability,semant,semantic,131,"icd10 with CUI and TUI of an entity; Hi,. Can we get icd10 value of an entity along with CUI ( Concept Unique Identifier) and TUI (semantic type ) ? We are able to get CUI and TUI with the help of ""[scispacy.linking](https://github.com/allenai/scispacy#example-usage-1) "" but I need ICD10 also . Is there any way to map CUI and TUI to get ICD10 ? Please suggest how can I get ICD10 of an entity ...",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/371:110,security,Ident,Identifier,110,"icd10 with CUI and TUI of an entity; Hi,. Can we get icd10 value of an entity along with CUI ( Concept Unique Identifier) and TUI (semantic type ) ? We are able to get CUI and TUI with the help of ""[scispacy.linking](https://github.com/allenai/scispacy#example-usage-1) "" but I need ICD10 also . Is there any way to map CUI and TUI to get ICD10 ? Please suggest how can I get ICD10 of an entity ...",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/371:189,usability,help,help,189,"icd10 with CUI and TUI of an entity; Hi,. Can we get icd10 value of an entity along with CUI ( Concept Unique Identifier) and TUI (semantic type ) ? We are able to get CUI and TUI with the help of ""[scispacy.linking](https://github.com/allenai/scispacy#example-usage-1) "" but I need ICD10 also . Is there any way to map CUI and TUI to get ICD10 ? Please suggest how can I get ICD10 of an entity ...",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/371
https://github.com/allenai/scispacy/issues/372:180,deployability,modul,module,180,"Getting zsh: illegal hardware instruction on MacBook M1; I am attempting to run the [EntityLinking example](https://github.com/allenai/scispacy#example-usage-1), but importing the module. `from scispacy.linking import EntityLinker`. results in . `zsh: illegal hardware instruction /Users/username/opt/anaconda3/envs/HCNLP/bin/python`. . I am able to run the first example to get the text entities fine. OS: MacOS 11.4 on M1 MBP. Python: 3.7.10. conda: 4.10.1. scispacy: 0.4.0. spacy: 3.0.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:180,modifiability,modul,module,180,"Getting zsh: illegal hardware instruction on MacBook M1; I am attempting to run the [EntityLinking example](https://github.com/allenai/scispacy#example-usage-1), but importing the module. `from scispacy.linking import EntityLinker`. results in . `zsh: illegal hardware instruction /Users/username/opt/anaconda3/envs/HCNLP/bin/python`. . I am able to run the first example to get the text entities fine. OS: MacOS 11.4 on M1 MBP. Python: 3.7.10. conda: 4.10.1. scispacy: 0.4.0. spacy: 3.0.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:180,safety,modul,module,180,"Getting zsh: illegal hardware instruction on MacBook M1; I am attempting to run the [EntityLinking example](https://github.com/allenai/scispacy#example-usage-1), but importing the module. `from scispacy.linking import EntityLinker`. results in . `zsh: illegal hardware instruction /Users/username/opt/anaconda3/envs/HCNLP/bin/python`. . I am able to run the first example to get the text entities fine. OS: MacOS 11.4 on M1 MBP. Python: 3.7.10. conda: 4.10.1. scispacy: 0.4.0. spacy: 3.0.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:282,usability,User,Users,282,"Getting zsh: illegal hardware instruction on MacBook M1; I am attempting to run the [EntityLinking example](https://github.com/allenai/scispacy#example-usage-1), but importing the module. `from scispacy.linking import EntityLinker`. results in . `zsh: illegal hardware instruction /Users/username/opt/anaconda3/envs/HCNLP/bin/python`. . I am able to run the first example to get the text entities fine. OS: MacOS 11.4 on M1 MBP. Python: 3.7.10. conda: 4.10.1. scispacy: 0.4.0. spacy: 3.0.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/372:288,usability,user,username,288,"Getting zsh: illegal hardware instruction on MacBook M1; I am attempting to run the [EntityLinking example](https://github.com/allenai/scispacy#example-usage-1), but importing the module. `from scispacy.linking import EntityLinker`. results in . `zsh: illegal hardware instruction /Users/username/opt/anaconda3/envs/HCNLP/bin/python`. . I am able to run the first example to get the text entities fine. OS: MacOS 11.4 on M1 MBP. Python: 3.7.10. conda: 4.10.1. scispacy: 0.4.0. spacy: 3.0.6",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/372
https://github.com/allenai/scispacy/issues/373:27,energy efficiency,model,model,27,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:225,energy efficiency,model,model,225,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:516,performance,memor,memory,516,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:27,security,model,model,27,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:225,security,model,model,225,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/373:516,usability,memor,memory,516,Adding UmlsEntityLinker to model in spacy v3; This is the same issue I asked in [here](https://github.com/explosion/spaCy/discussions/8608) and they told me to ask here:. I am trying to add the UmlsEntityLinker to my trained model in spacy (I'm using jupyter notebook). Here is my code:. ```. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. entity_linker = UmlsEntityLinker(resolve_abbreviations=True). ```. However I seem to keep running and use a lot of memory... Is this the right way to do this? or do I have to use `get_pipe` and `add_pipe` somewhere?,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/373
https://github.com/allenai/scispacy/issues/374:226,deployability,pipelin,pipeline,226,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:15,energy efficiency,model,model,15,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:50,energy efficiency,load,load,50,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:69,energy efficiency,model,model,69,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:88,energy efficiency,model,model,88,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:113,energy efficiency,load,load,113,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:137,energy efficiency,model,model,137,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:163,energy efficiency,load,load,163,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:245,energy efficiency,model,model,245,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:308,energy efficiency,model,models,308,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:226,integrability,pipelin,pipeline,226,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:50,performance,load,load,50,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:113,performance,load,load,113,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:163,performance,load,load,163,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:251,safety,detect,detect,251,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:15,security,model,model,15,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:69,security,model,model,69,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:88,security,model,model,88,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:137,security,model,model,137,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:245,security,model,model,245,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:251,security,detect,detect,251,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:308,security,model,models,308,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/374:338,usability,help,help,338,"Multiple spacy model; Hi @danielkingai2 ,. Can we load more than one model for example. model 1 ---> nlp = spacy.load(""en_core_sci_md""). model 2 ---> med7 = spacy.load(""en_core_med7_lg""). and so on.. then add them in a single pipeline . As both model detect different entities and I want the results of both models in a single call . Any help is really appreciated !!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/374
https://github.com/allenai/scispacy/issues/375:82,availability,error,error,82,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:225,availability,Error,Error,225,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:554,availability,error,error,554,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:471,deployability,contain,contains,471,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:133,energy efficiency,load,load,133,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:356,energy efficiency,reduc,reduce,356,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:610,energy efficiency,model,model,610,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:231,integrability,messag,message,231,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:231,interoperability,messag,message,231,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:82,performance,error,error,82,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:133,performance,load,load,133,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:225,performance,Error,Error,225,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:554,performance,error,error,554,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:560,reliability,doe,does,560,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:82,safety,error,error,82,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:225,safety,Error,Error,225,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:443,safety,input,input,443,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:554,safety,error,error,554,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:610,security,model,model,610,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:82,usability,error,error,82,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:225,usability,Error,Error,225,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:443,usability,input,input,443,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/375:554,usability,error,error,554,"RuntimeError running sentence with many backslashes; The following reproduces the error:. ``` python. import spacy. scispacy = spacy.load('en_core_sci_scibert'). scispacy(''.join(['\\'] * 511 ))). ```. using scispacy==0.4.0. Error message: `RuntimeError: The size of tensor a (513) must match the size of tensor b (512) at non-singleton dimension 1`. If I reduce `511` to a lower number in the call, it works. In my case, this is not the real input but my data sometimes contains equations that use LaTeX syntax and have a lot of backslashed items. This error does not occur with Spacy's own `en_core_web_trf` model.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/375
https://github.com/allenai/scispacy/issues/376:8,integrability,filter,filter,8,"Rename ""filter for no defintions"" to ""prefer_entities_with_definitions""; https://github.com/allenai/scispacy/blob/45e0ca1f5fad317f3daec8bb09377fa0f48d2485/scispacy/linking.py#L76. The name makes it seem like the entity linker will completely remove all entities without definitions, and you have to read the code to see how it works. This makes it more explicit",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:231,safety,compl,completely,231,"Rename ""filter for no defintions"" to ""prefer_entities_with_definitions""; https://github.com/allenai/scispacy/blob/45e0ca1f5fad317f3daec8bb09377fa0f48d2485/scispacy/linking.py#L76. The name makes it seem like the entity linker will completely remove all entities without definitions, and you have to read the code to see how it works. This makes it more explicit",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/376:231,security,compl,completely,231,"Rename ""filter for no defintions"" to ""prefer_entities_with_definitions""; https://github.com/allenai/scispacy/blob/45e0ca1f5fad317f3daec8bb09377fa0f48d2485/scispacy/linking.py#L76. The name makes it seem like the entity linker will completely remove all entities without definitions, and you have to read the code to see how it works. This makes it more explicit",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/376
https://github.com/allenai/scispacy/issues/378:20,deployability,pipelin,pipeline,20,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:100,deployability,pipelin,pipeline,100,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:20,integrability,pipelin,pipeline,20,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:100,integrability,pipelin,pipeline,100,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:122,safety,compl,complaining,122,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/378:122,security,compl,complaining,122,"Multiple linkers in pipeline; Hi, I would like to add multiple linkers (e.g., umls and mesh) to the pipeline and spacy is complaining since they have the same name. Is this even possible? ```python. nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'umls',. },. ). . nlp.add_pipe(. 'scispacy_linker',. config={. 'linker_name': 'mesh',. },. ). ```. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378
https://github.com/allenai/scispacy/issues/379:251,availability,down,downloaded,251,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1273,availability,error,error,1273,"w and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3217,availability,down,downloaded,3217,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3261,availability,down,downloaded,3261,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3645,availability,error,error,3645,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3738,availability,down,downloading,3738,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:29,deployability,fail,failed,29,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1292,deployability,fail,failed,1292,"nder /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1525,deployability,modul,module,1525,"mazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. Fil",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2960,deployability,fail,failed,2960,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3329,deployability,contain,contains,3329,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3651,deployability,continu,continues,3651,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1148,energy efficiency,load,load,1148,"e using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 77",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1450,energy efficiency,load,load,1450,"index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2892,energy efficiency,load,loadIndex,2892,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3075,energy efficiency,load,load,3075,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1883,interoperability,registr,registry,1883,"ed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_i",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1525,modifiability,modul,module,1525,"mazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. Fil",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1671,modifiability,pac,packages,1671,"linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_inde",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1817,modifiability,pac,packages,1817,"020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1983,modifiability,pac,packages,1983," this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of store",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2116,modifiability,pac,packages,2116,"inker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no pro",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2255,modifiability,pac,packages,2255,"the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2401,modifiability,pac,packages,2401,"of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377e",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2583,modifiability,pac,packages,2583,"8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no intern",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2782,modifiability,pac,packages,2782,"ronment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the s",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1148,performance,load,load,1148,"e using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 77",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1273,performance,error,error,1273,"w and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1450,performance,load,load,1450,"index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2892,performance,load,loadIndex,2892,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3075,performance,load,load,3075,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3645,performance,error,error,3645,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:29,reliability,fail,failed,29,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:67,reliability,doe,doesn,67,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1292,reliability,fail,failed,1292,"nder /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1380,reliability,doe,doesn,1380,"spacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:2960,reliability,fail,failed,2960,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3005,reliability,doe,doesn,3005,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1273,safety,error,error,1273,"w and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1503,safety,test,test,1503,"scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or C",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1525,safety,modul,module,1525,"mazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. Fil",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1905,safety,valid,validate,1905,"ation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1914,safety,valid,validate,1914,"to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(link",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3645,safety,error,error,3645,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1905,security,validat,validate,1905,"ation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1914,security,validat,validate,1914,"to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(link",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3340,security,hash,hash,3340,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3590,security,access,access,3590,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:140,testability,simpl,simple,140,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1461,testability,Trace,Traceback,1461,"noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candida",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1503,testability,test,test,1503,"scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or C",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:140,usability,simpl,simple,140,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:301,usability,user,user,301,"MeshLinker - Offline - Check failed: The number of stored elements doesn't match the number of datapoints; Hello! I've been trying to run a simple code using the MeshLinker on a server (Windows 2012, python 3.8.10) with no internet connection. I have downloaded the files below and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\co",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:1273,usability,error,error,1273,"w and stored them under /user/.scispacy. ```. MeshLinkerPaths = LinkerPaths(. ann_index=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/nmslib_index.bin"", # noqa. tfidf_vectorizer=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectorizer.joblib"", # noqa. tfidf_vectors=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/tfidf_vectors_sparse.npz"", # noqa. concept_aliases_list=""https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/mesh/concept_aliases.json"", # noqa. ). ```. I also changed the candidate_generation.py to point directly to the local files. . The code I'm trying to run is this below. I'm using a fresh new environment too. ```. import en_core_sci_sm. import scispacy. from scispacy.linking import EntityLinker. nlp = en_core_sci_sm.load(). nlp.add_pipe(""scispacy_linker"", config={""threshold"": 0.8, ""linker_name"": ""mesh""}). ```. I keep getting the following error:. ```. Check failed: totalElementsStored_ == this->data_.size() The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? Traceback (most recent call last):. File ""test.py"", line 6, in <module>. nlp.add_pipe(""scispacy_linker"",config={""threshold"":0.8,""linker_name"":""mesh""}). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 773, in add_pipe. pipe_component = self.create_pipe(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\spacy\language.py"", line 659, in create_pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/379:3645,usability,error,error,3645,"pipe. resolved = registry.resolve(cfg, validate=validate). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 727, in resolve. resolved, _ = cls._make(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 776, in _make. filled, _, resolved = cls._fill(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\thinc\config.py"", line 848, in _fill. getter_result = getter(*args, **kwargs). File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\linking.py"", line 84, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 241, in __init__. self.ann_index = ann_index or load_approximate_nearest_neighbours_index(. File ""F:\Working\PythonEnvironment\venvs\sal_doc\lib\site-packages\scispacy\candidate_generation.py"", line 160, in load_approximate_nearest_neighbours_index. ann_index.loadIndex(cached_path(linker_paths.ann_index)). RuntimeError: Check failed: The number of stored elements 120302 doesn't match the number of data points 5869671! Did you forget to re-load data? ```. I have it running with no problem in a different environment with internet connection. So I also have tried copying the files downloaded by the script instead of those I downloaded using the browser. The difference is that the file names contains a hash (?), like this:. ```. 1a5445257d097c1d2a9eba040029329993377ebc82785ee9ad18ed2b86f7fc7d.bc94249222c42b975a55db3a2b6f7badffe87b809e02f16907fca650f787f2f3.concept_aliases.json. ... ```. After copying those files to the environment with no internet access, repointing them in candidate_generator,py, the error continues. If I try to run the same code, but using 'umls' instead of 'mesh' (and also downloading the files manually, copying to the server and changing in candidate_generator.py), it works! Am I missing something?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/379
https://github.com/allenai/scispacy/issues/380:154,energy efficiency,model,models,154,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:406,energy efficiency,model,models,406,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:45,integrability,compon,component,45,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:213,integrability,compon,components,213,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:373,integrability,compon,components,373,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:45,interoperability,compon,component,45,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:213,interoperability,compon,components,213,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:373,interoperability,compon,components,373,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:45,modifiability,compon,component,45,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:213,modifiability,compon,components,213,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:373,modifiability,compon,components,373,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:270,performance,perform,performance,270,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:328,safety,test,test,328,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:154,security,model,models,154,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:406,security,model,models,406,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:328,testability,test,test,328,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:118,usability,document,documents,118,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:270,usability,perform,performance,270,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/380:358,usability,minim,minimum,358,"Question: Is it necessary to use the tok2vec component?; Hi! I've been doing some NER on a reasonably large number of documents using the specialised NER models, and to speed things up I have disabled a number of components including `tok2vec`. Is this likely to affect performance? I don't have an evaluation set with which to test yet. Equally, what i the minimum set of components I need to run the NER models?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/380
https://github.com/allenai/scispacy/issues/381:123,energy efficiency,load,load,123,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:123,performance,load,load,123,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:15,usability,close,closed,15,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:152,usability,close,closed,152,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:198,usability,close,closed,198,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:246,usability,close,close,246,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/381:291,usability,close,close,291,"Embeddings of ""closed"" and ""open"" are similar in en_core_sci_lg; This seems strange to me:. ```. import spacy. nlp = spacy.load(""en_core_sci_lg""). nlp(""closed"").similarity(nlp(""open"")) # 0.81. nlp(""closed"").similarity(nlp(""opened"")) # 0.58. nlp(""close"").similarity(nlp(""open"")) # 0.26. nlp(""close"").similarity(nlp(""opened"")) # 0.20. ```",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/381
https://github.com/allenai/scispacy/issues/382:7,availability,down,downloading,7,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:117,availability,down,download,117,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:724,availability,error,error,724,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:696,deployability,fail,fails,696,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:27,energy efficiency,current,currently,27,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:724,performance,error,error,724,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:696,reliability,fail,fails,696,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:724,safety,error,error,724,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/382:724,usability,error,error,724,aws s3 downloading; **I am currently trying to train using my own corpus following the project.yml file.**. I try to download several files:. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/med_mentions.tar.gz assets/med_mentions.tar.gz. tar -xzvf assets/med_mentions.tar.gz -C assets/. rm assets/med_mentions.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '*' --include '*.tsv'. But it fails due to . '''. **fatal error: Unable to locate credentials**. '''. I am wondering if anyone know how to solve this problem. Thanks!!!,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/382
https://github.com/allenai/scispacy/issues/383:1279,availability,error,error,1279,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:233,deployability,manag,managed,233,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:277,deployability,manag,managed,277,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1561,deployability,instal,install,1561,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1986,deployability,instal,install,1986,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:233,energy efficiency,manag,managed,233,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:277,energy efficiency,manag,managed,277,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1138,energy efficiency,load,load,1138,"eset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1493,energy efficiency,current,current,1493,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1449,integrability,compon,component,1449,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1535,integrability,Transform,Transformer,1535,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1576,integrability,transform,transformers,1576,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1616,integrability,compon,component,1616,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1675,integrability,compon,component,1675,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1700,integrability,compon,components,1700,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1746,integrability,compon,components,1746,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2039,integrability,repositor,repository,2039,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:308,interoperability,specif,specifically,308,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1449,interoperability,compon,component,1449,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1535,interoperability,Transform,Transformer,1535,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1576,interoperability,transform,transformers,1576,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1616,interoperability,compon,component,1616,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1675,interoperability,compon,component,1675,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1700,interoperability,compon,components,1700,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1746,interoperability,compon,components,1746,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2039,interoperability,repositor,repository,2039,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1449,modifiability,compon,component,1449,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1616,modifiability,compon,component,1616,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1654,modifiability,deco,decorator,1654,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1675,modifiability,compon,component,1675,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1700,modifiability,compon,components,1700,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1746,modifiability,compon,components,1746,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1138,performance,load,load,1138,"eset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1279,performance,error,error,1279,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:233,safety,manag,managed,233,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:277,safety,manag,managed,277,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1279,safety,error,error,1279,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1813,testability,understand,understanding,1813,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2088,testability,understand,understanding,2088,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:14,usability,custom,custom,14,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:95,usability,custom,custom,95,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:156,usability,help,helpful,156,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:341,usability,clear,clear,341,"Trying to add custom Knowledge base; Hello, I read several previous discussions about adding a custom KB to scispacy instead of using the preset ones and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1279,usability,error,error,1279,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1442,usability,custom,custom,1442,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:1609,usability,custom,custom,1609,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/383:2113,usability,help,help,2113,"es and a helpful discussion I found was in: #331 and #237. I followed the steps and I managed to run the script with my jsonl and managed to get some outputs (4 specifically) but I am not quite clear how to proceed from here. Here is my code (I ran on jupyter notebook):. ```. from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES. from scispacy.candidate_generation import (. CandidateGenerator,. LinkerPaths. ). from scispacy.linking_utils import KnowledgeBase. import spacy. CustomLinkerPaths_mycustom = LinkerPaths(. ann_index=""output/nmslib_index.bin"",. tfidf_vectorizer=""output//nmslib_index.bin"",. tfidf_vectors=""output/tfidf_vectorizer.joblib"",. concept_aliases_list=""output/concept_aliases.json"",. ). class myKnowledgeBase(KnowledgeBase):. def __init__(. self,. file_path: str = ""custom_kb.jsonl"",. ):. super().__init__(file_path). DEFAULT_PATHS[""myCustom""] = CustomLinkerPaths_mycustom. DEFAULT_KNOWLEDGE_BASES[""myCustom""] = myKnowledgeBase. nlp = spacy.load(""new_model""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""myCustom""}). ```. This gave me the error:. ```. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). ```. So I was wondering how I should do this. From my understanding, we would like to add this new `linker_name` and its paths to the scispacy EntityLinker class but is there a way to do it from importing the scispacy from pip install. Or is the only method to clone the scispacy repository and edit the EntityLinker file? Is my understanding right? Any help would be appreciated. Thanks!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/383
https://github.com/allenai/scispacy/issues/384:118,safety,test,test,118,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:201,safety,test,tested,201,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:118,testability,test,test,118,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:201,testability,test,tested,201,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:30,usability,custom,custom,30,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:88,usability,custom,custom,88,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/384:733,usability,help,helps,733,"kb_ents gives no results from custom KB; Following this discussion #383, where I got my custom KB to work. I tried to test the code and for some reason it is not giving me anything. Here is the code I tested it with:. ```. linker = CandidateGenerator(name=""myCustom""). text = ""TR Max Velocity: 2.3 m/s"". doc = nlp(text). spacy.displacy.render(doc, style = ""ent"", jupyter = True). entity = doc.ents[2]. print(""Name: "", entity). for umls_ent in entity._.kb_ents:. print(umls_ent). print(linker.kb.cui_to_entity[umls_ent[0]]). print(""----------------------""). ```. This would give:. ```. Name: m/s. ```. there was no `----------------------` which means it did not even enter the for loop. I was wondering why this is the case. If this helps, this is the jsonl file that I ran this script (https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py) with:. ```. ... {""concept_id"": ""U0013"", ""aliases"": [""m/s""], ""types"": [""UN1T5""], ""canonical_name"": ""m/s""}. ... ```.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/384
https://github.com/allenai/scispacy/issues/385:44,deployability,instal,install,44,python 3.8 or above; It seems that I cannot install scispacy if I am running a M1 Mac machine if I want to benefit the the native version of python. Is there any resolve to this issue? I am using miniforge to set up my environment.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/385
https://github.com/allenai/scispacy/issues/385:130,deployability,version,version,130,python 3.8 or above; It seems that I cannot install scispacy if I am running a M1 Mac machine if I want to benefit the the native version of python. Is there any resolve to this issue? I am using miniforge to set up my environment.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/385
https://github.com/allenai/scispacy/issues/385:130,integrability,version,version,130,python 3.8 or above; It seems that I cannot install scispacy if I am running a M1 Mac machine if I want to benefit the the native version of python. Is there any resolve to this issue? I am using miniforge to set up my environment.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/385
https://github.com/allenai/scispacy/issues/385:130,modifiability,version,version,130,python 3.8 or above; It seems that I cannot install scispacy if I am running a M1 Mac machine if I want to benefit the the native version of python. Is there any resolve to this issue? I am using miniforge to set up my environment.,MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/385
https://github.com/allenai/scispacy/issues/386:26,availability,down,downloading,26,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:793,availability,error,error,793,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:929,availability,error,error,929,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:939,availability,error,error,939,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:988,availability,operat,operation,988,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1109,availability,error,error,1109,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1119,availability,error,error,1119,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1180,availability,operat,operation,1180,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1285,availability,Servic,Services,1285,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1517,availability,down,download,1517,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1545,availability,down,download,1545,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1285,deployability,Servic,Services,1285,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:209,integrability,pub,public,209,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:259,integrability,pub,publicly,259,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1285,integrability,Servic,Services,1285,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1405,integrability,pub,public,1405,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1479,interoperability,format,format,1479,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1285,modifiability,Servic,Services,1285,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:793,performance,error,error,793,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:929,performance,error,error,929,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:939,performance,error,error,939,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1109,performance,error,error,1109,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1119,performance,error,error,1119,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:775,reliability,doe,doesn,775,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:337,safety,permiss,permissions,337,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:793,safety,error,error,793,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:929,safety,error,error,929,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:939,safety,error,error,939,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1109,safety,error,error,1109,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1119,safety,error,error,1119,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1135,security,Access,AccessDenied,1135,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1191,security,Access,Access,1191,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:793,usability,error,error,793,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:929,usability,error,error,929,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:939,usability,error,error,939,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1109,usability,error,error,1109,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/386:1119,usability,error,error,1119,"Problems Following aws s3 downloading #382; _Do you have this issue for all files (e.g. try just this one s3://ai2-s2-scispacy/data/umls_semantic_type_tree.tsv)? Unfortunately I cannot make the ontonotes file public due to licensing, but the others should be publicly readable. Let me know if they are not and I'll investigate which AWS permissions I may have set incorrectly. Thanks!_. This one works for me. I try again and now. '''. aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. tar -xzvf assets/ud_ontonotes.tar.gz -C assets/. rm assets/ud_ontonotes.tar.gz. #############################################################. aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. '''. These two still doesn't work. The error information is in the following. '''. $ aws s3 cp s3://ai2-s2-scispacy/data/ud_ontonotes.tar.gz assets/ud_ontonotes.tar.gz. fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden. $ aws s3 cp s3://ai2-s2-scispacy/data/ner/ assets --recursive --exclude '' --include '.tsv'. fatal error: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied. '''. I am wondering what kind of licensing do I need? Is that UMLS Terminology Services (UTS) (Link: [https://uts.nlm.nih.gov/uts/])? I already have one for this. If some of the files cannot be made public, can you show the details how to prepare it?(For example, the data format(one line of example), where to download the raw file(I can download by myself if I can apply for that licence), then how to convert the data into ud_ontonotes.tar.gz). Thank you so much!!!",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/386
https://github.com/allenai/scispacy/issues/387:21,deployability,contain,contain,21,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:49,energy efficiency,model,models,49,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:175,energy efficiency,model,model,175,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:37,integrability,transform,transformer,37,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:37,interoperability,transform,transformer,37,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:5,reliability,doe,does,5,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:300,reliability,doe,does,300,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:49,security,model,models,49,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:175,security,model,model,175,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/387:136,usability,efficien,efficiently,136,"What does Doc.tensor contain for non-transformer models?; Hi, we are processing large amounts of text and need to serialize Doc objects efficiently. We are using the `sci_md` model, and it appears that when converting a Doc to bytes, the majority of the space is taken by the `Doc.tensor` data. What does that data represent exactly? Is it static, and/or do I have to include it in each serialized Doc object?",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387
https://github.com/allenai/scispacy/issues/388:2770,deployability,configurat,configurations,2770,"apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionality to be):. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. def consolidated_entities_tuple(text: str, long_form_abbrev_ents: bool, model_list: list, scispacy_linker_config: dict):. # place code for function here, likely to utilize the imported modules above. return (nlp, doc). text = ""Spinal ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:3728,deployability,modul,modules,3728,"d""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionality to be):. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. def consolidated_entities_tuple(text: str, long_form_abbrev_ents: bool, model_list: list, scispacy_linker_config: dict):. # place code for function here, likely to utilize the imported modules above. return (nlp, doc). text = ""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily."". tup = consolidated_entities_tuple(text, True, [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""], . {""resolve_abbreviations"": True, ""filter_for_definitions"": False, . ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). nlp = tup[0]. doc = tup[1]. # Let's look at the first entity. entity = doc.ents[0]. print(""Name: "", entity). >>> Name: Spinal and bulbar muscular atrophy. # Each entity is linked to UMLS with a score. # (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation o",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:43,energy efficiency,Model,Models,43,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:329,energy efficiency,power,powerful,329,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:378,energy efficiency,model,models,378,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:475,energy efficiency,model,model,475,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:510,energy efficiency,model,model,510,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:539,energy efficiency,model,model,539,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:738,energy efficiency,model,model,738,"Combining Entities Recognized by Different Models & by the AbbreviationDetector; I recently encountered both spaCy and ScispaCy and so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1127,energy efficiency,model,models,1127,"d so far I think ScispaCy is an awesome tool to be able to identify and link biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1204,energy efficiency,model,model,1204,"biomedical entities found in text with concepts from UMLS and other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, u",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1263,energy efficiency,model,models,1263,"nd other knowledge bases. I was thinking it would be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next lo",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1313,energy efficiency,model,models,1313,"be even more powerful if the entities identified by different models and by the AbbreviationDetector can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1414,energy efficiency,model,models,1414,"can be combined. This would allow the shortcomings of one model to be compensated by another model. It would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be rough",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1516,energy efficiency,model,model,1516," would also allow a model's shortcomings to be compensated by the long forms of any detected abbreviations. For example, the identified entities in ""Spinal and bulbar muscular atrophy (SBMA)"" using the `en_core_sci_lg` model in the [ScispaCy Demo](url) are: . - ""Spinal"". - ""bulbar muscular atrophy"". - ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be iden",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1818,energy efficiency,model,model,1818," ""SBMA"". However, after adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including th",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1984,energy efficiency,model,model,1984,"he entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple wi",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2240,energy efficiency,model,model,2240,"ne entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the prop",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2653,energy efficiency,model,models,2653,"are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionality to be):. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. def consolidated_entities_tuple(text: str, long_form_abbrev_ents: bool, model_list: list, scispacy_linker_config:",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:4419,energy efficiency,current,currently,4419,"y. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. def consolidated_entities_tuple(text: str, long_form_abbrev_ents: bool, model_list: list, scispacy_linker_config: dict):. # place code for function here, likely to utilize the imported modules above. return (nlp, doc). text = ""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily."". tup = consolidated_entities_tuple(text, True, [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""], . {""resolve_abbreviations"": True, ""filter_for_definitions"": False, . ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). nlp = tup[0]. doc = tup[1]. # Let's look at the first entity. entity = doc.ents[0]. print(""Name: "", entity). >>> Name: Spinal and bulbar muscular atrophy. # Each entity is linked to UMLS with a score. # (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the. 				gene encoding the ANDROGEN RECEPTOR. >>> TUI(s): T047. >>> Aliases (abbreviated, total: 50):. Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0752353, Name: Atrophy, Muscular, Spinobulbar. >>> Definition: ..... >>> TUI(s): T047. >>> Aliases: (total: ?):. ... , ... , ... , ... >>> ..... # Now let's look at the abbreviations in the text. print(""Abbreviation"", ""\t"", ""Definition""). for abrv in doc._.abbreviations:. 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition. >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy. >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy. >>> AR 		 (29, 30)",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:5725,energy efficiency,power,powerful,5725,"ar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily."". tup = consolidated_entities_tuple(text, True, [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""], . {""resolve_abbreviations"": True, ""filter_for_definitions"": False, . ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). nlp = tup[0]. doc = tup[1]. # Let's look at the first entity. entity = doc.ents[0]. print(""Name: "", entity). >>> Name: Spinal and bulbar muscular atrophy. # Each entity is linked to UMLS with a score. # (currently just char-3gram matching). linker = nlp.get_pipe(""scispacy_linker""). for umls_ent in entity._.kb_ents:. 	print(linker.kb.cui_to_entity[umls_ent[0]]). >>> CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. >>> Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the. 				gene encoding the ANDROGEN RECEPTOR. >>> TUI(s): T047. >>> Aliases (abbreviated, total: 50):. Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, .... >>> CUI: C0752353, Name: Atrophy, Muscular, Spinobulbar. >>> Definition: ..... >>> TUI(s): T047. >>> Aliases: (total: ?):. ... , ... , ... , ... >>> ..... # Now let's look at the abbreviations in the text. print(""Abbreviation"", ""\t"", ""Definition""). for abrv in doc._.abbreviations:. 	print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). >>> Abbreviation	 Span	 Definition. >>> SBMA 		 (33, 34) Spinal and bulbar muscular atrophy. >>> SBMA 	 	 (6, 7) Spinal and bulbar muscular atrophy. >>> AR 		 (29, 30) androgen receptor. ```. Thank you for taking the time to read this. If this sort of function already exists in ScispaCy, please let me know. Otherwise, if this sort of function or some other code that accomplishes the same thing can be added to ScispaCy, that would be awesome. I believe it can be a powerful addition to the library. Let me know your thoughts.",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2392,integrability,compon,components,2392,"und by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionalit",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2770,integrability,configur,configurations,2770,"apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionality to be):. ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. def consolidated_entities_tuple(text: str, long_form_abbrev_ents: bool, model_list: list, scispacy_linker_config: dict):. # place code for function here, likely to utilize the imported modules above. return (nlp, doc). text = ""Spinal ",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2392,interoperability,compon,components,2392,"und by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two items:. - The nlp object that can be used to make the linker to the utilized knowledge base. - A Doc object with the longest length entities that also have matches above the user's desired mention threshold. Here is how use of the proposed function, which I call `consolidated_entities_tuple` might look like (This is NOT functioning code, just an example of how I imagine the functionalit",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:1836,modifiability,inherit,inherited,1836,"ter adding the AbbreviationDetector as a pipe, we would recognize ""SBMA"" as an abbreviation for ""Spinal and bulbar muscular atrophy"", so really, the entities should be the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g.,",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
https://github.com/allenai/scispacy/issues/388:2002,modifiability,inherit,inherited,2002,"e the following, but they are not corrected as such:. - ""Spinal and bulbar muscular atrophy"". - ""SBMA"". Similarly, some models may identify fragments of a phrase as separate entities while another model may recognize a whole phrase as one entity. Or, some models may recognize certain entities while other models may completely ignore them. If there is some way of consolidating entities found by different models, then a more accurate and complete list of entities will be obtained than just using any given model individually. There are also times when a longer phrased entity is not always better, because it may yield poor matching results that are below the desired mention threshold for a given knowledge base. For example, in the [ScispaCy Demo](https://scispacy.apps.allenai.org/), the `en_core_sci_md` model identifies ""inherited motor neuron disease"" as an entity but gives no results satisfying the mention threshold of 0.85. On the other hand, the `en_core_sci_sm` model identifies ""inherited"" and ""motor neuron disease"" as separate entities, each of which have matches above the 0.85 mention threshold. Therefore, it may generally be helpful to also keep track of any related original, unconsolidated entities from each model and pick the next longest phrased entities that have matching results above the desired mention threshold. Overall, a function with the following components would be roughly what I'm looking for:. - Parameters to take in:. - The text string from which entities will be identified. - A boolean for whether or not to identify the long forms of abbreviations as entities. (e.g., True). - A list of the desired models to use (e.g., [""en_core_sci_sm"", ""en_core_sci_scibert"", ""en_ner_bc5cdr_md""]). - A dictionary with any desired configurations of the scispacy linker, including the linker name (e.g., {""resolve_abbreviations"": True, ""filter_for_definitions"": False, ""no_definition_threshold"": 0.85, ""linker_name"": ""umls""}). . - Output: A tuple with the following two",MatchSource.ISSUE,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388
