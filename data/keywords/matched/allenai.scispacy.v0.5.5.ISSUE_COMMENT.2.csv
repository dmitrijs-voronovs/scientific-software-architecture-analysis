id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/allenai/scispacy/issues/258:8499,modifiability,pac,packages,8499," ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8559,modifiability,modul,module,8559,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8666,modifiability,pac,packages,8666,"hon/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);co",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8711,modifiability,modul,module,8711,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8836,modifiability,pac,packages,8836,"y"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8T",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8880,modifiability,modul,module,8880,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. T",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8988,modifiability,pac,packages,8988," line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9037,modifiability,modul,module,9037,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9091,modifiability,modul,module,9091,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9947,modifiability,modul,module,9947,". File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:10017,modifiability,modul,module,10017,""", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_cla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:10162,modifiability,pac,packages,10162,"-------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:10496,modifiability,pac,packages,10496,"[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:10775,modifiability,pac,packages,10775,"ist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/githu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11199,modifiability,pac,packages,11199,"5, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11344,modifiability,pac,packages,11344,"_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11460,modifiability,pac,packages,11460," ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11520,modifiability,modul,module,11520,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11627,modifiability,pac,packages,11627,"hon/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);co",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11672,modifiability,modul,module,11672,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11797,modifiability,pac,packages,11797,"y"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8T",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11841,modifiability,modul,module,11841,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Tr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11949,modifiability,pac,packages,11949," line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11998,modifiability,modul,module,11998,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package()",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12052,modifiability,modul,module,12052,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12907,modifiability,modul,module,12907,"n. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12977,modifiability,modul,module,12977,"y"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_cla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:13122,modifiability,pac,packages,13122,"--------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:13456,modifiability,pac,packages,13456,"rgv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:13735,modifiability,pac,packages,13735,"dist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/githu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14159,modifiability,pac,packages,14159,"5, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14304,modifiability,pac,packages,14304,"_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored ou",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14420,modifiability,pac,packages,14420," ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14480,modifiability,modul,module,14480,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14587,modifiability,pac,packages,14587,"hon/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14632,modifiability,modul,module,14632,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14757,modifiability,pac,packages,14757,"y"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14801,modifiability,modul,module,14801,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Runn",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14909,modifiability,pac,packages,14909," line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. Fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14958,modifiability,modul,module,14958,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15012,modifiability,modul,module,15012,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15902,modifiability,modul,module,15902,"ite-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15972,modifiability,modul,module,15972,"command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16191,modifiability,modul,module,16191," (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clea",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16305,modifiability,modul,module,16305,"t with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16421,modifiability,modul,module,16421," = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16582,modifiability,modul,module,16582,");code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you mu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16631,modifiability,modul,module,16631,"'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . -----------------------------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17449,modifiability,version,versioned,17449,"red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18396,modifiability,modul,module,18396,"cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18466,modifiability,modul,module,18466,"sn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18725,modifiability,pac,packages,18725," started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19004,modifiability,pac,packages,19004,"le__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/githu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19428,modifiability,pac,packages,19428,"7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19573,modifiability,pac,packages,19573,"_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command er",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19689,modifiability,pac,packages,19689," ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19749,modifiability,modul,module,19749,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19856,modifiability,pac,packages,19856,"hon/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19901,modifiability,modul,module,19901,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20026,modifiability,pac,packages,20026,"y"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20070,modifiability,modul,module,20070,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20178,modifiability,pac,packages,20178," line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20227,modifiability,modul,module,20227,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20281,modifiability,modul,module,20281,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. Fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21118,modifiability,extens,extension,21118,"in_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21157,modifiability,Extens,Extension,21157,"jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jyth",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21273,modifiability,modul,module,21273,"or: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21344,modifiability,modul,module,21344,"----------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21397,modifiability,pac,packages,21397,"hlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21731,modifiability,pac,packages,21731,"andir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22010,modifiability,pac,packages,22010," cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/githu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22434,modifiability,pac,packages,22434,"5, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22579,modifiability,pac,packages,22579,"_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not bui",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22695,modifiability,pac,packages,22695," ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall b",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22755,modifiability,modul,module,22755,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22862,modifiability,pac,packages,22862,"hon/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22907,modifiability,modul,module,22907,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at h",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23032,modifiability,pac,packages,23032,"y"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23076,modifiability,modul,module,23076,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, config",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23184,modifiability,pac,packages,23184," line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: E",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23233,modifiability,modul,module,23233,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. F",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23287,modifiability,modul,module,23287,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_inter",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23971,modifiability,pac,packages,23971,"i_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24271,modifiability,pac,packages,24271,"buteError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24406,modifiability,pac,packages,24406,"r. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. ass",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24542,modifiability,pac,packages,24542,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24678,modifiability,pac,packages,24678,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24810,modifiability,pac,packages,24810,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24942,modifiability,pac,packages,24942,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:25073,modifiability,pac,packages,25073,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:25191,modifiability,pac,packages,25191,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:25325,modifiability,pac,packages,25325,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:103,performance,error,error,103,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:145,performance,ERROR,ERROR,145,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:160,performance,error,errored,160,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2090,performance,load,load,2090,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2185,performance,load,load,2185,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2926,performance,ERROR,ERROR,2926,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3325,performance,cach,cache,3325,"_(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setupt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3508,performance,error,error,3508,"et_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3516,performance,ERROR,ERROR,3516,"ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3531,performance,error,errored,3531,", get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5451,performance,load,load,5451,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5546,performance,load,load,5546,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jytho",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6287,performance,ERROR,ERROR,6287,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6464,performance,error,error,6464,"nt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6472,performance,ERROR,ERROR,6472,"hub/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6487,performance,error,errored,6487,"ite-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8339,performance,load,load,8339,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8434,performance,load,load,8434,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jyt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9175,performance,ERROR,ERROR,9175,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setupto",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9354,performance,error,error,9354,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9362,performance,ERROR,ERROR,9362,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9377,performance,error,errored,9377,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jyth",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11300,performance,load,load,11300,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finishe",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11395,performance,load,load,11395,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/j",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12136,performance,ERROR,ERROR,12136,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptoo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12317,performance,error,error,12317,"/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12325,performance,ERROR,ERROR,12325,"jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12340,performance,error,errored,12340,"packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14260,performance,load,load,14260,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14355,performance,load,load,14355,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jyt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15096,performance,ERROR,ERROR,15096,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15275,performance,error,error,15275,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15283,performance,ERROR,ERROR,15283,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15298,performance,error,errored,15298,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16703,performance,ERROR,ERROR,16703,"l -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16777,performance,ERROR,ERROR,16777,"output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16792,performance,error,errored,16792,"):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with stat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17647,performance,ERROR,ERROR,17647,"rocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17800,performance,error,error,17800," with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17808,performance,ERROR,ERROR,17808,"it status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17823,performance,error,errored,17823,"mmand: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19529,performance,load,load,19529,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): f",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19624,performance,load,load,19624,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/gi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20365,performance,ERROR,ERROR,20365,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20552,performance,error,error,20552,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20560,performance,ERROR,ERROR,20560,"/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20575,performance,error,errored,20575,"es/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_com",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22535,performance,load,load,22535,""", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly num",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22630,performance,load,load,22630,"/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23371,performance,ERROR,ERROR,23371,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24180,performance,ERROR,ERROR,24180,"-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/L",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2933,reliability,Fail,Failed,2933,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6294,reliability,Fail,Failed,6294,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9182,reliability,Fail,Failed,9182,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12143,reliability,Fail,Failed,12143,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15103,reliability,Fail,Failed,15103,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16710,reliability,Fail,Failed,16710,"mp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17466,reliability,doe,doesn,17466,"lt_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <mod",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17654,reliability,Fail,Failed,17654,"ng. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20372,reliability,Fail,Failed,20372,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/g",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23378,reliability,Fail,Failed,23378,"_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23486,reliability,Fail,Failed,23486,"class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:103,safety,error,error,103,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:145,safety,ERROR,ERROR,145,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:160,safety,error,errored,160,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:639,safety,Compl,Complete,639,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:733,safety,modul,module,733,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:805,safety,modul,module,805,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2310,safety,modul,module,2310,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /h",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2462,safety,modul,module,2462,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel f",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2631,safety,modul,module,2631,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2788,safety,modul,module,2788,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2842,safety,modul,module,2842,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2926,safety,ERROR,ERROR,2926,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3508,safety,error,error,3508,"et_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3516,safety,ERROR,ERROR,3516,"ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3531,safety,error,errored,3531,", get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:4004,safety,Compl,Complete,4004,"ding wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:4098,safety,modul,module,4098,"shed with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:4168,safety,modul,module,4168,"hash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_cla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5671,safety,modul,module,5671,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__=",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5823,safety,modul,module,5823,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5992,safety,modul,module,5992,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_CO",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6149,safety,modul,module,6149,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <modu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6203,safety,modul,module,6203,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6287,safety,ERROR,ERROR,6287,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6464,safety,error,error,6464,"nt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6472,safety,ERROR,ERROR,6472,"hub/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6487,safety,error,errored,6487,"ite-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6957,safety,Compl,Complete,6957,"l/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:7079,safety,modul,module,7079,"b/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:7148,safety,modul,module,7148,"dule>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8559,safety,modul,module,8559,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8711,safety,modul,module,8711,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8880,safety,modul,module,8880,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. T",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9037,safety,modul,module,9037,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9091,safety,modul,module,9091,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9175,safety,ERROR,ERROR,9175,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setupto",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9354,safety,error,error,9354,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9362,safety,ERROR,ERROR,9362,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9377,safety,error,errored,9377,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jyth",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9853,safety,Compl,Complete,9853,"tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9947,safety,modul,module,9947,". File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:10017,safety,modul,module,10017,""", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_cla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11520,safety,modul,module,11520,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11672,safety,modul,module,11672,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11841,safety,modul,module,11841,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Tr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11998,safety,modul,module,11998,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package()",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12052,safety,modul,module,12052,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12136,safety,ERROR,ERROR,12136,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptoo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12317,safety,error,error,12317,"/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12325,safety,ERROR,ERROR,12325,"jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12340,safety,error,errored,12340,"packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12813,safety,Compl,Complete,12813,"5tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12907,safety,modul,module,12907,"n. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12977,safety,modul,module,12977,"y"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_cla",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14480,safety,modul,module,14480,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14632,safety,modul,module,14632,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14801,safety,modul,module,14801,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Runn",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14958,safety,modul,module,14958,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15012,safety,modul,module,15012,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15096,safety,ERROR,ERROR,15096,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15275,safety,error,error,15275,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15283,safety,ERROR,ERROR,15283,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15298,safety,error,errored,15298,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15771,safety,Compl,Complete,15771,"425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15902,safety,modul,module,15902,"ite-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15972,safety,modul,module,15972,"command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16191,safety,modul,module,16191," (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clea",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16305,safety,modul,module,16305,"t with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16342,safety,log,log,16342,"/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, us",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16399,safety,log,log,16399,", tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cl",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16421,safety,modul,module,16421," = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16582,safety,modul,module,16582,");code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you mu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16631,safety,modul,module,16631,"'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . -----------------------------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16703,safety,ERROR,ERROR,16703,"l -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16777,safety,ERROR,ERROR,16777,"output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16792,safety,error,errored,16792,"):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with stat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17239,safety,Compl,Complete,17239,"tall-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17647,safety,ERROR,ERROR,17647,"rocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17800,safety,error,error,17800," with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17808,safety,ERROR,ERROR,17808,"it status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17823,safety,error,errored,17823,"mmand: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18302,safety,Compl,Complete,18302," `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18396,safety,modul,module,18396,"cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18466,safety,modul,module,18466,"sn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19749,safety,modul,module,19749,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19901,safety,modul,module,19901,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20070,safety,modul,module,20070,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20227,safety,modul,module,20227,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20281,safety,modul,module,20281,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. Fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20365,safety,ERROR,ERROR,20365,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20552,safety,error,error,20552,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20560,safety,ERROR,ERROR,20560,"/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20575,safety,error,errored,20575,"es/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_com",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21054,safety,Compl,Complete,21054,"ine 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _par",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21273,safety,modul,module,21273,"or: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21344,safety,modul,module,21344,"----------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22755,safety,modul,module,22755,""", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22907,safety,modul,module,22907,"se_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at h",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23076,safety,modul,module,23076,"s = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, config",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23233,safety,modul,module,23233,"elf.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. F",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23287,safety,modul,module,23287,"thon/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_inter",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23371,safety,ERROR,ERROR,23371,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24180,safety,ERROR,ERROR,24180,"-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/L",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24187,safety,Except,Exception,24187,"/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:265,security,token,tokenize,265,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:411,security,token,tokenize,411,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:639,security,Compl,Complete,639,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3636,security,token,tokenize,3636,"from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3778,security,token,tokenize,3778,", in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jyt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:4004,security,Compl,Complete,4004,"ding wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6592,security,token,tokenize,6592,"ython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6732,security,token,tokenize,6732,"l=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6957,security,Compl,Complete,6957,"l/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9482,security,token,tokenize,9482,"hon/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9626,security,token,tokenize,9626,"File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9853,security,Compl,Complete,9853,"tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12445,security,token,tokenize,12445,"n/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12587,security,token,tokenize,12587,"File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jyt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12813,security,Compl,Complete,12813,"5tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15403,security,token,tokenize,15403,"hon/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"",",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15545,security,token,tokenize,15545,". File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15771,security,Compl,Complete,15771,"425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16342,security,log,log,16342,"/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, us",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16399,security,log,log,16399,", tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cl",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16897,security,token,tokenize,16897,"<module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17039,security,token,tokenize,17039,", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17239,security,Compl,Complete,17239,"tall-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17928,security,token,tokenize,17928,"ip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18074,security,token,tokenize,18074,"_);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18302,security,Compl,Complete,18302," `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20680,security,token,tokenize,20680,"site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_comma",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20826,security,token,tokenize,20826,"/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21054,security,Compl,Complete,21054,"ine 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _par",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:668,testability,Trace,Traceback,668,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:4033,testability,Trace,Traceback,4033,"up.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:7014,testability,Trace,Traceback,7014,"le import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_comman",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9882,testability,Trace,Traceback,9882,". from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12842,testability,Trace,Traceback,12842,">. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15837,testability,Trace,Traceback,15837,"act_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/g",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16342,testability,log,log,16342,"/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, us",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16399,testability,log,log,16399,", tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cl",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18331,testability,Trace,Traceback,18331,"rted, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_com",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21208,testability,Trace,Traceback,21208,"line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_comm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24199,testability,Trace,Traceback,24199,"x_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:25407,testability,assert,assert,25407,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:25440,testability,Assert,AssertionError,25440,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:52,usability,support,supports,52,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:103,usability,error,error,103,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:145,usability,ERROR,ERROR,145,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:152,usability,Command,Command,152,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:160,usability,error,errored,160,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:182,usability,statu,status,182,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:193,usability,command,command,193,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:507,usability,close,close,507,"So, now I am trying to run Scispacy in Jython which supports Python 2.7 but I am getting the following error: - . Any clue regarding this ? ```. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/preshed/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-QusvHE. cwd: /tmp/pip-install-K8TU7D/preshed/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 152, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:1813,usability,command,command,1813,"up_package(). File ""/tmp/pip-install-K8TU7D/preshed/setup.py"", line 116, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:1943,usability,command,command,1943,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2065,usability,command,command,2065,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:2926,usability,ERROR,ERROR,2926,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3111,usability,statu,status,3111,"hub/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3500,usability,statu,status,3500,"_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3508,usability,error,error,3508,"et_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3516,usability,ERROR,ERROR,3516,"ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3523,usability,Command,Command,3523,"_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3531,usability,error,errored,3531,", get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3553,usability,statu,status,3553,"""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3564,usability,command,command,3564,"ub/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:3874,usability,close,close,3874,"char'. ----------------------------------------. ERROR: Failed building wheel for preshed. Running setup.py clean for preshed. Building wheel for murmurhash (setup.py): started. Building wheel for murmurhash (setup.py): finished with status 'done'. Created wheel for murmurhash: filename=murmurhash-1.0.2-jy27-none-java11_0_8.whl size=5888 sha256=64f485d45727cee76ee7301c0493cef441063f4d8b954c1e0d7ccc9304ea2485. Stored in directory: /home/sachit/.cache/pip/wheels/15/e4/bd/9a3e3a5ac7ad943ff27bafc3a105ce572b73acd31019549ec2. Building wheel for thinc (setup.py): started. Building wheel for thinc (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/thinc/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-1ofVsa. cwd: /tmp/pip-install-K8TU7D/thinc/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 264, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(pa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5174,usability,command,command,5174,"etup_package(). File ""/tmp/pip-install-K8TU7D/thinc/setup.py"", line 201, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5304,usability,command,command,5304,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:5426,usability,command,command,5426,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6287,usability,ERROR,ERROR,6287,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6456,usability,statu,status,6456,"File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6464,usability,error,error,6464,"nt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6472,usability,ERROR,ERROR,6472,"hub/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6479,usability,Command,Command,6479,"on/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6487,usability,error,errored,6487,"ite-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6509,usability,statu,status,6509,"urces/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6520,usability,command,command,6520,"__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jytho",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:6828,usability,close,close,6828," from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for thinc. Running setup.py clean for thinc. Building wheel for blis (setup.py): started. Building wheel for blis (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/blis/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_Nn0OT. cwd: /tmp/pip-install-K8TU7D/blis/. Complete output (34 lines):. ('BLIS_COMPILER?', 'None'). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptoo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8062,usability,command,command,8062," line 1, in <module>. File ""/tmp/pip-install-K8TU7D/blis/setup.py"", line 239, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8192,usability,command,command,8192,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:8314,usability,command,command,8314,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasab",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9175,usability,ERROR,ERROR,9175,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setupto",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9346,usability,statu,status,9346,"le ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9354,usability,error,error,9354,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9362,usability,ERROR,ERROR,9362,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9369,usability,Command,Command,9369,"/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/git",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9377,usability,error,errored,9377,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jyth",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9399,usability,statu,status,9399,"ces/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9410,usability,command,command,9410,".py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:9722,usability,close,close,9722,".pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for blis. Running setup.py clean for blis. Building wheel for wasabi (setup.py): started. Building wheel for wasabi (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/wasabi/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-6GDqjI. cwd: /tmp/pip-install-K8TU7D/wasabi/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 41, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11023,usability,command,command,11023,"etup_package(). File ""/tmp/pip-install-K8TU7D/wasabi/setup.py"", line 24, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11153,usability,command,command,11153,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:11275,usability,command,command,11275,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for sr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12136,usability,ERROR,ERROR,12136,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptoo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12309,usability,statu,status,12309," ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_lin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12317,usability,error,error,12317,"/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). Fil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12325,usability,ERROR,ERROR,12325,"jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12332,usability,Command,Command,12332,"ib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12340,usability,error,errored,12340,"packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12362,usability,statu,status,12362,"s/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12373,usability,command,command,12373,"y"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:12683,usability,close,close,12683,".pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for wasabi. Running setup.py clean for wasabi. Building wheel for srsly (setup.py): started. Building wheel for srsly (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/srsly/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-JqCOON. cwd: /tmp/pip-install-K8TU7D/srsly/. Complete output (35 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 199, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(pa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:13983,usability,command,command,13983,"etup_package(). File ""/tmp/pip-install-K8TU7D/srsly/setup.py"", line 158, in setup_package. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14113,usability,command,command,14113,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:14235,usability,command,command,14235,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for nump",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15096,usability,ERROR,ERROR,15096,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15267,usability,statu,status,15267,"le ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/dis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15275,usability,error,error,15275,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15283,usability,ERROR,ERROR,15283,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15290,usability,Command,Command,15290,"/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15298,usability,error,errored,15298,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15320,usability,statu,status,15320,"ces/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from nump",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15331,usability,command,command,15331,".py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:15641,usability,close,close,15641,"m .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for srsly. Running setup.py clean for srsly. Building wheel for numpy (setup.py): started. Building wheel for numpy (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16703,usability,ERROR,ERROR,16703,"l -d /tmp/pip-wheel-hiH_Wq. cwd: /tmp/pip-install-K8TU7D/numpy/. Complete output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16777,usability,ERROR,ERROR,16777,"output (16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16784,usability,Command,Command,16784,"16 lines):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16792,usability,error,errored,16792,"):. Running from numpy source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with stat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16814,usability,statu,status,16814,"y source directory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Co",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:16825,usability,command,command,16825,"ectory. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 419, in <module>. setup_package(). File ""/tmp/pip-install-K8TU7D/numpy/setup.py"", line 398, in setup_package. from numpy.distutils.core import setup. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errore",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17135,usability,close,close,17135,"TU7D/numpy/numpy/distutils/__init__.py"", line 6, in <module>. from . import ccompiler. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/ccompiler.py"", line 18, in <module>. from numpy.distutils import log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace(",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17331,usability,support,supported,17331,"mport log. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/log.py"", line 10, in <module>. from .misc_util import (red_text, default_text, cyan_text, green_text,. File ""/tmp/pip-install-K8TU7D/numpy/numpy/distutils/misc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Trace",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17551,usability,command,command,17551,"isc_util.py"", line 12, in <module>. import multiprocessing. ImportError: No module named multiprocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17647,usability,ERROR,ERROR,17647,"rocessing. ----------------------------------------. ERROR: Failed building wheel for numpy. Running setup.py clean for numpy. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17792,usability,statu,status,17792,"ored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17800,usability,error,error,17800," with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17808,usability,ERROR,ERROR,17808,"it status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17815,usability,Command,Command,17815,"s 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_co",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17823,usability,error,errored,17823,"mmand: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17845,usability,statu,status,17845,"jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:17856,usability,command,command,17856,"ython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/numpy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jyt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:18170,usability,close,close,18170,"""'""'exec'""'""'))' clean --all. cwd: /tmp/pip-install-K8TU7D/numpy. Complete output (10 lines):. Running from numpy source directory. . `setup.py clean` is not supported, use one of the following instead:. . - `git clean -xdf` (cleans all files). - `git clean -Xdf` (cleans all versioned files, doesn't touch. files that aren't checked into the git repo). . Add `--force` to your command to use it anyway if you must (unsupported). . ----------------------------------------. ERROR: Failed cleaning build dir for numpy. Building wheel for pathlib (setup.py): started. Building wheel for pathlib (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/pathlib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-L42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19252,usability,command,command,19252,"42Bzi. cwd: /tmp/pip-install-K8TU7D/pathlib/. Complete output (31 lines):. Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19382,usability,command,command,19382,"ne 1, in <module>. File ""/tmp/pip-install-K8TU7D/pathlib/setup.py"", line 6, in <module>. setup(. File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:19504,usability,command,command,19504,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20365,usability,ERROR,ERROR,20365,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20544,usability,statu,status,20544,"/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 13",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20552,usability,error,error,20552,"b/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in se",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20560,usability,ERROR,ERROR,20560,"/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20567,usability,Command,Command,20567,"e-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20575,usability,error,errored,20575,"es/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_com",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20597,usability,statu,status,20597,"it__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20608,usability,command,command,20608,"ne 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:20922,usability,close,close,20922,"s import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:21136,usability,User,UserWarning,21136,"File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for pathlib. Running setup.py clean for pathlib. Building wheel for scandir (setup.py): started. Building wheel for scandir (setup.py): finished with status 'error'. ERROR: Command errored out with exit status 1:. command: /mnt/d/github/jython/bin/jython -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""'; __file__='""'""'/tmp/pip-install-K8TU7D/scandir/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-st4OQK. cwd: /tmp/pip-install-K8TU7D/scandir/. Complete output (35 lines):. /mnt/d/github/jython/Lib/distutils/extension.py:133: UserWarning: Unknown Extension options: 'optional'. warnings.warn(msg). Traceback (most recent call last):. File ""<string>"", line 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22258,usability,command,command,22258,"ine 1, in <module>. File ""/tmp/pip-install-K8TU7D/scandir/setup.py"", line 51, in <module>. setup(. File ""/mnt/d/github/jython/Lib/site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fiel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22388,usability,command,command,22388,"site-packages/setuptools/__init__.py"", line 145, in setup. return distutils.core.setup(**attrs). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed buildin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:22510,usability,command,command,22510,"n/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/distutils/core.py"", line 137, in setup. ok = dist.parse_command_line(). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 703, in parse_command_line. result = _Distribution.parse_command_line(self). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 467, in parse_command_line. args = self._parse_command_opts(parser, args). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 1018, in _parse_command_opts. nargs = _Distribution._parse_command_opts(self, parser, args). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/distutils/dist.py"", line 523, in _parse_command_opts. cmd_class = self.get_command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23371,usability,ERROR,ERROR,23371,"command_class(command). File ""/mnt/d/github/jython/Lib/site-packages/setuptools/dist.py"", line 838, in get_command_class. self.cmdclass[command] = cmdclass = ep.load(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2434, in load. return self.resolve(). File ""/mnt/d/github/jython/Lib/site-packages/pkg_resources/__init__.py"", line 2440, in resolve. module = __import__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:23764,usability,support,support,23764,"port__(self.module_name, fromlist=['__name__'], level=0). File ""/mnt/d/github/jython/Lib/site-packages/wheel/bdist_wheel.py"", line 24, in <module>. from .pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag, get_platform. File ""/mnt/d/github/jython/Lib/site-packages/wheel/pep425tags.py"", line 10, in <module>. from .macosx_libfile import extract_macosx_min_system_version. File ""/mnt/d/github/jython/Lib/site-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24180,usability,ERROR,ERROR,24180,"-packages/wheel/macosx_libfile.py"", line 132, in <module>. segment_command_fields = [. AttributeError: 'module' object has no attribute 'c_char'. ----------------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/L",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24336,usability,statu,status,24336,"------------------------------. ERROR: Failed building wheel for scandir. Running setup.py clean for scandir. Successfully built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24701,usability,command,commands,24701,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:24833,usability,command,commands,24833,"y built cymem murmurhash. Failed to build preshed thinc blis wasabi srsly numpy pathlib scandir. DEPRECATION: Could not build wheels for preshed, thinc, blis, wasabi, srsly, numpy, pathlib, scandir which do not use PEP 517. pip will fall back to legacy 'setup.py install' for these. pip 21.0 will remove support for this functionality. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368. Installing collected packages: setuptools, wheel, cython, cymem, murmurhash, preshed, numpy, blis, wasabi, pathlib, srsly, configparser, contextlib2, zipp, scandir, six, pathlib2, importlib-metadata, catalogue, plac, tqdm, thinc. ERROR: Exception:. Traceback (most recent call last):. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/base_command.py"", line 216, in _main. status = self.run(options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/cli/req_command.py"", line 182, in wrapper. return func(self, options, args). File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/commands/install.py"", line 412, in run. installed = install_given_reqs(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/__init__.py"", line 82, in install_given_reqs. requirement.install(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/req/req_install.py"", line 823, in install. install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 821, in install_wheel. _install_wheel(. File ""/mnt/d/github/jython/Lib/site-packages/pip/_internal/operations/install/wheel.py"", line 703, in _install_wheel. assert os.path.exists(pyc_path). AssertionError. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:185,deployability,upgrad,upgrading,185,"Hi @Sachit1137 - python 2.7 reached it's end of life on Jan 1st, 2020 - scispacy certainly won't work with it, because it uses features from python 3.6+ (type hints etc). I'd recommend upgrading to python 3.6+.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:185,modifiability,upgrad,upgrading,185,"Hi @Sachit1137 - python 2.7 reached it's end of life on Jan 1st, 2020 - scispacy certainly won't work with it, because it uses features from python 3.6+ (type hints etc). I'd recommend upgrading to python 3.6+.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/258:159,usability,hint,hints,159,"Hi @Sachit1137 - python 2.7 reached it's end of life on Jan 1st, 2020 - scispacy certainly won't work with it, because it uses features from python 3.6+ (type hints etc). I'd recommend upgrading to python 3.6+.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/258
https://github.com/allenai/scispacy/issues/259:402,interoperability,share,share,402,"Hi @maxfarrell, . Unfortunately we haven't preserved that information in the pruning of UMLS that we use to do entity linking (it would be nice to keep all the info in UMLS, but it quickly makes the KB impractically large). To do this you'd need a copy of UMLS yourself, which you could then extract these relations from. . One idea as an approximation to this would be to group all the concepts which share any aliases? This might give you reasonable groupings, maybe. (also if you describe your objective at a higher level, we might have suggestions for alternative methods which would be easier/more effective).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/259
https://github.com/allenai/scispacy/issues/259:603,usability,effectiv,effective,603,"Hi @maxfarrell, . Unfortunately we haven't preserved that information in the pruning of UMLS that we use to do entity linking (it would be nice to keep all the info in UMLS, but it quickly makes the KB impractically large). To do this you'd need a copy of UMLS yourself, which you could then extract these relations from. . One idea as an approximation to this would be to group all the concepts which share any aliases? This might give you reasonable groupings, maybe. (also if you describe your objective at a higher level, we might have suggestions for alternative methods which would be easier/more effective).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/259
https://github.com/allenai/scispacy/issues/260:76,deployability,updat,update,76,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:144,deployability,version,versions,144,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:216,deployability,version,versions,216,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:144,integrability,version,versions,144,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:216,integrability,version,versions,216,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:42,interoperability,compatib,compatible,42,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:144,modifiability,version,versions,144,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:216,modifiability,version,versions,216,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:76,safety,updat,update,76,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:76,security,updat,update,76,"So, scispacy v0.2.4 shouldn't actually be compatible with v2.3.1. Could you update to scispacy v0.2.5 just to make sure its not something about versions? I've never seen this before and can't reproduce on the latest versions.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:55,interoperability,incompatib,incompatibility,55,"Thanks a lot for your reply! I did not know about this incompatibility. Unfortunately, I have the same issue with scispacy v0.2.5 and spacy v2.3.1. Do you have an idea what it could be ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:47,energy efficiency,model,model,47,Do you have this problem with the raw scispacy model (en_ner_bc5cdr_md)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:47,security,model,model,47,Do you have this problem with the raw scispacy model (en_ner_bc5cdr_md)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:58,energy efficiency,model,model,58,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:244,energy efficiency,model,model,244,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:331,energy efficiency,model,model,331,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:321,performance,tune,tuned,321,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:271,safety,input,input,271,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:58,security,model,model,58,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:244,security,model,model,244,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:331,security,model,model,331,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:13,usability,behavi,behaviour,13,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/260:271,usability,input,input,271,"I think this behaviour might be expected, because the NER model will run at the sentence level (i.e entities which cross sentence boundaries would not be allowed) - if the parser is not set, there will be no sentences in the doc and so the NER model will run on the full input, which looks like it has confused your fine tuned NER model here. Are the entities you are fine tuning it on very long? Generally this is not recommended and you should try a different approach, such as text classification.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/260
https://github.com/allenai/scispacy/issues/261:67,deployability,pipelin,pipeline,67,"There isn't currently a way to use multiple ner models in the same pipeline. You can definitely run them each independently and then combine results however you like (by writing your own code). Also, see this issue (explosion/spaCy#2920) about the same thing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/261
https://github.com/allenai/scispacy/issues/261:12,energy efficiency,current,currently,12,"There isn't currently a way to use multiple ner models in the same pipeline. You can definitely run them each independently and then combine results however you like (by writing your own code). Also, see this issue (explosion/spaCy#2920) about the same thing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/261
https://github.com/allenai/scispacy/issues/261:48,energy efficiency,model,models,48,"There isn't currently a way to use multiple ner models in the same pipeline. You can definitely run them each independently and then combine results however you like (by writing your own code). Also, see this issue (explosion/spaCy#2920) about the same thing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/261
https://github.com/allenai/scispacy/issues/261:67,integrability,pipelin,pipeline,67,"There isn't currently a way to use multiple ner models in the same pipeline. You can definitely run them each independently and then combine results however you like (by writing your own code). Also, see this issue (explosion/spaCy#2920) about the same thing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/261
https://github.com/allenai/scispacy/issues/261:48,security,model,models,48,"There isn't currently a way to use multiple ner models in the same pipeline. You can definitely run them each independently and then combine results however you like (by writing your own code). Also, see this issue (explosion/spaCy#2920) about the same thing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/261
https://github.com/allenai/scispacy/issues/262:48,deployability,api,api,48,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:52,deployability,pipelin,pipeline-functions,52,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:114,deployability,pipelin,pipeline,114,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:138,energy efficiency,model,model,138,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:48,integrability,api,api,48,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:52,integrability,pipelin,pipeline-functions,52,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:114,integrability,pipelin,pipeline,114,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:48,interoperability,api,api,48,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:138,security,model,model,138,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:216,security,token,tokens,216,"Hi @madhurkgp ,. You might try https://spacy.io/api/pipeline-functions#merge_noun_chunks . If you add this to the pipeline BEFORE the NER model it might fix your issue, as then noun chunks would be treated as single tokens. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:60,security,ident,identified,60,and what if we want to remove some words from being getting identified?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:122,deployability,contain,contain,122,"@madhurkgp You can't stop them from being identified, but for whatever you are using them for, you can just check if they contain words that you don't want in them?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:42,security,ident,identified,42,"@madhurkgp You can't stop them from being identified, but for whatever you are using them for, you can just check if they contain words that you don't want in them?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/issues/262:21,usability,stop,stop,21,"@madhurkgp You can't stop them from being identified, but for whatever you are using them for, you can just check if they contain words that you don't want in them?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/262
https://github.com/allenai/scispacy/pull/264:231,energy efficiency,current,current,231,"Hi @danielkingai2 I assume you are talking about the issue related to pysbd sentence segmentation (issue: https://github.com/allenai/scispacy/issues/265) which you have fixed in https://github.com/allenai/scispacy/pull/266. In the current pull request, its a different change. Here I have corrected the shebang for the bash scripts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/264
https://github.com/allenai/scispacy/pull/268:128,integrability,repositor,repository,128,"Hi, thanks for the PR - sorry, I don't think we'll merge this. It's hard to keep jupyter notebooks in sync with the rest of the repository, and we already have examples in the readme. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/268
https://github.com/allenai/scispacy/pull/268:128,interoperability,repositor,repository,128,"Hi, thanks for the PR - sorry, I don't think we'll merge this. It's hard to keep jupyter notebooks in sync with the rest of the repository, and we already have examples in the readme. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/268
https://github.com/allenai/scispacy/issues/269:327,availability,error,error,327,"I'm sorry, I don't quite understand your question. The `ann_concept_aliases_list` is ``` A list of strings, mapping the indices used in the ann_index to possible KB mentions. This is essentially used a lookup between the ann index and actual mention strings.```, so it can't really be missing from the kb. Did you encounter an error?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:327,performance,error,error,327,"I'm sorry, I don't quite understand your question. The `ann_concept_aliases_list` is ``` A list of strings, mapping the indices used in the ann_index to possible KB mentions. This is essentially used a lookup between the ann index and actual mention strings.```, so it can't really be missing from the kb. Did you encounter an error?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:327,safety,error,error,327,"I'm sorry, I don't quite understand your question. The `ann_concept_aliases_list` is ``` A list of strings, mapping the indices used in the ann_index to possible KB mentions. This is essentially used a lookup between the ann index and actual mention strings.```, so it can't really be missing from the kb. Did you encounter an error?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:25,testability,understand,understand,25,"I'm sorry, I don't quite understand your question. The `ann_concept_aliases_list` is ``` A list of strings, mapping the indices used in the ann_index to possible KB mentions. This is essentially used a lookup between the ann index and actual mention strings.```, so it can't really be missing from the kb. Did you encounter an error?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:327,usability,error,error,327,"I'm sorry, I don't quite understand your question. The `ann_concept_aliases_list` is ``` A list of strings, mapping the indices used in the ann_index to possible KB mentions. This is essentially used a lookup between the ann index and actual mention strings.```, so it can't really be missing from the kb. Did you encounter an error?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:10,availability,error,error,10,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:154,energy efficiency,load,loaded,154,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:217,energy efficiency,load,loaded,217,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:243,interoperability,mismatch,mismatch,243,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:10,performance,error,error,10,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:154,performance,load,loaded,154,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:217,performance,load,loaded,217,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:10,safety,error,error,10,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:10,usability,error,error,10,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/269:260,usability,close,close,260,Sorry the error stemmed from a silly mistake at my end. I had misspelt the name of a kb i created and edited `DEFAULT_KNOWLEDGE_BASES`. The KB by default loaded UMLS while the `ann_concept_aliases_list` was correctly loaded. Hence there was a mismatch. I will close the issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/269
https://github.com/allenai/scispacy/issues/270:25,deployability,instal,install,25,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:52,deployability,instal,install,52,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:95,deployability,instal,install,95,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:161,deployability,instal,install,161,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:316,deployability,instal,install,316,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:135,modifiability,pac,package,135,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:337,reliability,Doe,Does,337,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:193,safety,except,except,193,"Hi @swartchris8, you can install scispacy with `pip install scispacy --no-deps`. This will not install any of the requirements for the package, and then you can install all of the requirements except for `nmslib`. You could also clone the repo, edit the requirements file and `setup.py` to remove `nmslib`, and then install from source. Does one of these solutions work for you? Or did you have another solution in mind?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:78,deployability,instal,installed,78,"Thanks for the great package, I realise I might not need the scispacy package installed if I just start working from the model directly and then I don't have to worry about disabling the linker until I can see if I can run this on Dataflow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:121,energy efficiency,model,model,121,"Thanks for the great package, I realise I might not need the scispacy package installed if I just start working from the model directly and then I don't have to worry about disabling the linker until I can see if I can run this on Dataflow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:21,modifiability,pac,package,21,"Thanks for the great package, I realise I might not need the scispacy package installed if I just start working from the model directly and then I don't have to worry about disabling the linker until I can see if I can run this on Dataflow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:70,modifiability,pac,package,70,"Thanks for the great package, I realise I might not need the scispacy package installed if I just start working from the model directly and then I don't have to worry about disabling the linker until I can see if I can run this on Dataflow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/270:121,security,model,model,121,"Thanks for the great package, I realise I might not need the scispacy package installed if I just start working from the model directly and then I don't have to worry about disabling the linker until I can see if I can run this on Dataflow.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/270
https://github.com/allenai/scispacy/issues/271:192,deployability,resourc,resources,192,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:192,energy efficiency,resourc,resources,192,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:192,performance,resourc,resources,192,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:192,safety,resourc,resources,192,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:192,testability,resourc,resources,192,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:43,usability,support,support,43,I believe I found it where to add language support using umls_utils. https://github.com/allenai/scispacy/blob/8994934b986d4e16414ef038c162b44c1a4bdd2b/scispacy/umls_utils.py#L60-L68. language resources and indexes are generated at https://github.com/allenai/scispacy/blob/15dfead09af3af3ff6a565b977b5128be9224867/scispacy/candidate_generation.py#L360.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:126,energy efficiency,Current,Currently,126,"Hi @arianpasquali, looks like you figured this out! Those are the right directions. Let us know if you run into any problems! Currently it would require modifying that function to pick up non-english sources, from UMLS (and requires that you have access to a UMLS export).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:153,security,modif,modifying,153,"Hi @arianpasquali, looks like you figured this out! Those are the right directions. Let us know if you run into any problems! Currently it would require modifying that function to pick up non-english sources, from UMLS (and requires that you have access to a UMLS export).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/271:247,security,access,access,247,"Hi @arianpasquali, looks like you figured this out! Those are the right directions. Let us know if you run into any problems! Currently it would require modifying that function to pick up non-english sources, from UMLS (and requires that you have access to a UMLS export).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/271
https://github.com/allenai/scispacy/issues/272:22,energy efficiency,model,models,22,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/272:156,energy efficiency,model,model,156,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/272:220,performance,time,time,220,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/272:22,security,model,models,22,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/272:156,security,model,model,156,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/272:195,usability,support,supports,195,"Hi, all of scispacy's models are trained on english text exclusively. If you have some Portuguese labeled datasets, you can retrain spacy's base Portuguese model on your texts, but scispacy only supports English at this time.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/272
https://github.com/allenai/scispacy/issues/273:122,availability,down,download,122,"Hi @martijnvanbeers,. We tag all our releases on github, so hopefully you should be able to checkout that tag and get the download URLs from there. For example:. https://github.com/allenai/scispacy/tree/v0.2.0. Admittedly this isn't completely transparent, I agree. Sorry about that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/273
https://github.com/allenai/scispacy/issues/273:37,deployability,releas,releases,37,"Hi @martijnvanbeers,. We tag all our releases on github, so hopefully you should be able to checkout that tag and get the download URLs from there. For example:. https://github.com/allenai/scispacy/tree/v0.2.0. Admittedly this isn't completely transparent, I agree. Sorry about that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/273
https://github.com/allenai/scispacy/issues/273:233,safety,compl,completely,233,"Hi @martijnvanbeers,. We tag all our releases on github, so hopefully you should be able to checkout that tag and get the download URLs from there. For example:. https://github.com/allenai/scispacy/tree/v0.2.0. Admittedly this isn't completely transparent, I agree. Sorry about that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/273
https://github.com/allenai/scispacy/issues/273:233,security,compl,completely,233,"Hi @martijnvanbeers,. We tag all our releases on github, so hopefully you should be able to checkout that tag and get the download URLs from there. For example:. https://github.com/allenai/scispacy/tree/v0.2.0. Admittedly this isn't completely transparent, I agree. Sorry about that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/273
https://github.com/allenai/scispacy/pull/274:5,security,disclosur,disclosure,5,"Full disclosure, I did not check that those are the right paths",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:1081,deployability,Updat,Update,1081,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:836,energy efficiency,Reduc,Reduce,836,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:854,energy efficiency,load,load,854,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:847,performance,memor,memory,847,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:854,performance,load,load,854,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:1174,reliability,doe,doesn,1174,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:940,safety,test,test,940,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:956,safety,test,tests,956,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:1081,safety,Updat,Update,1081,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:1081,security,Updat,Update,1081,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:940,testability,test,test,940,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:956,testability,test,tests,956,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/274:847,usability,memor,memory,847,"@DeNeutoy . Instead of removing duplicates later in. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L67. `concept[""aliases""] = list(set(concept[""aliases""]))`. isn't it better if its taken care of during insertion? And to do this, create concept_details[concept_id][""aliases""] as set. https://github.com/allenai/scispacy/blob/master/scispacy/umls_utils.py#L77. ```. concept_details[concept_id] = {. ""concept_id"": concept_id,. ""aliases"": [],. ""types"": [],. }. ```. **Advantages**:. 1. This would provide correct statistics for count of statistics for 1 or > 1 aliases. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L55. ```. print(f'Number of concepts with 1 alias: {with_one_alias_count}'). print(f'Number of concepts with > 1 alias: {with_more_than_one_alias_count}'). ```. 2. Reduce the memory load while concept_details gets populated. This would mean changing the corresponding test code too. `tests/test_umls_utils.py`. Though I haven't checked whether changing ""aliases"" data type to set will impact anywhere else. **Update**: Convert back set to list as done in export_umls_json.py to ensure rest of the code doesn't get effected. https://github.com/allenai/scispacy/blob/master/scripts/export_umls_json.py#L71. ```. if 'canonical_name' not in concept:. aliases = concept['aliases']. concept['canonical_name'] = aliases[0]. del aliases[0]. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/274
https://github.com/allenai/scispacy/pull/277:90,integrability,bridg,bridge,90,"Ya no prob, there'll probably be some kinks when we actually try it but we can cross that bridge when we actually want to publish it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/277
https://github.com/allenai/scispacy/pull/277:122,integrability,pub,publish,122,"Ya no prob, there'll probably be some kinks when we actually try it but we can cross that bridge when we actually want to publish it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/277
https://github.com/allenai/scispacy/pull/277:90,interoperability,bridg,bridge,90,"Ya no prob, there'll probably be some kinks when we actually try it but we can cross that bridge when we actually want to publish it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/277
https://github.com/allenai/scispacy/issues/279:597,deployability,contain,contains,597,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:199,energy efficiency,model,models,199,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:359,energy efficiency,Current,Currently,359,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:267,integrability,coupl,couple,267,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:721,integrability,filter,filtering,721,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:771,integrability,filter,filter,771,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:267,modifiability,coupl,couple,267,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:199,security,model,models,199,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:540,security,control,controlled,540,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:267,testability,coupl,couple,267,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:540,testability,control,controlled,540,"Hi @predestination,. Thanks for the comprehensive description of what you are trying to do. Broadly this seems like a good approach. Also, there is an element of non-determinism in using statistical models, so they are never going to be perfect. However, there are a couple of things you might want to try:. - https://github.com/allenai/scispacy#entitylinker Currently you are using the UMLS entity linker. You might want to try either the MESH or HPO linkers, as these are less noisy (because they have fewer concepts and are more tightly controlled). - The data the entity linker was trained on contains verbs which can be linked. In your case, diseases and treatments are very unlikely to be verbs, so I would suggest filtering them out. . - Finally, you might try to filter the entities you extract based on some sentence level keywords. E.g if ""treated"" or ""patient"" is in the sentence, you can be a little more aggressive in your extractions, perhaps expanding to noun phrases etc. Hopefully these ideas will give your method a boost!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:133,deployability,contain,contains,133,"Thank you for the suggestions. I will start with MESH, HPO linkers. - When you mentioned, ""The data the entity linker was trained on contains verbs which can be linked. "", are you talking about the entity linkers MESH & HPO or else all the entity linkers that are present? - https://spacy.io/usage/training Also, what are your thoughts on training a model on the data I have using Spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:350,energy efficiency,model,model,350,"Thank you for the suggestions. I will start with MESH, HPO linkers. - When you mentioned, ""The data the entity linker was trained on contains verbs which can be linked. "", are you talking about the entity linkers MESH & HPO or else all the entity linkers that are present? - https://spacy.io/usage/training Also, what are your thoughts on training a model on the data I have using Spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:350,security,model,model,350,"Thank you for the suggestions. I will start with MESH, HPO linkers. - When you mentioned, ""The data the entity linker was trained on contains verbs which can be linked. "", are you talking about the entity linkers MESH & HPO or else all the entity linkers that are present? - https://spacy.io/usage/training Also, what are your thoughts on training a model on the data I have using Spacy?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:284,availability,avail,available,284,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:232,deployability,depend,depends,232,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:260,deployability,resourc,resources,260,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:76,energy efficiency,model,model,76,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:222,energy efficiency,model,model,222,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:260,energy efficiency,resourc,resources,260,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:388,energy efficiency,model,models,388,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:232,integrability,depend,depends,232,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:232,modifiability,depend,depends,232,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:260,performance,resourc,resources,260,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:284,reliability,availab,available,284,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:62,safety,detect,detection,62,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:232,safety,depend,depends,232,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:260,safety,resourc,resources,260,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:284,safety,avail,available,284,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:62,security,detect,detection,62,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:76,security,model,model,76,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:222,security,model,model,222,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:284,security,availab,available,284,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:388,security,model,models,388,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:232,testability,depend,depends,232,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/279:260,testability,resourc,resources,260,"Hi @predestination,. Yes all the entity linkers use a mention detection NER model which was trained on the Med Mentions corpus, which is a general entity linking dataset for biomed. . It might be worthwhile to train a new model, it depends a little bit on the resources that you have available, as it can take a while to collect good data etc. You might also be interested in these spacy models:. https://github.com/NLPatVCU/medaCy. https://github.com/kormilitzin/med7. which are more for drug and dosage information. It might give you some idea of how to create your own, or perhaps they are useful directly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/279
https://github.com/allenai/scispacy/issues/281:152,availability,sli,slight,152,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:296,energy efficiency,model,model,296,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:63,performance,perform,performance,63,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:152,reliability,sli,slight,152,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:296,security,model,model,296,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:63,usability,perform,performance,63,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:356,usability,tool,tools,356,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:405,usability,tool,tool,405,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:414,usability,help,help,414,"Hi @wpan5,. Yes, training from scratch would result in similar performance for spacy and scispacy. The only difference is the vocabulary, which makes a slight difference but is basically negligible. . You're also right that you probably won't get much benefit from training on top of another NER model that has different types. I would suggest you look at tools like prodigy (Spacy's very nice annotation tool) to help you create a dataset for your task. It's a little hard to give suggestions as I'm not sure what problem you are working on - if you describe it in more detail we might be able to give you more precise advice. E.g for biomed, we have quite a few entity linkers which might do what you need?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:94,availability,avail,available,94,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:144,interoperability,ontolog,ontology,144,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:94,reliability,availab,available,94,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:94,safety,avail,available,94,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:94,security,availab,available,94,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/281:278,usability,tool,tools,278,"Hi Mark,. Thanks for the clarification and suggestions. I just checked out the entity linkers available in scispaCy. We sort of created our own ontology and annotated our data accordingly, so I assume it might be difficult to incorporate existing entity linkers. . I'll explore tools like Prodigy. Thanks again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/281
https://github.com/allenai/scispacy/issues/283:19,usability,paus,paused,19,"Sorry, my internet paused for a second, and it looks like I made a duplicate issue (#284)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/283
https://github.com/allenai/scispacy/issues/284:647,availability,slo,slowdown,647,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:689,availability,slo,slowdown,689,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:95,deployability,instal,installing,95,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:325,deployability,instal,installing,325,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:338,deployability,version,version,338,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:458,deployability,log,logs,458,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:401,energy efficiency,CPU,CPU,401,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:490,energy efficiency,CPU,CPU,490,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1308,energy efficiency,measur,measuring,1308,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:38,integrability,coupl,couple,38,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:338,integrability,version,version,338,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1548,integrability,sub,substantially,1548,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:38,modifiability,coupl,couple,38,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:123,modifiability,pac,package,123,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:338,modifiability,version,version,338,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1185,modifiability,paramet,parameter,1185,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1479,modifiability,paramet,parameter,1479,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:401,performance,CPU,CPU,401,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:490,performance,CPU,CPU,490,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:820,performance,memor,memory,820,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:647,reliability,slo,slowdown,647,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:689,reliability,slo,slowdown,689,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:788,reliability,doe,does,788,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:458,safety,log,logs,458,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:458,security,log,logs,458,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1201,security,control,controls,1201,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:38,testability,coupl,couple,38,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:293,testability,understand,understand,293,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:458,testability,log,logs,458,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1201,testability,control,controls,1201,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:420,usability,clear,clearly,420,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:494,usability,support,supports,494,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:820,usability,memor,memory,820,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1360,usability,user,user-images,1360,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:1705,usability,help,helpful,1705,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:140,performance,tune,tuned,140,@DeNeutoy . Thanks so much for the detailed response. This is very helpful. . I will avoid colab for now and try out the mesh linker with a tuned `ef_search`!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:85,safety,avoid,avoid,85,@DeNeutoy . Thanks so much for the detailed response. This is very helpful. . I will avoid colab for now and try out the mesh linker with a tuned `ef_search`!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/284:67,usability,help,helpful,67,@DeNeutoy . Thanks so much for the detailed response. This is very helpful. . I will avoid colab for now and try out the mesh linker with a tuned `ef_search`!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284
https://github.com/allenai/scispacy/issues/285:8,energy efficiency,load,loading,8,"For me, loading the small model and the EntityLinker takes ~11gb of RAM. I don't have an immediate solution to make it use less, but we do also 4 more specific entity linkers (see the EntityLinker section of the readme) that are smaller than the large UMLS one. Not sure if @DeNeutoy has other ideas since he has been messing around with these more than me lately.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:26,energy efficiency,model,model,26,"For me, loading the small model and the EntityLinker takes ~11gb of RAM. I don't have an immediate solution to make it use less, but we do also 4 more specific entity linkers (see the EntityLinker section of the readme) that are smaller than the large UMLS one. Not sure if @DeNeutoy has other ideas since he has been messing around with these more than me lately.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:151,interoperability,specif,specific,151,"For me, loading the small model and the EntityLinker takes ~11gb of RAM. I don't have an immediate solution to make it use less, but we do also 4 more specific entity linkers (see the EntityLinker section of the readme) that are smaller than the large UMLS one. Not sure if @DeNeutoy has other ideas since he has been messing around with these more than me lately.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:8,performance,load,loading,8,"For me, loading the small model and the EntityLinker takes ~11gb of RAM. I don't have an immediate solution to make it use less, but we do also 4 more specific entity linkers (see the EntityLinker section of the readme) that are smaller than the large UMLS one. Not sure if @DeNeutoy has other ideas since he has been messing around with these more than me lately.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:26,security,model,model,26,"For me, loading the small model and the EntityLinker takes ~11gb of RAM. I don't have an immediate solution to make it use less, but we do also 4 more specific entity linkers (see the EntityLinker section of the readme) that are smaller than the large UMLS one. Not sure if @DeNeutoy has other ideas since he has been messing around with these more than me lately.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:233,availability,sli,slight,233,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:474,deployability,updat,update,474,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:328,energy efficiency,current,currently,328,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:365,energy efficiency,power,powerful,365,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:169,performance,memor,memory,169,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:240,performance,perform,performance,240,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:233,reliability,sli,slight,233,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:474,safety,updat,update,474,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:292,security,access,accessible,292,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:351,security,access,access,351,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:474,security,updat,update,474,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:169,usability,memor,memory,169,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:240,usability,perform,performance,240,"What if all the lookup tables, like the `ann_concept_aliases_list` and the jsons in the knowledge base are replaced by sqlite or other light-weight DBs? Keeping them in memory would of course be faster, but saving a lot of RAM for a slight performance decrease may make the EntityLinker more accessible for many people. Since I currently am unable to access a more powerful machine, I may try to implement the DB solution in my own fork. If and when I finish, I can give an update about how it works",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:27,energy efficiency,reduc,reduced,27,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:47,integrability,coupl,couple,47,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:47,modifiability,coupl,couple,47,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:35,performance,memor,memory,35,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:47,testability,coupl,couple,47,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:35,usability,memor,memory,35,Try again off of master? I reduced memory by a couple gigs in #287,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:59,energy efficiency,reduc,reducing,59,"btw, if this isn't enough and you want to mess around with reducing memory further, https://github.com/pythonprofilers/memory_profiler is a nice tool (and is how i made the graphs in that PR)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:68,performance,memor,memory,68,"btw, if this isn't enough and you want to mess around with reducing memory further, https://github.com/pythonprofilers/memory_profiler is a nice tool (and is how i made the graphs in that PR)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:68,usability,memor,memory,68,"btw, if this isn't enough and you want to mess around with reducing memory further, https://github.com/pythonprofilers/memory_profiler is a nice tool (and is how i made the graphs in that PR)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:145,usability,tool,tool,145,"btw, if this isn't enough and you want to mess around with reducing memory further, https://github.com/pythonprofilers/memory_profiler is a nice tool (and is how i made the graphs in that PR)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:41,deployability,build,building,41,@fschlatt. Alternative approach could be building a custom UMLS subset in case there are [semantic types](https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml) which you don't need. Have a look at the thread: https://github.com/allenai/scispacy/issues/237.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:64,integrability,sub,subset,64,@fschlatt. Alternative approach could be building a custom UMLS subset in case there are [semantic types](https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml) which you don't need. Have a look at the thread: https://github.com/allenai/scispacy/issues/237.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:90,interoperability,semant,semantic,90,@fschlatt. Alternative approach could be building a custom UMLS subset in case there are [semantic types](https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml) which you don't need. Have a look at the thread: https://github.com/allenai/scispacy/issues/237.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:134,interoperability,Semant,SemanticTypesAndGroups,134,@fschlatt. Alternative approach could be building a custom UMLS subset in case there are [semantic types](https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml) which you don't need. Have a look at the thread: https://github.com/allenai/scispacy/issues/237.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/285:52,usability,custom,custom,52,@fschlatt. Alternative approach could be building a custom UMLS subset in case there are [semantic types](https://metamap.nlm.nih.gov/SemanticTypesAndGroups.shtml) which you don't need. Have a look at the thread: https://github.com/allenai/scispacy/issues/237.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/285
https://github.com/allenai/scispacy/issues/289:71,availability,outag,outage,71,"Yeah @jusjosgra, I think this might have been related to the `us-west` outage last week or something? I saw a lot of people's IOT doorbells not working  - but if it isn't we can look into it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:71,reliability,outag,outage,71,"Yeah @jusjosgra, I think this might have been related to the `us-west` outage last week or something? I saw a lot of people's IOT doorbells not working  - but if it isn't we can look into it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:13,usability,close,close,13,"I'm going to close this due to inactivity, but feel free to reopen if you are still having issues!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:60,availability,error,error,60,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:87,availability,outag,outage,87,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:47,modifiability,interm,intermittent,47,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:60,performance,error,error,60,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:87,reliability,outag,outage,87,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:60,safety,error,error,60,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/289:60,usability,error,error,60,sorry for not replying -- it did seem to be an intermittent error. Perhaps relating to outage. I will reopen if the issue returns. Thanks!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/289
https://github.com/allenai/scispacy/issues/290:159,deployability,version,version,159,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:186,deployability,version,version,186,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:180,energy efficiency,model,model,180,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:159,integrability,version,version,159,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:186,integrability,version,version,186,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:159,modifiability,version,version,159,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:186,modifiability,version,version,186,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:51,reliability,Doe,Does,51,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:180,security,model,model,180,"Hi, this may be a transient internet or AWS issue. Does it persist for you? I am not able to reproduce it. What is the exact code you are running and scispacy version and scispacy model version?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:85,deployability,version,version,85,"Hi Daniel. Thanks for your reply. yes, this problem persists yet. . I use the latest version of Scispacy and en_ner_bc5cdr_md model. I have this problem with google colab too! . I search for this problem and other people refer to AWS. ...-west-2 or -west-1 or 1a or something like that :(. unfortunately, I have not more information about AWS. . anyway I am very thankful for your replay",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:126,energy efficiency,model,model,126,"Hi Daniel. Thanks for your reply. yes, this problem persists yet. . I use the latest version of Scispacy and en_ner_bc5cdr_md model. I have this problem with google colab too! . I search for this problem and other people refer to AWS. ...-west-2 or -west-1 or 1a or something like that :(. unfortunately, I have not more information about AWS. . anyway I am very thankful for your replay",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:85,integrability,version,version,85,"Hi Daniel. Thanks for your reply. yes, this problem persists yet. . I use the latest version of Scispacy and en_ner_bc5cdr_md model. I have this problem with google colab too! . I search for this problem and other people refer to AWS. ...-west-2 or -west-1 or 1a or something like that :(. unfortunately, I have not more information about AWS. . anyway I am very thankful for your replay",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:85,modifiability,version,version,85,"Hi Daniel. Thanks for your reply. yes, this problem persists yet. . I use the latest version of Scispacy and en_ner_bc5cdr_md model. I have this problem with google colab too! . I search for this problem and other people refer to AWS. ...-west-2 or -west-1 or 1a or something like that :(. unfortunately, I have not more information about AWS. . anyway I am very thankful for your replay",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:126,security,model,model,126,"Hi Daniel. Thanks for your reply. yes, this problem persists yet. . I use the latest version of Scispacy and en_ner_bc5cdr_md model. I have this problem with google colab too! . I search for this problem and other people refer to AWS. ...-west-2 or -west-1 or 1a or something like that :(. unfortunately, I have not more information about AWS. . anyway I am very thankful for your replay",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:53,availability,down,download,53,"@HodaMemar Any update on this? Also, are you able to download the file just via the url? (https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:15,deployability,updat,update,15,"@HodaMemar Any update on this? Also, are you able to download the file just via the url? (https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:15,safety,updat,update,15,"@HodaMemar Any update on this? Also, are you able to download the file just via the url? (https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:15,security,updat,update,15,"@HodaMemar Any update on this? Also, are you able to download the file just via the url? (https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:37,availability,down,download,37,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:213,availability,down,download,213,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:172,deployability,updat,update,172,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:172,safety,updat,update,172,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:172,security,updat,update,172,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:618,security,auth,auth,618,"Thank you for your attention, I will download it. On Wed, 6 Jan 2021, 01:06 Daniel King, <notifications@github.com> wrote:. > @HodaMemar <https://github.com/HodaMemar> Any update on this? Also, are. > you able to download the file just via the url? (. > https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/linkers/2020-10-09/hpo/tfidf_vectors_sparse.npz. > ). >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#issuecomment-754916900>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FI2VOM2NJIQH4GGYGDSYOA7JANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:249,availability,error,error,249,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:1168,availability,down,downloaded,1168,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:64,interoperability,prox,proxy,64,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:176,interoperability,prox,proxy,176,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:218,interoperability,prox,proxy,218,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:315,modifiability,pac,packages,315,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:502,modifiability,pac,packages,502,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:670,modifiability,pac,packages,670,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:864,modifiability,pac,packages,864,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:1027,modifiability,pac,packages,1027,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:249,performance,error,error,249,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:1455,performance,cach,cache,1455,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:249,safety,error,error,249,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:249,usability,error,error,249,"@HodaMemar . You are facing ConnectionError probably because of proxy settings. Even I had faced the same issue. . In my workplace, the machine I am using needs to be behind a proxy server. Initially due to absence of proxy, I was getting a similar error:. ```. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/linking.py"", line 77, in __init__. self.candidate_generator = candidate_generator or CandidateGenerator(name=name). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 222, in __init__. linker_paths=linker_paths, ef_search=ef_search. File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 132, in load_approximate_nearest_neighbours_index. cached_path(linker_paths.tfidf_vectors). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 36, in cached_path. return get_from_cache(url_or_filename, cache_dir). File ""/home/KA/anaconda2/envs/py3/lib/python3.7/site-packages/scispacy/file_cache.py"", line 112, in get_from_cache. response = requests.head(url, allow_redirects=True). ```. Even after you have downloaded the file, you would need to be able to connect to the AWS server. Have a look at. https://github.com/allenai/scispacy/blob/master/scispacy/file_cache.py#L112. `response = requests.head(url, allow_redirects=True)`. It checks the URL response, before it reads the file from the cache path:. ```. cache_path = os.path.join(cache_dir, filename). if not os.path.exists(cache_path):. ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:324,integrability,event,event-,324,"Hello. Thank you for your help. On Wed, Mar 24, 2021 at 9:15 AM Daniel King ***@***.***>. wrote:. > Closed #290 <https://github.com/allenai/scispacy/issues/290>. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#event-4499735615>, or. > unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FKTKUOHAJBGVGKS2QTTFFU75ANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:410,security,auth,auth,410,"Hello. Thank you for your help. On Wed, Mar 24, 2021 at 9:15 AM Daniel King ***@***.***>. wrote:. > Closed #290 <https://github.com/allenai/scispacy/issues/290>. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#event-4499735615>, or. > unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FKTKUOHAJBGVGKS2QTTFFU75ANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:26,usability,help,help,26,"Hello. Thank you for your help. On Wed, Mar 24, 2021 at 9:15 AM Daniel King ***@***.***>. wrote:. > Closed #290 <https://github.com/allenai/scispacy/issues/290>. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#event-4499735615>, or. > unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FKTKUOHAJBGVGKS2QTTFFU75ANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/290:100,usability,Close,Closed,100,"Hello. Thank you for your help. On Wed, Mar 24, 2021 at 9:15 AM Daniel King ***@***.***>. wrote:. > Closed #290 <https://github.com/allenai/scispacy/issues/290>. >. > . > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/290#event-4499735615>, or. > unsubscribe. > <https://github.com/notifications/unsubscribe-auth/AP4V5FKTKUOHAJBGVGKS2QTTFFU75ANCNFSM4UNVJLSQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/290
https://github.com/allenai/scispacy/issues/291:16,deployability,instal,install,16,Are you able to install `nmslib` outside of the scispacy installation?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:57,deployability,instal,installation,57,Are you able to install `nmslib` outside of the scispacy installation?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:10,deployability,patch,patch,10,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:158,deployability,releas,release,158,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:184,deployability,patch,patch,184,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:211,deployability,releas,release,211,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:10,safety,patch,patch,10,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:184,safety,patch,patch,184,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:10,security,patch,patch,10,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:97,security,Auth,Author,97,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:184,security,patch,patch,184,There's a patch mentioned in https://github.com/nmslib/nmslib/issues/399#issuecomment-734988832. Author has mentioned that they will be merging with the next release. You can try this patch or wait for the next release.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:61,deployability,version,versions,61,I'm going to close this under the assumption that the latest versions of nmslib fix this issue. Feel free to reopen if you are still having trouble!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:61,integrability,version,versions,61,I'm going to close this under the assumption that the latest versions of nmslib fix this issue. Feel free to reopen if you are still having trouble!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:61,modifiability,version,versions,61,I'm going to close this under the assumption that the latest versions of nmslib fix this issue. Feel free to reopen if you are still having trouble!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:13,usability,close,close,13,I'm going to close this under the assumption that the latest versions of nmslib fix this issue. Feel free to reopen if you are still having trouble!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:533,deployability,instal,installed,533,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:556,deployability,instal,installed,556,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:158,energy efficiency,Current,Currently,158,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:97,interoperability,share,share,97,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:478,reliability,doe,does,478,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:23,usability,close,closed,23,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:106,usability,experien,experience,106,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:181,usability,support,supports,181,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:340,usability,user,user-images,340,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:487,usability,support,support,487,"Sorry - I know this is closed. I ran into a similar issue but it's all fixed now and I wanted to share my experience incase anybody runs into the same issue. Currently, nmslib only supports python < 3.9. I've tried 3.9, 3.91, and 3.85. **Only 3.85 worked.** Here's an image showing that their latest python binary is 3.8:. ![image](https://user-images.githubusercontent.com/78378555/106503778-15e1f780-6494-11eb-8f45-a37f4aa8a0e9.png). Once I had Python 3.85 **64-bit** (nmslib does not support 32-bit - atleast on windows) - nmslib installed fine. I then installed scispacy and it seems to be working right.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,availability,error,error,135,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:101,deployability,instal,installing,101,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,performance,error,error,135,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,safety,error,error,135,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:20,usability,confirm,confirm,20,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,usability,error,error,135,@alexbellin . I can confirm that this is the case. In my case I used 3.8.7. I am not really sure why installing 3.9.1 creates a syntax error in the code but it is what it is.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,availability,error,error,135,"@f0lie @alexbellin . I also confirm the same. I used python 3.8.10 (64-bit) with no issue. As of this writing, 3.9 still results in an error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,performance,error,error,135,"@f0lie @alexbellin . I also confirm the same. I used python 3.8.10 (64-bit) with no issue. As of this writing, 3.9 still results in an error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,safety,error,error,135,"@f0lie @alexbellin . I also confirm the same. I used python 3.8.10 (64-bit) with no issue. As of this writing, 3.9 still results in an error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:28,usability,confirm,confirm,28,"@f0lie @alexbellin . I also confirm the same. I used python 3.8.10 (64-bit) with no issue. As of this writing, 3.9 still results in an error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:135,usability,error,error,135,"@f0lie @alexbellin . I also confirm the same. I used python 3.8.10 (64-bit) with no issue. As of this writing, 3.9 still results in an error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:3,testability,plan,plans,3,no plans on fixing this?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:272,deployability,instal,installed,272,"Unfortunately, this is not a scispacy issue, but rather an nmslib issue. Please have a look at other closed issues around this (https://github.com/allenai/scispacy/issues?q=is%3Aissue+nmslib) or check out nmslib itself (https://github.com/nmslib/nmslib) and try to get it installed in your environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/291:101,usability,close,closed,101,"Unfortunately, this is not a scispacy issue, but rather an nmslib issue. Please have a look at other closed issues around this (https://github.com/allenai/scispacy/issues?q=is%3Aissue+nmslib) or check out nmslib itself (https://github.com/nmslib/nmslib) and try to get it installed in your environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/291
https://github.com/allenai/scispacy/issues/292:27,deployability,fail,failing,27,I don't think this test is failing on master. What are you trying to do?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:27,reliability,fail,failing,27,I don't think this test is failing on master. What are you trying to do?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:19,safety,test,test,19,I don't think this test is failing on master. What are you trying to do?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:19,testability,test,test,19,I don't think this test is failing on master. What are you trying to do?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:29,availability,failur,failure,29,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:29,deployability,fail,failure,29,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:29,performance,failur,failure,29,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:29,reliability,fail,failure,29,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:19,safety,test,test,19,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:19,testability,test,test,19,"Just reporting the test case failure, seen without any code change.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:18,deployability,fail,fail,18,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:142,deployability,version,versions,142,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:142,integrability,version,versions,142,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:171,interoperability,specif,specified,171,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:142,modifiability,version,versions,142,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:162,modifiability,pac,packages,162,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:10,reliability,doe,doesn,10,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:18,reliability,fail,fail,18,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:5,safety,test,test,5,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:5,testability,test,test,5,"That test doesn't fail for me on master. Are you sure you haven't changed any code or files? If so, please make sure your environment has the versions of all the packages specified in the requirements file.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:87,deployability,fail,failing,87,"I haven't changed any code. The other asserts/example-strings in the same test are not failing. Just this one, so thought of reporting it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:87,reliability,fail,failing,87,"I haven't changed any code. The other asserts/example-strings in the same test are not failing. Just this one, so thought of reporting it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:74,safety,test,test,74,"I haven't changed any code. The other asserts/example-strings in the same test are not failing. Just this one, so thought of reporting it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:38,testability,assert,asserts,38,"I haven't changed any code. The other asserts/example-strings in the same test are not failing. Just this one, so thought of reporting it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:74,testability,test,test,74,"I haven't changed any code. The other asserts/example-strings in the same test are not failing. Just this one, so thought of reporting it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:92,deployability,fail,failing,92,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:173,deployability,version,version,173,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:173,integrability,version,version,173,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:173,modifiability,version,version,173,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:92,reliability,fail,failing,92,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:26,safety,test,test,26,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:278,safety,accid,accidentally,278,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:26,testability,test,test,26,"I am quite sure that this test passes on master. Do you have any thought of why it might be failing for your local setup? What are your environment details (e.g. OS, python version, output of `pip list`)? Can you try recloning the repo, just to make absolutely sure you haven't accidentally changed something?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/issues/292:62,usability,support,support,62,"Turns out this is because you are on windows. We don't really support windows, but i've fixed this one in #293",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/292
https://github.com/allenai/scispacy/pull/293:29,deployability,fail,failing,29,I have no idea why the CI is failing. It can't find numpy.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:29,reliability,fail,failing,29,I have no idea why the CI is failing. It can't find numpy.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:34,availability,failur,failure,34,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:34,deployability,fail,failure,34,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:82,deployability,fail,failing,82,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:34,performance,failur,failure,34,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:34,reliability,fail,failure,34,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:82,reliability,fail,failing,82,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:74,safety,test,test,74,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/pull/293:74,testability,test,test,74,The encoding change has fixed the failure on windows. Checked locally. No test is failing now.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/293
https://github.com/allenai/scispacy/issues/296:324,safety,test,test,324,"Made a little progress:. ### instead of: from scispacy.umls_linking import UmlsEntityLinker. from scispacy.linking import EntityLinker. ### instead of: linker = UmlsEntityLinker(resolve_abbreviations=True). linker = EntityLinker(name='mesh', resolve_abbreviations=True). Now, looking for the equivalent of:. for umls_ent in test[0]._.umls_ents:. print(len(umls_ent), linker.umls.cui_to_entity[umls_ent[0]]). Am finding that doc.ents[0]._.kb_ents returns an empty list.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/296
https://github.com/allenai/scispacy/issues/296:324,testability,test,test,324,"Made a little progress:. ### instead of: from scispacy.umls_linking import UmlsEntityLinker. from scispacy.linking import EntityLinker. ### instead of: linker = UmlsEntityLinker(resolve_abbreviations=True). linker = EntityLinker(name='mesh', resolve_abbreviations=True). Now, looking for the equivalent of:. for umls_ent in test[0]._.umls_ents:. print(len(umls_ent), linker.umls.cui_to_entity[umls_ent[0]]). Am finding that doc.ents[0]._.kb_ents returns an empty list.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/296
https://github.com/allenai/scispacy/issues/296:14,usability,progress,progress,14,"Made a little progress:. ### instead of: from scispacy.umls_linking import UmlsEntityLinker. from scispacy.linking import EntityLinker. ### instead of: linker = UmlsEntityLinker(resolve_abbreviations=True). linker = EntityLinker(name='mesh', resolve_abbreviations=True). Now, looking for the equivalent of:. for umls_ent in test[0]._.umls_ents:. print(len(umls_ent), linker.umls.cui_to_entity[umls_ent[0]]). Am finding that doc.ents[0]._.kb_ents returns an empty list.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/296
https://github.com/allenai/scispacy/issues/296:13,usability,close,close,13,"I'm going to close this under the assumption that the readme answers your questions, but feel free to reopen or open a new issue if your still having problems!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/296
https://github.com/allenai/scispacy/issues/297:89,energy efficiency,model,models,89,"Hi @sbhttchryy,. That would be difficult because scispacy only provides english language models. Sorry about that! You may wish to look at the various German sources in UMLS:. https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html. Good luck!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/297
https://github.com/allenai/scispacy/issues/297:89,security,model,models,89,"Hi @sbhttchryy,. That would be difficult because scispacy only provides english language models. Sorry about that! You may wish to look at the various German sources in UMLS:. https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html. Good luck!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/297
https://github.com/allenai/scispacy/pull/298:16,interoperability,share,share,16,"@wlwg Could you share an example that produces different results on retries? I am not able to reproduce this locally, and tie-breaking in sorting is deterministic in python, so it would only produce different results if nmslib is returning results in a different order each time (which it might be, but I'm not aware of this)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:274,performance,time,time,274,"@wlwg Could you share an example that produces different results on retries? I am not able to reproduce this locally, and tie-breaking in sorting is deterministic in python, so it would only produce different results if nmslib is returning results in a different order each time (which it might be, but I'm not aware of this)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:56,performance,time,times,56,"@danielkingai2 This is my code example. Run it multiple times can produce results with different order. ```. from scispacy.candidate_generation import CandidateGenerator. candidate_generator = CandidateGenerator(name='umls'). candidates = candidate_generator(['Benlysta'], 30)[0]. print(','.join([c.concept_id for c in candidates])). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:352,deployability,version,version,352,"@wlwg I still can't reproduce this, I ran . ```. In [17]: candidates = candidate_generator(['Benlysta'], 30)[0] . ...: for i in range(1000): . ...: candidates_loop = candidate_generator(['Benlysta'], 30)[0] . ...: assert candidates_loop == candidates . ```. without hitting the assert. Is it possible you are changing something else between runs? What version of scispacy and nmslib do you have?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:352,integrability,version,version,352,"@wlwg I still can't reproduce this, I ran . ```. In [17]: candidates = candidate_generator(['Benlysta'], 30)[0] . ...: for i in range(1000): . ...: candidates_loop = candidate_generator(['Benlysta'], 30)[0] . ...: assert candidates_loop == candidates . ```. without hitting the assert. Is it possible you are changing something else between runs? What version of scispacy and nmslib do you have?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:352,modifiability,version,version,352,"@wlwg I still can't reproduce this, I ran . ```. In [17]: candidates = candidate_generator(['Benlysta'], 30)[0] . ...: for i in range(1000): . ...: candidates_loop = candidate_generator(['Benlysta'], 30)[0] . ...: assert candidates_loop == candidates . ```. without hitting the assert. Is it possible you are changing something else between runs? What version of scispacy and nmslib do you have?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:214,testability,assert,assert,214,"@wlwg I still can't reproduce this, I ran . ```. In [17]: candidates = candidate_generator(['Benlysta'], 30)[0] . ...: for i in range(1000): . ...: candidates_loop = candidate_generator(['Benlysta'], 30)[0] . ...: assert candidates_loop == candidates . ```. without hitting the assert. Is it possible you are changing something else between runs? What version of scispacy and nmslib do you have?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:278,testability,assert,assert,278,"@wlwg I still can't reproduce this, I ran . ```. In [17]: candidates = candidate_generator(['Benlysta'], 30)[0] . ...: for i in range(1000): . ...: candidates_loop = candidate_generator(['Benlysta'], 30)[0] . ...: assert candidates_loop == candidates . ```. without hitting the assert. Is it possible you are changing something else between runs? What version of scispacy and nmslib do you have?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:210,performance,time,times,210,@danielkingai2 You can't reproduce it by having a loop like that. (I assume it's because some kind of random seed is fixed during the whole program). You need to put it into a python script and run it multiple times in command line. You'll be able to see different results.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:219,usability,command,command,219,@danielkingai2 You can't reproduce it by having a loop like that. (I assume it's because some kind of random seed is fixed during the whole program). You need to put it into a python script and run it multiple times in command line. You'll be able to see different results.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:436,testability,verif,verifying,436,"Ok, I was able to reproduce it, thanks. I believe it is caused by [this](https://github.com/allenai/scispacy/blob/7c852a4c34b09a68ca3db2e01dca09628ab1b425/scispacy/candidate_generation.py#L350-L353) iteration over a `defaultdict`. I think the candidate generator is the proper place to fix, rather than the linker. Would you be up for changing that loop to iterate in a deterministic order (probably just sort the dictionary keys), and verifying that fixes the issue? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:154,safety,test,test,154,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:193,safety,review,review,193,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:149,testability,unit,unit,149,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:154,testability,test,test,154,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:193,testability,review,review,193,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/pull/298:26,usability,confirm,confirming,26,@danielkingai2 Thanks for confirming. I also agree that the change is better to be in candidate generator. I've made the changes and fixed a related unit test. Would appreciate it if you could review it.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/298
https://github.com/allenai/scispacy/issues/299:188,deployability,pipelin,pipeline,188,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:291,energy efficiency,model,model,291,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:363,energy efficiency,load,load,363,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:380,energy efficiency,model,model,380,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:479,energy efficiency,load,load,479,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:116,integrability,compon,components,116,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:188,integrability,pipelin,pipeline,188,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:116,interoperability,compon,components,116,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:439,interoperability,share,share,439,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:116,modifiability,compon,components,116,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:200,performance,disk,disk,200,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:310,performance,disk,disk,310,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:363,performance,load,load,363,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:391,performance,disk,disk,391,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:473,performance,disk,disk,473,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:479,performance,load,load,479,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:489,performance,disk,disk,489,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:99,reliability,doe,doesn,99,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:336,reliability,doe,doesn,336,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:403,reliability,doe,does,403,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:291,security,model,model,291,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/299:380,security,model,model,380,"hm, i'm not sure off the top of my head what the issue is. That being said, the entity linker pipe doesn't have any components for you to retrain, so you should just be able to write your pipeline to disk without the entity linker, and then create an entity linker pipe wherever you use the model you wrote to disk. . When you say ""but doesn't work when I try to load the trained model from disk"", what does not working look like? Can you share the code you use to save to disk, load from disk, and what happens when you try to?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/299
https://github.com/allenai/scispacy/issues/303:31,usability,progress,progress,31,"Yes, please see #295. It is in progress, thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:62,usability,close,close,62,"Appreciate the rapid response, thank you! Please feel free to close this issue if you feel it's duplicative.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:0,availability,Sli,Slightly,0,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:126,deployability,releas,release,126,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:249,deployability,releas,release,249,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:47,energy efficiency,model,models,47,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:209,energy efficiency,model,models,209,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:63,interoperability,compatib,compatible,63,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:0,reliability,Sli,Slightly,0,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:47,security,model,models,47,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:209,security,model,models,209,"Slightly premature closing. We've uploaded new models that are compatible with master (and spacy 3), but haven't quite done a release yet. If you need it urgently, you should be able to use master and the new models. Otherwise, we should be doing a release sometime in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:0,deployability,Releas,Release,0,Release released!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/303:8,deployability,releas,released,8,Release released!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/303
https://github.com/allenai/scispacy/issues/304:25,interoperability,share,share,25,I am interested. Can you share some references so that I can implement it?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:67,energy efficiency,model,models,67,@danielkingai2 . 1. where can I find **training code** for all the models mentioned below. 2. Are they supervised or unsupervised. 3. I want to know what options do I have to proceed with unsupervised training. ![Screenshot 2021-03-06 at 1 43 49 AM](https://user-images.githubusercontent.com/45640029/110168602-6fd03700-7e1d-11eb-947b-9a8570648e25.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:67,security,model,models,67,@danielkingai2 . 1. where can I find **training code** for all the models mentioned below. 2. Are they supervised or unsupervised. 3. I want to know what options do I have to proceed with unsupervised training. ![Screenshot 2021-03-06 at 1 43 49 AM](https://user-images.githubusercontent.com/45640029/110168602-6fd03700-7e1d-11eb-947b-9a8570648e25.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:258,usability,user,user-images,258,@danielkingai2 . 1. where can I find **training code** for all the models mentioned below. 2. Are they supervised or unsupervised. 3. I want to know what options do I have to proceed with unsupervised training. ![Screenshot 2021-03-06 at 1 43 49 AM](https://user-images.githubusercontent.com/45640029/110168602-6fd03700-7e1d-11eb-947b-9a8570648e25.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:30,energy efficiency,model,models,30,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:151,energy efficiency,model,model,151,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:220,energy efficiency,model,model,220,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:30,security,model,models,30,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:151,security,model,model,151,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/issues/304:220,security,model,model,220,"The full training for all the models is done via spacy projects, so using our project.yml and the configs it references. It is supervised. The scibert model starts from the scibert pretrained weights, but any pretrained model from huggingface could be used instead.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/304
https://github.com/allenai/scispacy/pull/307:41,deployability,releas,releases,41,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:127,deployability,releas,releases,127,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:206,energy efficiency,load,load,206,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:298,modifiability,inherit,inherited,298,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:519,modifiability,inherit,inherited,519,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:206,performance,load,load,206,"Has NER been added already? I see in the releases that NER is yet to be added for scibert. https://github.com/allenai/scispacy/releases. But it seems to run fine in the code. ```. import spacy. nlp = spacy.load(""en_core_sci_scibert""). doc2 = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. inherited motor neuron disease caused by the expansion \. of a polyglutamine tract within the androgen receptor (AR). \. SBMA can be caused by this easily.""). doc2.ents. > (Spinal,. > bulbar muscular atrophy,. > SBMA,. > inherited,. > motor neuron disease,. > expansion,. > polyglutamine tract,. > androgen receptor,. > AR,. > SBMA). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:170,deployability,pipelin,pipeline,170,"Not really. Maintaining scispacy is not my main project, so I don't have timelines for anything. It should't be hard to train your own if you want to. The whole training pipeline is in a config file, using spacy projects. Feel free to open an issue and ask questions if you are interested in doing this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:170,integrability,pipelin,pipeline,170,"Not really. Maintaining scispacy is not my main project, so I don't have timelines for anything. It should't be hard to train your own if you want to. The whole training pipeline is in a config file, using spacy projects. Feel free to open an issue and ask questions if you are interested in doing this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:12,modifiability,Maintain,Maintaining,12,"Not really. Maintaining scispacy is not my main project, so I don't have timelines for anything. It should't be hard to train your own if you want to. The whole training pipeline is in a config file, using spacy projects. Feel free to open an issue and ask questions if you are interested in doing this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:73,performance,time,timelines,73,"Not really. Maintaining scispacy is not my main project, so I don't have timelines for anything. It should't be hard to train your own if you want to. The whole training pipeline is in a config file, using spacy projects. Feel free to open an issue and ask questions if you are interested in doing this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/pull/307:12,safety,Maintain,Maintaining,12,"Not really. Maintaining scispacy is not my main project, so I don't have timelines for anything. It should't be hard to train your own if you want to. The whole training pipeline is in a config file, using spacy projects. Feel free to open an issue and ask questions if you are interested in doing this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/307
https://github.com/allenai/scispacy/issues/311:21,availability,down,downgrade,21,"For now, you need to downgrade spacy to 2.x. You can also install scispacy from master, or wait, and we should be doing a new release compatible with spacy 3 in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:58,deployability,instal,install,58,"For now, you need to downgrade spacy to 2.x. You can also install scispacy from master, or wait, and we should be doing a new release compatible with spacy 3 in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:126,deployability,releas,release,126,"For now, you need to downgrade spacy to 2.x. You can also install scispacy from master, or wait, and we should be doing a new release compatible with spacy 3 in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:134,interoperability,compatib,compatible,134,"For now, you need to downgrade spacy to 2.x. You can also install scispacy from master, or wait, and we should be doing a new release compatible with spacy 3 in the next week or so.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:28,deployability,instal,install,28,"I see. Thanks a lot, I will install scispacy from master.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:6,deployability,releas,released,6,"We've released the latest version, so this should work now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:26,deployability,version,version,26,"We've released the latest version, so this should work now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:26,integrability,version,version,26,"We've released the latest version, so this should work now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/311:26,modifiability,version,version,26,"We've released the latest version, so this should work now.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/311
https://github.com/allenai/scispacy/issues/316:470,availability,avail,available,470,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:517,availability,avail,available-models,517,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:409,deployability,Depend,Depending,409,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:28,energy efficiency,core,core,28,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:42,energy efficiency,model,models,42,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:193,energy efficiency,core,core,193,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:204,energy efficiency,model,models,204,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:463,energy efficiency,model,models,463,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:527,energy efficiency,model,models,527,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:409,integrability,Depend,Depending,409,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:555,integrability,filter,filter,555,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:450,interoperability,specif,specific,450,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:409,modifiability,Depend,Depending,409,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:470,reliability,availab,available,470,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:517,reliability,availab,available-models,517,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:409,safety,Depend,Depending,409,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:470,safety,avail,available,470,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:517,safety,avail,available-models,517,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:42,security,model,models,42,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:204,security,model,models,204,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:359,security,ident,identify,359,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:463,security,model,models,463,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:470,security,availab,available,470,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:517,security,availab,available-models,517,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:409,testability,Depend,Depending,409,"The entity extractor in the core scispacy models is trained on the medmentions dataset (https://github.com/chanzuckerberg/MedMentions). This is a _very_ different dataset from the one that the core spacy models are trained on, and will broadly recognize things that can be linked to UMLS (https://www.nlm.nih.gov/research/umls/index.html). This means it will identify seemingly generic terms like`considers`. Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models), or you may want to filter entities based on their UMLS type (type tree is here: https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/umls_semantic_type_tree.tsv)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:63,availability,avail,available,63,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:110,availability,avail,available-models,110,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2,deployability,Depend,Depending,2,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:143,deployability,instal,install,143,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:202,deployability,releas,releases,202,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:524,deployability,instal,install,524,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:583,deployability,releas,releases,583,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:56,energy efficiency,model,models,56,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:120,energy efficiency,model,models,120,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:511,energy efficiency,model,model,511,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1050,energy efficiency,estimat,estimates,1050,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2,integrability,Depend,Depending,2,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:43,interoperability,specif,specific,43,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2,modifiability,Depend,Depending,2,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:63,reliability,availab,available,63,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:110,reliability,availab,available-models,110,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2,safety,Depend,Depending,2,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:63,safety,avail,available,63,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:110,safety,avail,available-models,110,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:56,security,model,models,56,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:63,security,availab,available,63,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:110,security,availab,available-models,110,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:511,security,model,model,511,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2,testability,Depend,Depending,2,"> Depending on your use case, we have more specific NER models available (https://github.com/allenai/scispacy#available-models). I tried . pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz. and got worse (but different) results. considers is no longer an `entity` but other non-entity words became entities. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY')]`. And for md model:. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_md-0.3.0.tar.gz. `. `[('San Francisco', 0, 13, 'ENTITY'), ('banning', 24, 31, 'ENTITY'), ('sidewalk', 32, 40, 'ENTITY'), ('delivery', 41, 49, 'ENTITY'), ('robots', 50, 56, 'ENTITY')]`. What I was hoping for, was that scispacy would be better at picking out biomedical entities than spacy. I have a mix of biomedical and non-biomedical text, so I was hoping a differential between the two would be useful. Also... are there vocabulary size estimates for: en_ner_craft_md, en_ner_jnlpba_md, en_ner_bc5cdr_md, en_ner_bionlp13cg_md sizes? . (I'm a bit confused in this transition period between Spacy 2.3.5 -> 3.0.1 and SciSpacy 0.3.0 and 0.4.0. It seems like 2.3.5 goes with 0.3.0 and not 0.4.0).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1063,deployability,compos,composed,1063,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1351,deployability,compos,composed,1351,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:33,energy efficiency,model,models,33,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:507,energy efficiency,core,core,507,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:512,energy efficiency,model,models,512,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:707,energy efficiency,model,model,707,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:805,energy efficiency,core,core,805,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:816,energy efficiency,model,models,816,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:936,energy efficiency,load,load,936,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:981,energy efficiency,load,load,981,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:723,integrability,filter,filtering,723,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:20,interoperability,specif,specific,20,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:667,interoperability,specif,specific,667,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1063,modifiability,compos,composed,1063,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1351,modifiability,compos,composed,1351,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:936,performance,load,load,936,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:981,performance,load,load,981,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1216,reliability,growth,growth,1216,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1504,reliability,growth,growth,1504,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1785,reliability,growth,growth,1785,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:33,security,model,models,33,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:512,security,model,models,512,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:707,security,model,model,707,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:816,security,model,models,816,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:224,usability,user,user-images,224,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:395,usability,user,user-images,395,"These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. <img width=""866"" alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. <img width=""557"" alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. ```. In [27]: sm = spacy.load('en_core_web_sm'). In [28]: sci = spacy.load('en_core_sci_sm'). In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. ...: sms and many viruses.""). In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. ...: nisms and many viruses.""). In [31]: sm_doc.ents. Out[31]: (two,). In [32]: sci_doc.ents. Out[32]: . (Deoxyribonucleic acid,. molecule,. polynucleotide chains,. coil,. double helix,. genetic instructions,. development,. functioning,. growth,. reproduction,. organisms,. viruses). ```.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1103,deployability,compos,composed,1103,"dical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1399,deployability,compos,composed,1399,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:37,energy efficiency,model,models,37,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:521,energy efficiency,core,core,521,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:526,energy efficiency,model,models,526,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:721,energy efficiency,model,model,721,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:825,energy efficiency,core,core,825,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:836,energy efficiency,model,models,836,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:964,energy efficiency,load,load,964,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1015,energy efficiency,load,load,1015," ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2076,energy efficiency,model,models,2076,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:737,integrability,filter,filtering,737,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:24,interoperability,specif,specific,24,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:681,interoperability,specif,specific,681,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2063,interoperability,specif,specific,2063,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2152,interoperability,share,share,2152,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1103,modifiability,compos,composed,1103,"dical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1399,modifiability,compos,composed,1399,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:964,performance,load,load,964,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1015,performance,load,load,1015," ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1256,reliability,growth,growth,1256,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1552,reliability,growth,growth,1552,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:1869,reliability,growth,growth,1869,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:37,security,model,models,37,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:526,security,model,models,526,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:721,security,model,model,721,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:836,security,model,models,836,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:2076,security,model,models,2076,"mg alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my own corpus from scratch? Basically, I want to train my own ""more specific NER models"" with my corpus. How do I proceed.. Any reference links that you can share?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:230,usability,user,user-images,230,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:403,usability,user,user-images,403,". > These are the ""more specific NER models"" I was referring to. They are each trained on a different biomedical corpus, and have different entity types. > <img alt=""Screen Shot 2021-02-16 at 10 00 04 AM"" width=""866"" src=""https://user-images.githubusercontent.com/43149077/108102715-c11ab100-703d-11eb-8aaa-c9ff1f79634f.png"">. > <img alt=""Screen Shot 2021-02-16 at 10 15 33 AM"" width=""557"" src=""https://user-images.githubusercontent.com/43149077/108104293-f7593000-703f-11eb-9645-32d813b9c147.png"">. > . > As I said, the core models are trained on the medmentions dataset, which takes a broad definition for what is an entity (and there will be both obvious mistakes, and use case specific mistakes, as this is a trained model). Further filtering can be done using the entity linker and its types. > . > That being said, the core spacy models are not trained to recognize biomedical entities at all. Here is an example difference. > . > ```. > In [27]: sm = spacy.load('en_core_web_sm'). > . > In [28]: sci = spacy.load('en_core_sci_sm'). > . > In [29]: sm_doc = sm(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known organi. > ...: sms and many viruses.""). > . > In [30]: sci_doc = sci(""Deoxyribonucleic acid is a molecule composed of two polynucleotide chains that coil around each other to form a double helix carrying genetic instructions for the development, functioning, growth and reproduction of all known orga. > ...: nisms and many viruses.""). > . > In [31]: sm_doc.ents. > Out[31]: (two,). > . > In [32]: sci_doc.ents. > Out[32]: . > (Deoxyribonucleic acid,. > molecule,. > polynucleotide chains,. > coil,. > double helix,. > genetic instructions,. > development,. > functioning,. > growth,. > reproduction,. > organisms,. > viruses). > ```. Hey @danielkingai2 Can I know how to train my own ner extractor on my ow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:315,usability,guid,guide,315,"we're using spacy 3's new config system and [spacy projects](https://spacy.io/usage/projects). So our project file lives [here](https://github.com/allenai/scispacy/blob/master/project.yml) and our configs live [here](https://github.com/allenai/scispacy/tree/master/configs). You should be able to follow these as a guide, an basically just run the ner training commands, but with your data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:361,usability,command,commands,361,"we're using spacy 3's new config system and [spacy projects](https://spacy.io/usage/projects). So our project file lives [here](https://github.com/allenai/scispacy/blob/master/project.yml) and our configs live [here](https://github.com/allenai/scispacy/tree/master/configs). You should be able to follow these as a guide, an basically just run the ner training commands, but with your data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:13,availability,error,error,13,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:48,availability,down,download,48,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:81,interoperability,compatib,compatible,81,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:92,modifiability,pac,package,92,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:13,performance,error,error,13,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:13,safety,error,error,13,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/316:13,usability,error,error,13,getting this error in pycharm whenever i try to download en_ner_bc5cdr_md .  No compatible package found for 'en_ner_bc5cdr_md' (spaCy v3.5.3). python is 3.11.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316
https://github.com/allenai/scispacy/issues/317:20,energy efficiency,model,model,20,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:96,energy efficiency,gpu,gpu,96,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:42,integrability,transform,transformers,42,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:42,interoperability,transform,transformers,42,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:5,modifiability,reu,reuploaded,5,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:96,performance,gpu,gpu,96,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/317:20,security,model,model,20,"I've reuploaded the model with the `spacy-transformers` requirement, and am adding a note about gpu usage in the readme here #320. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/317
https://github.com/allenai/scispacy/issues/318:94,availability,down,downloading,94,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:72,deployability,updat,update,72,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:23,energy efficiency,model,models,23,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:35,energy efficiency,model,model,35,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:110,energy efficiency,model,models,110,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:72,safety,updat,update,72,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:23,security,model,models,23,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:35,security,model,model,35,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:72,security,updat,update,72,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:110,security,model,models,110,"Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:96,availability,down,downloading,96,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:149,availability,down,downloaded,149,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:219,availability,avail,available-models,219,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:74,deployability,updat,update,74,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:293,deployability,releas,releases,293,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:25,energy efficiency,model,models,25,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:37,energy efficiency,model,model,37,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:112,energy efficiency,model,models,112,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:164,energy efficiency,model,model,164,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:229,energy efficiency,model,models,229,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:219,reliability,availab,available-models,219,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:74,safety,updat,update,74,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:219,safety,avail,available-models,219,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:25,security,model,models,25,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:37,security,model,model,37,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:74,security,updat,update,74,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:112,security,model,models,112,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:164,security,model,model,164,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:219,security,availab,available-models,219,"> Did you redownload the models/what model are you using? This is a major update, that requires downloading new models (links in readme). Yes, I had downloaded the model mentioned in https://github.com/allenai/scispacy#available-models. i.e. https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:46,deployability,instal,installing,46,"hmm, i'm not able to reproduce. Could you try installing everything in a new clean environment?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:23,deployability,instal,installing,23,"@s20ss Yes, please try installing everything in a new clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:132,deployability,instal,installing,132,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:180,energy efficiency,core,core,180,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:186,energy efficiency,model,models,186,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:203,reliability,doe,doesn,203,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:66,safety,avoid,avoid,66,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:186,security,model,models,186,"As @danielkingai2 suggests, a new clean environment is needed. To avoid creating a new environment, I had tried uninstalling and re-installing spacy and scispacy along with the en-core* models. But that doesn't works. Not sure which library is the culprit. Closing the issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:11,deployability,fail,failed,11,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:55,deployability,Instal,Installed,55,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:111,deployability,automat,automatically,111,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:48,energy efficiency,model,model,48,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:75,energy efficiency,model,model,75,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:11,reliability,fail,failed,11,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:48,security,model,model,48,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:75,security,model,model,75,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/318:111,testability,automat,automatically,111,"For me, It failed when trying to use the Medium model. Installed the Small model, tried it. Worked and then it automatically worked for the medium as well. Strange",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/318
https://github.com/allenai/scispacy/issues/319:57,deployability,upgrad,upgrade,57,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:88,deployability,version,version,88,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,deployability,version,version,107,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:88,integrability,version,version,88,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,integrability,version,version,107,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:57,modifiability,upgrad,upgrade,57,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:88,modifiability,version,version,88,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,modifiability,version,version,107,I suspect this is the bug fixed by #298. Are you able to upgrade to the latest scispacy version (and spacy version)?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:8,deployability,upgrad,upgrading,8,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,deployability,version,version,107,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:181,energy efficiency,load,loading,181,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,integrability,version,version,107,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:8,modifiability,upgrad,upgrading,8,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:107,modifiability,version,version,107,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:150,performance,memor,memory,150,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:181,performance,load,loading,181,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:150,usability,memor,memory,150,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:258,usability,user,user-images,258,"Indeed, upgrading to the latest scispacy (and spacy) fixed it - thanks a lot! :). Quick sidenote - the new version seems to be eating up WAY too much memory, is this expected? (I'm loading all three - core_sci_sm, ner_bc5cdr & ner_bionlp):. ![image](https://user-images.githubusercontent.com/2000186/108050698-fb9d4180-706f-11eb-83fc-898145a66c8b.png). Thanks again!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:13,energy efficiency,load,loading,13,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:40,energy efficiency,load,loaded,40,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:59,energy efficiency,model,models,59,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:13,performance,load,loading,13,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:40,performance,load,loaded,40,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:59,security,model,models,59,"What are you loading/running exactly? I loaded those three models, and it used ~3G of ram.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:677,deployability,pipelin,pipelines,677,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:973,deployability,pipelin,pipelines,973,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:16,energy efficiency,load,loading,16,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:34,energy efficiency,model,models,34,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:148,energy efficiency,model,model,148,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:254,energy efficiency,model,model,254,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:261,energy efficiency,core,core,261,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:276,energy efficiency,load,load,276,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:300,energy efficiency,model,model,300,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:323,energy efficiency,load,load,323,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:349,energy efficiency,model,model,349,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:373,energy efficiency,load,load,373,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:403,energy efficiency,model,model,403,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:410,energy efficiency,core,core,410,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:468,energy efficiency,model,model,468,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:535,energy efficiency,model,model,535,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:677,integrability,pipelin,pipelines,677,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:779,integrability,compon,component,779,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:973,integrability,pipelin,pipelines,973,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:779,interoperability,compon,component,779,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:779,modifiability,compon,component,779,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:16,performance,load,loading,16,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:276,performance,load,load,276,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:323,performance,load,load,323,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:373,performance,load,load,373,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:864,performance,memor,memory,864,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:712,reliability,doe,doesn,712,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:34,security,model,models,34,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:148,security,model,model,148,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:254,security,model,model,254,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:300,security,model,model,300,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:349,security,model,model,349,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:403,security,model,model,403,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:468,security,model,model,468,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:535,security,model,model,535,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:864,usability,memor,memory,864,"That's correct: loading the three models take up ~3GB of RAM. It's the EntityLinker that takes up the remaining space. Here's what I'm doing:. ```. model = {}. linker_config={""resolve_abbreviations"": True, ""name"": ""umls"", ""max_entities_per_mention"": 1}. model['core'] = spacy.load(""en_core_sci_sm""). model['bc5cdr']= spacy.load(""en_ner_bc5cdr_md""). model['bionlp'] = spacy.load(""en_ner_bionlp13cg_md""). model['core'].add_pipe(""scispacy_linker"", config=linker_config). model['bc5cdr'].add_pipe(""scispacy_linker"", config=linker_config). model['bionlp'].add_pipe(""scispacy_linker"", config=linker_config). ```. In spacy 2.x we could instantiate the EntityLinker and use it for all pipelines. With spacy 3.x add_pipe doesn't take callables anymore and needs a string (the name of the component factory). This duplicates the EntityLinker instantiation, I think, causing memory use to balloon up. Thoughts on working around it and using a single EntityLinker instantiation across pipelines? Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:76,energy efficiency,load,loading,76,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:114,energy efficiency,core,core,114,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:119,energy efficiency,model,model,119,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:140,energy efficiency,model,model,140,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:147,energy efficiency,core,core,147,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:196,energy efficiency,model,model,196,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:76,performance,load,loading,76,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:119,security,model,model,119,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:140,security,model,model,140,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/319:196,security,model,model,196,"ahh, i see. As a work around, i think you can do something like this, after loading the entity linker on just the core model. ```. linker = model['core'].get_pipe('scispacy_linker'). doc = linker(model['bc5cdr'](text)). ```. I'm not sure if there is a way to add a pipe without creating a copy of it in spacy3, will have to look into it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/319
https://github.com/allenai/scispacy/issues/321:29,deployability,version,versions,29,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:196,energy efficiency,model,models,196,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:29,integrability,version,versions,29,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:29,modifiability,version,versions,29,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:140,security,token,tokenization,140,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:166,security,token,tokenizer,166,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:196,security,model,models,196,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:236,security,token,tokenizer,236,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:229,usability,custom,custom,229,"Could you please provide the versions of scispacy and spacy in your environment, and a concrete example of what you are doing and where the tokenization differs? The tokenizer on all the scispacy models should be the same as the custom tokenizer in the repo.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:45,energy efficiency,model,model,45,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:116,energy efficiency,load,load,116,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:116,performance,load,load,116,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:45,security,model,model,45,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:285,security,Token,Tokens,285,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:442,security,ident,identified,442,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:456,security,token,tokens,456,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:506,security,token,tokens,506,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:189,usability,user,user-images,189,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:311,usability,user,user-images,311,scispacy==0.3.0. spacy.__version__ == 2.3.5. model = en_ner_bionlp13cg_md-0.3.0. Code :. nlp = en_ner_bionlp13cg_md.load(). doc = nlp(' '.join(text)). Entities extracted:. ![image](https://user-images.githubusercontent.com/63639210/108105266-b4717980-708c-11eb-9ca4-e3aed22f03d5.png). Tokens:. ![image](https://user-images.githubusercontent.com/63639210/108105372-d1a64800-708c-11eb-9dda-256921b5fa9d.png). Some of the 4-word entities aren't identified as tokens. Is it possible to get all the entities as tokens?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:201,deployability,compos,composed,201,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:537,energy efficiency,model,model,537,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:548,energy efficiency,model,models,548,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:2,integrability,coupl,couple,2,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:2,modifiability,coupl,couple,2,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:201,modifiability,compos,composed,201,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:23,security,token,token,23,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:142,security,token,tokens,142,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:219,security,token,tokens,219,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:344,security,token,token,344,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:461,security,Token,Tokens,461,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:537,security,model,model,537,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:548,security,model,models,548,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/321:2,testability,coupl,couple,2,"A couple things. 1) a ""token"" is approximately a ""word"". Entities can be multiple words in general. The entity boundaries should line up with tokens though. So, ""New York City"" might be an entity, but composed of three tokens, ""New"", ""York"", and ""City"". If you want a vector for ""New York City"", one way to do this is to average the individual token vectors. I do not know if this would work well for your use case, but it is a common thing to do. 2) From the ""Tokens"" output, it looks like you are passing a long list of strings to the model. The models were trained on proper sentences, and may not work as well when applied to ungrammatical text.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/321
https://github.com/allenai/scispacy/issues/322:95,availability,down,downloading,95,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:194,availability,avail,available-models,194,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:62,deployability,version,versions,62,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:124,deployability,version,versions,124,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:78,energy efficiency,model,models,78,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:204,energy efficiency,model,models,204,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:62,integrability,version,versions,62,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:124,integrability,version,versions,124,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:62,modifiability,version,versions,62,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:124,modifiability,version,versions,124,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:194,reliability,availab,available-models,194,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:194,safety,avail,available-models,194,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:78,security,model,models,78,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:194,security,availab,available-models,194,I hit this when I used the latest spacy + scispacy with older versions of the models. Fixed by downloading the latest 0.4.0 versions found [on the home page](https://github.com/allenai/scispacy#available-models).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:107,deployability,updat,updated,107,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:50,energy efficiency,model,models,50,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:132,energy efficiency,model,models,132,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:107,safety,updat,updated,107,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:50,security,model,models,50,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:107,security,updat,updated,107,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:132,security,model,models,132,"Excellent, it has worked, thank you. The links to models at https://allenai.github.io/scispacy/ need to be updated, I have used the models 0.3.0 from there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:17,availability,error,error,17,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,availability,Error,Error,191,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:662,availability,Error,Errors,662,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:806,availability,error,error,806,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:197,deployability,stack,stack,197,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:139,energy efficiency,load,load,139,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:508,energy efficiency,model,model,508,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:550,energy efficiency,model,model,550,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:565,energy efficiency,model,model,565,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:714,energy efficiency,model,model,714,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:251,integrability,Buffer,Buffer,251,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:462,modifiability,pac,packages,462,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:17,performance,error,error,17,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:139,performance,load,load,139,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,performance,Error,Error,191,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:262,performance,memor,memoryview,262,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:662,performance,Error,Errors,662,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:806,performance,error,error,806,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:17,safety,error,error,17,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,safety,Error,Error,191,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:344,safety,except,exception,344,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:363,safety,except,exception,363,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:615,safety,except,except,615,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:662,safety,Error,Errors,662,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:806,safety,error,error,806,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:508,security,model,model,508,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:550,security,model,model,550,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:565,security,model,model,565,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:714,security,model,model,714,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:397,testability,Trace,Traceback,397,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:17,usability,error,error,17,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:191,usability,Error,Error,191,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:262,usability,memor,memoryview,262,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:662,usability,Error,Errors,662,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:806,usability,error,error,806,"I am facing this error on colab scispacy-0.4.0+ en_core_sci_sm-0.4.0. Any pointers how to solve this? Code. =====. nlpipe = en_core_sci_sm.load(). doc = nlpipe(""This is a scientific text.""). Error stack. =======. py.pyx in blis.py.gemm(). ValueError: Buffer and memoryview are not contiguous in the same dimension. During handling of the above exception, another exception occurred:. RuntimeError Traceback (most recent call last). /usr/local/lib/python3.6/dist-packages/spacy/ml/staticvectors.py in forward(model, docs, is_train). 46 vectors_data = model.ops.gemm(model.ops.as_contig(V[rows]), W, trans2=True). 47 except ValueError:. ---> 48 raise RuntimeError(Errors.E896). 49 output = Ragged(. 50 vectors_data, model.ops.asarray([len(doc) for doc in docs], dtype=""i""). RuntimeError: [E896] There was an error using the static vectors. Ensure that the vectors of the vocab are properly initialized, or set 'include_static_vectors' to False.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:74,deployability,version,version,74,Could you show the output of `pip list`? this is almost certainly a model version issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:68,energy efficiency,model,model,68,Could you show the output of `pip list`? this is almost certainly a model version issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:74,integrability,version,version,74,Could you show the output of `pip list`? this is almost certainly a model version issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:74,modifiability,version,version,74,Could you show the output of `pip list`? this is almost certainly a model version issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:68,security,model,model,68,Could you show the output of `pip list`? this is almost certainly a model version issue.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:355,availability,state,state,355,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:81,deployability,instal,install,81,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,deployability,version,versions,100,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:270,deployability,version,version,270,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:264,energy efficiency,model,model,264,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,integrability,version,versions,100,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:270,integrability,version,version,270,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:355,integrability,state,state,355,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:100,modifiability,version,versions,100,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:270,modifiability,version,version,270,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:264,security,model,model,264,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:331,security,modif,modified,331,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:20,testability,Simpl,Simply,20,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:20,usability,Simpl,Simply,20,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:349,usability,close,close,349,"Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. > . > . > Could you show the output of pip list? this is almost certainly a model version issue. > . > . > You are receiving this because you modified the open/close state. > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:476,availability,state,state,476,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:189,deployability,instal,install,189,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:208,deployability,version,versions,208,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:388,deployability,version,version,388,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:382,energy efficiency,model,model,382,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:208,integrability,version,versions,208,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:388,integrability,version,version,388,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:476,integrability,state,state,476,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:208,modifiability,version,versions,208,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:388,modifiability,version,version,388,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:382,security,model,model,382,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:452,security,modif,modified,452,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:128,testability,Simpl,Simply,128,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:128,usability,Simpl,Simply,128,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:470,usability,close,close,470,"And obviously restart the kernel! > On 18. 2. 2021, at 20:22, Jiri Hradec <jiri@hrad.ec> wrote:. > . > . > Had the same issue. Simply meticilously uninstall all libraries and make sure to install the newest versions. . > . >>> On 18. 2. 2021, at 18:48, Daniel King <notifications@github.com> wrote:. >>> . >> . >> Could you show the output of pip list? this is almost certainly a model version issue. >> . >> . >> You are receiving this because you modified the open/close state. >> Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:37,energy efficiency,model,models,37,Output of pip list for spacy and the models. -------. en-core-sci-sm 0.4.0 . spacy 3.0.3 . spacy-legacy 3.0.1 . scipy 1.4.1 . scispacy 0.4.0 .,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:57,energy efficiency,core,core-sci-sm,57,Output of pip list for spacy and the models. -------. en-core-sci-sm 0.4.0 . spacy 3.0.3 . spacy-legacy 3.0.1 . scipy 1.4.1 . scispacy 0.4.0 .,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:37,security,model,models,37,Output of pip list for spacy and the models. -------. en-core-sci-sm 0.4.0 . spacy 3.0.3 . spacy-legacy 3.0.1 . scipy 1.4.1 . scispacy 0.4.0 .,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:145,deployability,version,version,145,what about the output of this? ```. In [7]: scispacy.__version__. Out[7]: '0.4.0'. In [8]: spacy.__version__. Out[8]: '3.0.3'. In [9]: nlp.meta['version']. Out[9]: '0.4.0'. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:145,integrability,version,version,145,what about the output of this? ```. In [7]: scispacy.__version__. Out[7]: '0.4.0'. In [8]: spacy.__version__. Out[8]: '3.0.3'. In [9]: nlp.meta['version']. Out[9]: '0.4.0'. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/322:145,modifiability,version,version,145,what about the output of this? ```. In [7]: scispacy.__version__. Out[7]: '0.4.0'. In [8]: spacy.__version__. Out[8]: '3.0.3'. In [9]: nlp.meta['version']. Out[9]: '0.4.0'. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/322
https://github.com/allenai/scispacy/issues/326:46,deployability,depend,dependency,46,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:68,deployability,depend,dependency,68,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:339,deployability,depend,dependency,339,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:477,deployability,depend,dependency,477,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:626,deployability,observ,observe,626,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:643,deployability,depend,dependency,643,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:574,energy efficiency,adapt,adapting,574,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:46,integrability,depend,dependency,46,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:68,integrability,depend,dependency,68,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:339,integrability,depend,dependency,339,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:477,integrability,depend,dependency,477,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:574,integrability,adapt,adapting,574,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:643,integrability,depend,dependency,643,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:330,interoperability,specif,specific,330,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:574,interoperability,adapt,adapting,574,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:46,modifiability,depend,dependency,46,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:68,modifiability,depend,dependency,68,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:339,modifiability,depend,dependency,339,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:477,modifiability,depend,dependency,477,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:574,modifiability,adapt,adapting,574,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:643,modifiability,depend,dependency,643,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:46,safety,depend,dependency,46,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:68,safety,depend,dependency,68,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:339,safety,depend,dependency,339,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:477,safety,depend,dependency,477,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:643,safety,depend,dependency,643,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:46,testability,depend,dependency,46,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:68,testability,depend,dependency,68,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:339,testability,depend,dependency,339,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:477,testability,depend,dependency,477,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:626,testability,observ,observe,626,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:643,testability,depend,dependency,643,"so, I think this is due to differences in the dependency parse. our dependency parser is more accurate on biomedical data (but different from spacy's), and spacy's noun chunker is defined here (https://github.com/explosion/spaCy/blob/a59f3fcf5dab3acf5570483cc314b47cc5833f39/spacy/lang/en/syntax_iterators.py#L8), with respect to specific dependency relations. See an example of the difference for your sentence below. Perhaps we should write our own noun chunker based on our dependency parser, but I am really not an expert in linguistics. You might get some mileage from adapting spacy's noun chunker based on patterns you observe from our dependency parser. Also, @DeNeutoy do you have any thoughts about this? ```. In [14]: [(t.text, t.pos_, t.dep_) for t in sci_doc]. Out[14]: . [('CCR5(+', 'NOUN', 'nsubjpass'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'NOUN', 'compound'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('increased', 'VERB', 'ROOT'),. ('in', 'ADP', 'case'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'nmod'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'nmod:poss'),. ('ligands', 'NOUN', 'conj'),. ('MIP-1alpha', 'NOUN', 'dep'),. ('and', 'CCONJ', 'cc'),. ('IP-10', 'NOUN', 'conj'),. ('are', 'VERB', 'auxpass'),. ('expressed', 'VERB', 'conj'),. ('in', 'ADP', 'case'),. ('demyelinating', 'VERB', 'amod'),. ('brain', 'NOUN', 'compound'),. ('lesions', 'NOUN', 'nmod'),. ('.', 'PUNCT', 'punct')]. In [15]: [(t.text, t.pos_, t.dep_) for t in web_doc]. Out[15]: . [('CCR5(+', 'NOUN', 'ROOT'),. (')', 'PUNCT', 'punct'),. ('and', 'CCONJ', 'cc'),. ('CXCR3(+', 'PROPN', 'npadvmod'),. (')', 'PUNCT', 'punct'),. ('T', 'NOUN', 'compound'),. ('cells', 'NOUN', 'nsubjpass'),. ('are', 'AUX', 'auxpass'),. ('increased', 'VERB', 'conj'),. ('in', 'ADP', 'prep'),. ('multiple', 'ADJ', 'amod'),. ('sclerosis', 'NOUN', 'pobj'),. ('and', 'CCONJ', 'cc'),. ('their', 'PRON', 'poss'),. ('ligands', 'NOUN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:22,energy efficiency,adapt,adapting,22,Did you have any luck adapting the noun chunker?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:22,integrability,adapt,adapting,22,Did you have any luck adapting the noun chunker?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:22,interoperability,adapt,adapting,22,Did you have any luck adapting the noun chunker?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:22,modifiability,adapt,adapting,22,Did you have any luck adapting the noun chunker?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:124,performance,time,time,124,Sorry for my late response. Your answer was very helpful! I decided to try a different approach since I did not have enough time in my project to look for these patterns. Thank you very much!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/326:49,usability,help,helpful,49,Sorry for my late response. Your answer was very helpful! I decided to try a different approach since I did not have enough time in my project to look for these patterns. Thank you very much!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/326
https://github.com/allenai/scispacy/issues/327:249,availability,error,error,249,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:196,deployability,depend,dependency,196,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:259,deployability,observ,observed,259,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11,integrability,coupl,couple,11,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:196,integrability,depend,dependency,196,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11,modifiability,coupl,couple,11,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:196,modifiability,depend,dependency,196,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:249,performance,error,error,249,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:196,safety,depend,dependency,196,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:249,safety,error,error,249,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11,testability,coupl,couple,11,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:196,testability,depend,dependency,196,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:259,testability,observ,observed,259,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:249,usability,error,error,249,"You have a couple options that I know of to get different sentence segmentation. In general, nothing is going to be perfect. In particular, the default spacy sentence segmentation is based on the dependency parse and for sure can do things like the error you observed. FWIW, if you add a `.` at the end of the first example, it gets it right. Options:. 1) Check out the pysbd-based sentence segmentation pipe here (https://github.com/allenai/scispacy/blob/5df54e468c649e465b98ff6d924fa910eb3cb50c/scispacy/custom_sentence_segmenter.py#L12). You can add it with from scispacy.custom_sentence_segmenter import pysbd_sentencizer; `nlp.add_pipe('pysbd_sentencizer', first=True)`. 2) You can use spacy's default rule based sentencizer by `nlp.add_pipe('sentencizer', first=True)`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:24,energy efficiency,load,load,24,"With . ```. nlp = spacy.load('en_core_sci_sm'). nlp.add_pipe('pysbd_sentencizer', first=True). ```. Is the scispacy model being used at all, or is it just pysbd being used?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:116,energy efficiency,model,model,116,"With . ```. nlp = spacy.load('en_core_sci_sm'). nlp.add_pipe('pysbd_sentencizer', first=True). ```. Is the scispacy model being used at all, or is it just pysbd being used?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:24,performance,load,load,24,"With . ```. nlp = spacy.load('en_core_sci_sm'). nlp.add_pipe('pysbd_sentencizer', first=True). ```. Is the scispacy model being used at all, or is it just pysbd being used?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:116,security,model,model,116,"With . ```. nlp = spacy.load('en_core_sci_sm'). nlp.add_pipe('pysbd_sentencizer', first=True). ```. Is the scispacy model being used at all, or is it just pysbd being used?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:23,deployability,pipelin,pipeline,23,you can view the whole pipeline via `nlp.pipeline`. It is just adding the pysbd pipe for sentence segmentation.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:41,deployability,pipelin,pipeline,41,you can view the whole pipeline via `nlp.pipeline`. It is just adding the pysbd pipe for sentence segmentation.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:23,integrability,pipelin,pipeline,23,you can view the whole pipeline via `nlp.pipeline`. It is just adding the pysbd pipe for sentence segmentation.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:41,integrability,pipelin,pipeline,41,you can view the whole pipeline via `nlp.pipeline`. It is just adding the pysbd pipe for sentence segmentation.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:882,availability,mask,masked,882,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2267,availability,down,downstream,2267,"el on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2445,availability,consist,consisting,2445,"o not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3400,availability,state,stateof-the-art,3400," SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are eff",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3746,availability,state,state-of-the-art,3746,"that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4440,availability,down,downstream,4440,"y how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4614,availability,incid,incidental,4614,"aper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose u",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5009,availability,state,state,5009," for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language proc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5363,availability,mask,masked,5363,"earn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6748,availability,down,downstream,6748,"el on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6926,availability,consist,consisting,6926,"o not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we stud",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7886,availability,state,stateof-the-art,7886,"OCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8232,availability,state,state-of-the-art,8232,"help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining object",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8929,availability,down,downstream,8929,"ow to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9103,availability,incid,incidental,9103,"r, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer')",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9498,availability,state,state,9498,"or document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9557,availability,error,error,9557,"ommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10115,availability,error,error,10115,"rvision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11170,availability,error,error,11170," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:14,deployability,pipelin,pipeline,14,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:66,deployability,pipelin,pipeline,66,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:150,deployability,pipelin,pipeline,150,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:269,deployability,pipelin,pipeline,269,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2691,deployability,continu,continues,2691,"training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7172,deployability,continu,continues,7172,"training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10204,deployability,Modul,ModuleNotFoundError,10204," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10294,deployability,modul,module,10294," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10666,deployability,Modul,ModuleNotFoundError,10666," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10690,deployability,modul,module,10690," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:710,energy efficiency,model,models,710,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:766,energy efficiency,model,model,766,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:898,energy efficiency,model,modeling,898,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1040,energy efficiency,optim,optimal,1040,"ibute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1269,energy efficiency,model,model,1269,"eline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1575,energy efficiency,model,models,1575," is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1598,energy efficiency,power,powerful,1598,"ndent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1642,energy efficiency,model,models,1642,"nspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the ben",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1828,energy efficiency,power,power,1828," LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1935,energy efficiency,power,power,1935,"ra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2112,energy efficiency,model,model,2112,"sentations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2123,energy efficiency,power,powerful,2123,"e propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2226,energy efficiency,model,models,2226,"earning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relati",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2388,energy efficiency,model,models,2388,"for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to inc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2508,energy efficiency,predict,prediction,2508,"ural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn e",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2962,energy efficiency,model,models,2962,"d tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lea",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3050,energy efficiency,model,models,3050,"ific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3467,energy efficiency,power,power,3467,"ent-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3496,energy efficiency,model,models,3496,"citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3733,energy efficiency,model,model,3733,"ssing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3763,energy efficiency,model,model,3763,"ers to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike man",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3908,energy efficiency,model,modeling,3908," been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3949,energy efficiency,model,model,3949," language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4316,energy efficiency,model,models,4316,"document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs su",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4536,energy efficiency,model,model,4536,"ocuments. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4804,energy efficiency,model,model,4804,"ecent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5191,energy efficiency,model,models,5191,"ocuments. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectiv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5247,energy efficiency,model,model,5247,"ment context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document rel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5379,energy efficiency,model,modeling,5379,"resentations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification an",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5521,energy efficiency,optim,optimal,5521,"d language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5750,energy efficiency,model,model,5750,"e. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6056,energy efficiency,model,models,6056," is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6079,energy efficiency,power,powerful,6079,"ndent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6123,energy efficiency,model,models,6123,"nspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the ben",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6309,energy efficiency,power,power,6309," LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6416,energy efficiency,power,power,6416,"ra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6593,energy efficiency,model,model,6593,"sentations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6604,energy efficiency,power,powerful,6604,"e propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribut",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6707,energy efficiency,model,models,6707,"earning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are rel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6869,energy efficiency,model,models,6869,"for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6989,energy efficiency,predict,prediction,6989,"ural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to le",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7443,energy efficiency,model,models,7443,"d tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do n",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7534,energy efficiency,model,models,7534,"c documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7953,energy efficiency,power,power,7953,"evel tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7982,energy efficiency,model,models,7982,"ion prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8219,energy efficiency,model,model,8219," (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8249,energy efficiency,model,model,8249,"o search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8395,energy efficiency,model,modeling,8395,"brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not y",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8436,energy efficiency,model,model,8436,"age models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8804,energy efficiency,model,models,8804,"ment embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9025,energy efficiency,model,model,9025,"ments. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer'])",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9293,energy efficiency,model,model,9293,"nt SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <mo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9705,energy efficiency,load,load,9705,"TER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'sci",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9828,energy efficiency,load,load,9828,"tagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. htt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9953,energy efficiency,load,load,9953,"the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10435,energy efficiency,load,load,10435," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10560,energy efficiency,load,load,10560," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11194,energy efficiency,model,model,11194," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:14,integrability,pipelin,pipeline,14,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:66,integrability,pipelin,pipeline,66,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:150,integrability,pipelin,pipeline,150,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:269,integrability,pipelin,pipeline,269,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:507,integrability,abstract,abstracts,507,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:561,integrability,Abstract,Abstract,561,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:689,integrability,Transform,Transformer,689,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:754,integrability,Transform,Transformer,754,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1554,integrability,Transform,Transformer,1554," Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommend",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2091,integrability,Transform,Transformer,2091,"ocumentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2679,integrability,pub,publication,2679,"ence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual f",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2781,integrability,discover,discover,2781,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scie",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2870,integrability,sub,substantial,2870," such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate pape",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3400,integrability,state,stateof-the-art,3400," SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are eff",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3569,integrability,abstract,abstract,3569,"w that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3746,integrability,state,state-of-the-art,3746,"that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4046,integrability,topic,topic,4046," models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstrac",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4272,integrability,Transform,Transformer,4272," inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4980,integrability,sub,substantially,4980,"ntations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5009,integrability,state,state,5009," for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language proc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5042,integrability,Abstract,Abstract,5042,"topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5170,integrability,Transform,Transformer,5170,"ns of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5235,integrability,Transform,Transformer,5235,"nter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-doc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6035,integrability,Transform,Transformer,6035," Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommend",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6572,integrability,Transform,Transformer,6572,"ocumentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing indi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7160,integrability,pub,publication,7160,"ence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these text",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7262,integrability,discover,discover,7262,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7351,integrability,sub,substantial,7351," such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7886,integrability,state,stateof-the-art,7886,"OCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8055,integrability,abstract,abstract,8055,"t SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8232,integrability,state,state-of-the-art,8232,"help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining object",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8533,integrability,topic,topic,8533,"dels are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sen",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8759,integrability,Transform,Transformer,8759,"ter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9469,integrability,sub,substantially,9469,"ations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9498,integrability,state,state,9498,"or document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:689,interoperability,Transform,Transformer,689,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:754,interoperability,Transform,Transformer,754,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:772,interoperability,architectur,architecture,772,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1407,interoperability,share,share,1407,"e the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce S",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1554,interoperability,Transform,Transformer,1554," Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommend",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2091,interoperability,Transform,Transformer,2091,"ocumentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2304,interoperability,specif,specific,2304,"ing this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to pro",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2781,interoperability,discover,discover,2781,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scie",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3591,interoperability,semant,semantic,3591,"orms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurri",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4272,interoperability,Transform,Transformer,4272," inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4488,interoperability,specif,specific,4488,"age models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4546,interoperability,specif,specifically,4546,"apers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representation",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5170,interoperability,Transform,Transformer,5170,"ns of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5235,interoperability,Transform,Transformer,5235,"nter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-doc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5253,interoperability,architectur,architecture,5253,"ext into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5888,interoperability,share,share,5888,"ers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce S",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6035,interoperability,Transform,Transformer,6035," Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommend",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6572,interoperability,Transform,Transformer,6572,"ocumentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing indi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6785,interoperability,specif,specific,6785,"ing this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7262,interoperability,discover,discover,7262,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8077,interoperability,semant,semantic,8077,"a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8759,interoperability,Transform,Transformer,8759,"ter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger',",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8977,interoperability,specif,specific,8977," models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disab",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9035,interoperability,specif,specifically,9035,"rs title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.ad",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:507,modifiability,abstract,abstracts,507,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:561,modifiability,Abstract,Abstract,561,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2660,modifiability,pac,pace,2660,"towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3179,modifiability,extens,extensions,3179,"raph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of sc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3569,modifiability,abstract,abstract,3569,"w that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5042,modifiability,Abstract,Abstract,5042,"topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7141,modifiability,pac,pace,7141,"towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, si",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7663,modifiability,extens,extensions,7663,"h. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8055,modifiability,abstract,abstract,8055,"t SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10204,modifiability,Modul,ModuleNotFoundError,10204," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10294,modifiability,modul,module,10294," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10666,modifiability,Modul,ModuleNotFoundError,10666," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10690,modifiability,modul,module,10690," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1948,performance,perform,performance,1948,"ext and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3600,performance,content,content,3600,"riety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4794,performance,time,time,4794," like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using thi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6429,performance,perform,performance,6429,"ext and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8086,performance,content,content,8086," of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9283,performance,time,time,9283,"ke the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac54",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9557,performance,error,error,9557,"ommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9705,performance,load,load,9705,"TER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'sci",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9828,performance,load,load,9828,"tagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. htt",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9953,performance,load,load,9953,"the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10115,performance,error,error,10115,"rvision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10435,performance,load,load,10435," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10560,performance,load,load,10560," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11170,performance,error,error,11170," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:434,reliability,Doe,Does,434,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3844,reliability,doe,does,3844,"tical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation inform",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4614,reliability,incid,incidental,4614,"aper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose u",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4810,reliability,doe,does,4810," SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, enc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8331,reliability,doe,does,8331," In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation info",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9103,reliability,incid,incidental,9103,"r, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer')",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9299,reliability,doe,does,9299,"iBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:810,safety,input,input,810,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2508,safety,predict,prediction,2508,"ural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn e",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5291,safety,input,input,5291,"t al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6989,safety,predict,prediction,6989,"ural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to le",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9557,safety,error,error,9557,"ommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10115,safety,error,error,10115,"rvision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10204,safety,Modul,ModuleNotFoundError,10204," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10268,safety,input,input-,10268," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10294,safety,modul,module,10294," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10666,safety,Modul,ModuleNotFoundError,10666," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10690,safety,modul,module,10690," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11170,safety,error,error,11170," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:710,security,model,models,710,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:766,security,model,model,766,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:898,security,model,modeling,898,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1186,security,sign,signal,1186,"f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1223,security,loss,loss,1223,"r spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1269,security,model,model,1269,"eline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1575,security,model,models,1575," is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1642,security,model,models,1642,"nspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the ben",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1670,security,token,token,1670,"s of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scien",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2112,security,model,model,2112,"sentations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2132,security,sign,signal,2132,"e using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2226,security,model,models,2226,"earning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relati",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2388,security,model,models,2388,"for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to inc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2962,security,model,models,2962,"d tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lea",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3050,security,model,models,3050,"ific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3294,security,sign,signals,3294," task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3496,security,model,models,3496,"citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3733,security,model,model,3733,"ssing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3763,security,model,model,3763,"ers to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike man",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3908,security,model,modeling,3908," been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3949,security,model,model,3949," language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4316,security,model,models,4316,"document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs su",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4536,security,model,model,4536,"ocuments. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4637,security,sign,signal,4637," in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4706,security,sign,signal,4706,"lf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4728,security,loss,loss,4728,"ge modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. W",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4804,security,model,model,4804,"ecent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5191,security,model,models,5191,"ocuments. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectiv",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5247,security,model,model,5247,"ment context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document rel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5379,security,model,modeling,5379,"resentations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification an",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5667,security,sign,signal,5667," are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5704,security,loss,loss,5704,"signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5750,security,model,model,5750,"e. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6056,security,model,models,6056," is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show tha",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6123,security,model,models,6123,"nspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the ben",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6151,security,token,token,6151,"s of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scien",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6593,security,model,model,6593,"sentations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6613,security,sign,signal,6613,"e using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 ht",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6707,security,model,models,6707,"earning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are rel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6869,security,model,models,6869,"for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7443,security,model,models,7443,"d tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do n",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7534,security,model,models,7534,"c documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7778,security,sign,signals,7778,"sk-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7982,security,model,models,7982,"ion prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8219,security,model,model,8219," (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8249,security,model,model,8249,"o search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8395,security,model,modeling,8395,"brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not y",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8436,security,model,model,8436,"age models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8804,security,model,models,8804,"ment embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec'",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9025,security,model,model,9025,"ments. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer'])",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9126,security,sign,signal,9126," this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. --",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9195,security,sign,signal,9195," pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. -----------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9217,security,loss,loss,9217,"modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9293,security,model,model,9293,"nt SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <mo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11194,security,model,model,11194," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:950,testability,context,context,950,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2794,testability,understand,understand,2794,"ocument-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3655,testability,simpl,simply,3655,"e pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating whi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4255,testability,context,context,4255,"methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model archit",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5431,testability,context,context,5431,"y of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong perfor",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7275,testability,understand,understand,7275,"ocument-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific te",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8141,testability,simpl,simply,8141,"e of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8742,testability,context,context,8742,"hods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10224,testability,Trace,Traceback,10224," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:356,usability,custom,custom,356,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:585,usability,learn,learn,585,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:810,usability,input,input,810,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:941,usability,document,document,941,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:983,usability,document,document,983,"for scispacy `pipeline` gives . ```. [('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1034,usability,learn,learn,1034,"[('attribute_ruler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level emb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1048,usability,document,document,1048,"ler',. <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scien",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1077,usability,learn,learn,1077,"buteruler.AttributeRuler at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1096,usability,document,documentlevel,1096,"r at 0x7f1a5969e3c0>),. ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1165,usability,document,document,1165,"r.Sentencizer at 0x7f1a59754640>)]. ```. Where as regular spacy gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness:",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1228,usability,learn,learning,1228," gives. ```. [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]. ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right? scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1472,usability,learn,learning,1472," better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-le",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1592,usability,learn,learn,1592,"-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1757,usability,document,document,1757,"rmer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that he",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1798,usability,document,document-level,1798," the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1866,usability,document,documents,1866,"y based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:1948,usability,perform,performance,1948,"ext and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2019,usability,document,document-level,2019,"lity to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2058,usability,document,documents,2058,"entations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are wi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2142,usability,document,document-level,2142,"ons as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2373,usability,document,document-level,2373,"re similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2445,usability,consist,consisting,2445,"o not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2465,usability,document,document-level,2465,"learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2523,usability,document,document,2523,"processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2748,usability,tool,tools,2748,"inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2759,usability,help,help,2759,"ment relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-ar",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2764,usability,user,users,2764,"elatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art mode",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2781,usability,discov,discover,2781,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scie",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:2902,usability,tool,tools,2902,"ecommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The langua",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3199,usability,document,document,3199,"ng pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3285,usability,document,document,3285,"s without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vasw",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3319,usability,document,document,3319,"g. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3506,usability,learn,learn,3506,"rediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3538,usability,document,documents,3538,"ion and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:3655,usability,simpl,simply,3655,"e pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating whi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4005,usability,help,helpful,4005,"l., 2019; Yang et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4017,usability,document,document-level,4017,"et al., 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4131,usability,learn,learning,4131,"://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired b",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4193,usability,document,documents,4193,"-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4246,usability,document,document,4246,"ikewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer mod",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4365,usability,learn,learn,4365,", 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on maske",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4371,usability,document,document,4371," have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked langu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4405,usability,effectiv,effective,4405,"e-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only consid",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4605,usability,document,document,4605,"bout the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4644,usability,indicat,indicating,4644,"work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-doc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:4661,usability,document,documents,4661,"ing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5066,usability,learn,learn,5066," recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like B",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5291,usability,input,input,5291,"t al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5422,usability,document,document,5422,"de-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power stro",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5464,usability,document,document,5464,"e need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SP",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5515,usability,learn,learn,5515,"etrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level emb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5529,usability,document,document,5529,"ge model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scien",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5558,usability,learn,learn,5558,"e citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5577,usability,document,documentlevel,5577,"lly occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5646,usability,document,document,5646,"ting which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness:",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5709,usability,learn,learning,5709,"into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:5953,usability,learn,learning,5953,"CTERs representations substantially outperform the state. ```. en_core_web_sm. ```. Abstract Our goal is to learn task-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-le",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6073,usability,learn,learn,6073,"-independent representations of academic papers. Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6238,usability,document,document,6238,"rmer model architecture as basis of encoding the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that he",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6279,usability,document,document-level,6279," the input paper. Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6347,usability,document,documents,6347,"y based on masked language modeling objective, only considering intra-document context and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6429,usability,perform,performance,6429,"ext and do not use any inter-document information. This limits their ability to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6500,usability,document,document-level,6500,"lity to learn optimal document representations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6539,usability,document,documents,6539,"entations. To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6623,usability,document,document-level,6623,"ons as an inter-document relatedness signal and formulate it as a triplet loss learning objective. We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6854,usability,document,document-level,6854,"re similar for papers that share a citation link than for those that do not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6926,usability,consist,consisting,6926,"o not. Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we stud",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:6946,usability,document,document-level,6946,"learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7004,usability,document,document,7004,"processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7229,usability,tool,tools,7229,"inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7240,usability,help,help,7240,"ment relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7245,usability,user,users,7245,"elatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7262,usability,discov,discover,7262,"limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7383,usability,tool,tools,7383,"ecommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7683,usability,document,document,7683,"pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documen",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7769,usability,document,document,7769,"ithout task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (V",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7803,usability,document,document,7803,"Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language mod",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:7992,usability,learn,learn,7992,"tion, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuni",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8024,usability,document,documents,8024,"nd recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark. As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language mode",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8141,usability,simpl,simply,8141,"e of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical. In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,. 2018; Devlin et al.,. 2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8492,usability,help,helpful,8492,"2019; Yang et al.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8504,usability,document,document-level,8504,"l.,. 2019). While such models are widely used for representing individual words  Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8618,usability,learn,learning,8618,"github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. fr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8680,usability,document,documents,8680,"cument embeddings are relatively underexplored. Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencize",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8733,usability,document,document,8733,"wise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,. 2017; Chen et al.,. 2019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable =",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8854,usability,learn,learn,8854,"019) have yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disabl",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8860,usability,document,document,8860,"ve yet to incorporate stateof-the-art pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['n",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:8894,usability,effectiv,effective,8894,"rt pretrained LMs. Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents. A papers title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatize",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9094,usability,document,document,9094,"t the paper, but, as we show in this work, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sen",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9133,usability,indicat,indicating,9133,"k, simply passing these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. -----------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9150,usability,document,documents,9150," these textual fields to an off-the-shelf pretrained language modeleven a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,. 2019)does not result in accurate paper representations. The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ----------------------------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:9557,usability,error,error,9557,"ommendation. In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents. Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,. 2017) language models (e.g., SciBERT (Beltagy et al.,. 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model. We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10115,usability,error,error,10115,"rvision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:10268,usability,input,input-,10268," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:11170,usability,error,error,11170," triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information. This is critical for embedding new papers that have not yet been cited. In experiments, we show that SPECTERs representations substantially outperform the state. ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```. import spacy. import scispacy. from scispacy.custom_sentence_segmentater import pysbd_sentencizer. nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']). nlpSciMd.add_pipe('pysbd_sentencizer'). nlpSciSm.add_pipe('pysbd_sentencizer'). ```. error. ```. ---------------------------------------------------------------------------. ModuleNotFoundError Traceback (most recent call last). <ipython-input-3-45556ac5415d> in <module>(). 1 import spacy. 2 import scispacy. ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer. 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'. ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:533,availability,error,error,533,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:276,deployability,pipelin,pipeline,276,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:352,deployability,pipelin,pipeline,352,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:418,deployability,pipelin,pipeline,418,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:461,deployability,pipelin,pipeline,461,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:159,energy efficiency,load,load,159,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:276,integrability,pipelin,pipeline,276,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:352,integrability,pipelin,pipeline,352,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:418,integrability,pipelin,pipeline,418,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:461,integrability,pipelin,pipeline,461,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:159,performance,load,load,159,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:533,performance,error,error,533,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:533,safety,error,error,533,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/327:533,usability,error,error,533,"If you disable everything and just add the sentencizer, you should end up with just the sentencizer, whether it is spacy or scispacy. ```. In [8]: nlp = spacy.load('en_core_sci_sm', disable=['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner']). In [9]: nlp.pipeline. Out[9]: []. In [10]: nlp.add_pipe('sentencizer'). Out[10]: <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>. In [11]: nlp.pipeline. Out[11]: [('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7faf4642b640>)]. ```. As for your error, it is just a typo. You need `from scispacy.custom_sentence_segmenter import pysbd_sentencizer`. Sorry about that. Fixed the typo above.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327
https://github.com/allenai/scispacy/issues/328:28,deployability,version,version,28,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:91,deployability,version,version,91,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:28,integrability,version,version,28,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:91,integrability,version,version,91,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:28,modifiability,version,version,28,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:91,modifiability,version,version,91,you probably have the wrong version of spacy. you'll need to make sure that you have spacy version >= 3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:289,availability,error,error,289,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:170,deployability,version,versions,170,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:170,integrability,version,versions,170,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:170,modifiability,version,versions,170,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:289,performance,error,error,289,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:289,safety,error,error,289,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:17,usability,user,user-images,17,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:289,usability,error,error,289,"![image](https://user-images.githubusercontent.com/31191581/110045415-ba06ca80-7cee-11eb-909d-aba40dc4cb64.png). Thanks for the note. I've tried it again with the latest versions of Spacy and Scispacy, in a new notebook and fresh environment, and it's unfortunately still showing the same error.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:44,availability,error,error,44,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:243,availability,error,error,243,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:44,performance,error,error,44,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:243,performance,error,error,243,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:44,safety,error,error,44,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:243,safety,error,error,243,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:44,usability,error,error,44,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:243,usability,error,error,243,"We've had a bunch of people report the same error, and have it go away after using a clean environment (see #318). Could you quadruple check you are reinstalling everything in a totally clean environment? I haven't been able to reproduce this error otherwise, so I don't have another suggestion at the moment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:332,availability,error,error,332,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:123,deployability,log,log,123,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:332,performance,error,error,332,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:123,safety,log,log,123,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:332,safety,error,error,332,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:123,security,log,log,123,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:123,testability,log,log,123,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:272,usability,help,helps,272,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:332,usability,error,error,332,"Hi Daniel, per your advice, I've tried to:. 1. Factory Reset my notebook and run it again. 2. Set up a new Google account, log into a new Colab Notebook and run the same code. 3. Run it on a jupyter notebook. Unfortunately, I'm still running into the similar issue. If it helps; this is the Colab link I was trying to reproduce the error on. https://colab.research.google.com/drive/1QdK7nKD37vnZDNMHi329NDxRfUeNWO6Z?usp=sharing. Johnson",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:413,availability,error,error,413,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:224,deployability,instal,installed,224,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:312,deployability,version,version,312,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:362,deployability,instal,install,362,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:470,deployability,version,version,470,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:312,integrability,version,version,312,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:470,integrability,version,version,470,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:215,modifiability,pac,packages,215,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:312,modifiability,version,version,312,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:462,modifiability,pac,package,462,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:470,modifiability,version,version,470,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:413,performance,error,error,413,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:413,safety,error,error,413,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:413,usability,error,error,413,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:586,usability,help,helps,586,"It appears that your colab environment is not actually a clean environment. I don't use colab much so I'm not too sure why that happens, but if you restart the runtime and just run `!pip list`, there are a lot more packages installed than would exist in a clean python environment. I happened to notice that the version of `blis` in the colab is old, and if you install the latest blis before spacy/scispacy, the error goes away. I'm not sure if there are other package version issues lurking, and would recommend figuring out how to actually create a clean base environment. Hope that helps!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:171,deployability,updat,updates,171,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:194,deployability,updat,update,194,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:116,modifiability,pac,packages,116,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:231,modifiability,pac,packages,231,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:160,reliability,doe,doesn,160,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:171,safety,updat,updates,171,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:194,safety,updat,update,194,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:171,security,updat,updates,171,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:194,security,updat,update,194,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:24,usability,help,help,24,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:50,usability,help,helpful,50,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/328:352,usability,help,help,352,Thanks so much for your help. This was incredibly helpful. . I get your point now. I think Google Colab preinstalls packages that runs well with each other and doesn't do updates unless the new update is stable with all it's other packages. It's unfortunate as a large community of folks must be on colab pulling their hairs out. Thanks again for your help; I'll see how the rest of the notebook goes.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/328
https://github.com/allenai/scispacy/issues/329:67,usability,close,close,67,"I think I've answered on the other issues you commented on, so ill close this one",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/329
https://github.com/allenai/scispacy/issues/331:718,deployability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,integrability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,interoperability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,modifiability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,reliability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:447,safety,input,input,447,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:675,safety,test,tested,675,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,security,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:675,testability,test,tested,675,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:718,testability,integr,integrated,718,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:447,usability,input,input,447,"Hi @BlakeList,. Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. https://github.com/allenai/scispacy/issues/237. You can use this script: . https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:42,usability,close,close,42,Great! Thank you @DeNeutoy . Feel free to close ,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:29,availability,slo,slow,29,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:65,availability,down,down,65,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:49,energy efficiency,current,currently,49,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:145,energy efficiency,model,model,145,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:85,integrability,rout,route,85,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:270,interoperability,ontolog,ontology,270,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:313,interoperability,format,format,313,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:332,interoperability,standard,standard,332,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:369,interoperability,ontolog,ontology,369,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:396,interoperability,ontolog,ontology,396,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:29,reliability,slo,slow,29,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:145,security,model,model,145,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:252,testability,understand,understanding,252,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:434,testability,plan,plant,434,"Hi @DeNeutoy,. Sorry for the slow response, I am currently going down an alternative route. I have instead built an entity-ruler (and now an ner model using prodigy + spacy). The code above is quite straightforward, however, I was having some troubles understanding how ontology classes can be used with the umls format. Is there a standard approach to convert between ontology rdfs to umls? Can ontology classes (e.g. TO:0000387 for plant trait) be used as the concept id? Cheers,. Blake",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,deployability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:937,deployability,resourc,resources,937,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:937,energy efficiency,resourc,resources,937,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,integrability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:891,integrability,compon,component,891,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,interoperability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:891,interoperability,compon,component,891,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:915,interoperability,ontolog,ontology,915,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,modifiability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:891,modifiability,compon,component,891,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:937,performance,resourc,resources,937,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,reliability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:429,safety,input,input,429,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:669,safety,test,tested,669,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:937,safety,resourc,resources,937,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,security,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:669,testability,test,tested,669,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:712,testability,integr,integrated,712,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:937,testability,resourc,resources,937,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:429,usability,input,input,429,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/331:908,usability,custom,custom,908,"> Hi @BlakeList,. > . > Creating your own entity linker is quite straightforward - there is one fiddly bit in how the linkers are registered with scispacy at the moment which is less than ideal, but you should be able to follow the instructions in this issue:. > . > #237. > . > You can use this script: https://github.com/allenai/scispacy/blob/master/scripts/create_linker.py. > . > to create the files for the linker. The only input you need is a json/jsonl file with objects which look like this class:. > . > https://github.com/allenai/scispacy/blob/4ade4ec897fa48c2ecf3187caa08a949920d126d/scispacy/linking_utils.py#L12. > . > Once you've generated the linker and tested it out, we can see about getting it integrated into scispacy, if you think it would be useful! Hi @DeNeutoy , . I'm a bit confused! Are you able to provide a sample codebase that was used to create an entity linker component with a custom ontology and relevant resources?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/331
https://github.com/allenai/scispacy/issues/332:213,deployability,version,version,213,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:234,deployability,instal,installed,234,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:75,energy efficiency,load,load,75,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:134,energy efficiency,cool,cool,134,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:174,energy efficiency,cool,cool,174,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:213,integrability,version,version,213,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:213,modifiability,version,version,213,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:75,performance,load,load,75,"I am not able to reproduce. ```. In [1]: import spacy. In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). In [3]: doc = nlp(""Genes are cool.""). In [4]: doc. Out[4]: Genes are cool. ```. Probably you have the wrong version of something installed. Please try again in a clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:251,deployability,version,version,251,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:272,deployability,instal,installed,272,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:330,deployability,version,version,330,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:356,deployability,version,version,356,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:424,deployability,version,versions,424,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:91,energy efficiency,load,load,91,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:156,energy efficiency,cool,cool,156,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:204,energy efficiency,cool,cool,204,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:251,integrability,version,version,251,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:330,integrability,version,version,330,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:356,integrability,version,version,356,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:424,integrability,version,versions,424,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:251,modifiability,version,version,251,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:330,modifiability,version,version,330,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:356,modifiability,version,version,356,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:416,modifiability,pac,package,416,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:424,modifiability,version,versions,424,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:91,performance,load,load,91,"> I am not able to reproduce. > . > ```. > In [1]: import spacy. > . > In [2]: nlp = spacy.load('en_ner_bionlp13cg_md'). > . > In [3]: doc = nlp(""Genes are cool.""). > . > In [4]: doc. > Out[4]: Genes are cool. > ```. > . > Probably you have the wrong version of something installed. Please try again in a clean environment. spacy version - 3.0.5. scispacy version - 0.4.0. en_ner_bionlp13cg_md- 0.4.0. These are the package versions",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:36,deployability,version,versions,36,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:137,deployability,depend,dependency,137,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:36,integrability,version,versions,36,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:137,integrability,depend,dependency,137,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:124,interoperability,incompatib,incompatible,124,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:28,modifiability,pac,package,28,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:36,modifiability,version,versions,36,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:137,modifiability,depend,dependency,137,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:137,safety,depend,dependency,137,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:2,testability,understand,understand,2,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:137,testability,depend,dependency,137,"I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:38,deployability,version,versions,38,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:139,deployability,depend,dependency,139,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:38,integrability,version,versions,38,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:139,integrability,depend,dependency,139,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:126,interoperability,incompatib,incompatible,126,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:30,modifiability,pac,package,30,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:38,modifiability,version,versions,38,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:139,modifiability,depend,dependency,139,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:139,safety,depend,dependency,139,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:4,testability,understand,understand,4,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:139,testability,depend,dependency,139,"> I understand, those are the package versions in my environment as well. I am saying that you probably have some out of date/incompatible dependency in that environment. Please try again in a new, clean environment. Okay sure",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:30,energy efficiency,load,loading,30,Check if it helps. It's about loading bionlp package. https://github.com/explosion/spaCy/discussions/7390,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:45,modifiability,pac,package,45,Check if it helps. It's about loading bionlp package. https://github.com/explosion/spaCy/discussions/7390,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:30,performance,load,loading,30,Check if it helps. It's about loading bionlp package. https://github.com/explosion/spaCy/discussions/7390,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:12,usability,help,helps,12,Check if it helps. It's about loading bionlp package. https://github.com/explosion/spaCy/discussions/7390,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:69,deployability,version,version,69,Thank you!!! The issue is resolved. There was the problem of package version,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:69,integrability,version,version,69,Thank you!!! The issue is resolved. There was the problem of package version,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:61,modifiability,pac,package,61,Thank you!!! The issue is resolved. There was the problem of package version,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/332:69,modifiability,version,version,69,Thank you!!! The issue is resolved. There was the problem of package version,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/332
https://github.com/allenai/scispacy/issues/333:21,deployability,instal,install,21,wget followed by pip install works.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/333
https://github.com/allenai/scispacy/issues/334:73,availability,Error,Error,73,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:547,availability,Avail,Available,547,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:60,deployability,Instal,Installation,60,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:349,deployability,instal,install,349,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:281,energy efficiency,current,current,281,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1004,energy efficiency,load,load,1004,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:237,integrability,compon,component,237,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:323,integrability,Transform,Transformer,323,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:364,integrability,transform,transformers,364,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:404,integrability,compon,component,404,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:463,integrability,compon,component,463,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:488,integrability,compon,components,488,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:534,integrability,compon,components,534,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:237,interoperability,compon,component,237,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:323,interoperability,Transform,Transformer,323,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:364,interoperability,transform,transformers,364,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:404,interoperability,compon,component,404,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:463,interoperability,compon,component,463,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:488,interoperability,compon,components,488,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:534,interoperability,compon,components,534,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1294,interoperability,Platform,Platform,1294,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:237,modifiability,compon,component,237,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:404,modifiability,compon,component,404,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:442,modifiability,deco,decorator,442,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:463,modifiability,compon,component,463,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:488,modifiability,compon,components,488,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:534,modifiability,compon,components,534,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:73,performance,Error,Error,73,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1004,performance,load,load,1004,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:547,reliability,Availab,Available,547,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:73,safety,Error,Error,73,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:547,safety,Avail,Available,547,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:547,security,Availab,Available,547,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:73,usability,Error,Error,73,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:230,usability,custom,custom,230,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:397,usability,custom,custom,397,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:1279,usability,help,help,1279,"I made a new Env, I am still facing this same issue, NMSLIB Installation Error. ValueError: [E002] Can't find factory for 'scispacy_linker' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components). Available factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, textcat_multilabel, abbreviation_detector, en.lemmatizer. Code . ```. import spacy. import scispacy. from scispacy.linking import EntityLinker. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). nlp.add_pipe(""abbreviation_detector""). nlp.add_pipe(""scispacy_linker"", config={""resolve_abbreviations"": True, ""linker_name"": ""umls""}). doc = nlp(""Adenocarcinoma of GE junction \. Locally advanced \. Neoadjuvant Chemotherapy""). print(doc). ```. Please help. Thanks . Platform : Windows 11 x64",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:37,deployability,fail,fails,37,"Same issue as above` nlp.add_pipe()` fails to find ""scispacy_linker"". Any easy workarounds?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/334:37,reliability,fail,fails,37,"Same issue as above` nlp.add_pipe()` fails to find ""scispacy_linker"". Any easy workarounds?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/334
https://github.com/allenai/scispacy/issues/335:402,interoperability,specif,specific,402,"I think its just because your initialize section asks for those things, have you tried just setting those things to null? ```. [initialize]. vectors = ${paths.vectors}. init_tok2vec = ${paths.init_tok2vec}. vocab_data = ${paths.vocab_path}. lookups = null. before_init = null. after_init = null. ```. Also, this might end up being a better question for the spacy folks as there isn't much here that is specific to scispacy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:55,availability,error,error,55,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:93,availability,error,error,93,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:536,energy efficiency,model,models,536,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:61,integrability,messag,message,61,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:61,interoperability,messag,message,61,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:55,performance,error,error,55,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:93,performance,error,error,93,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:55,safety,error,error,55,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:82,safety,valid,validation,82,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:93,safety,error,error,93,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:82,security,validat,validation,82,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:536,security,model,models,536,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:55,usability,error,error,55,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:93,usability,error,error,93,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:610,usability,close,close,610,"Well if I remove all those things, I got the following error message. ```. Config validation error. vocab_data field required. lookups field requiredthi. vectors field required. init_tok2vec field required. before_init field required. after_init field required. ```. And if I set everything to null, I end up with :. ```. TypeError: [E930] Received invalid get_examples callback in `Tok2Vec.initialize`. Expected function that returns an iterable of Example objects but got: []. ```. I asked here because it has to do with transferring models and you did that at scispacy, but please tell me if you want me to close the issue and ask to spacy folks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:327,availability,error,errors,327,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:352,availability,error,errors,352,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:403,deployability,pipelin,pipeline,403,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:427,deployability,automat,automatically,427,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:403,integrability,pipelin,pipeline,403,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:217,interoperability,format,format,217,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:327,performance,error,errors,327,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:352,performance,error,errors,352,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:525,performance,time,time,525,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:146,safety,compl,completely,146,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:327,safety,error,errors,327,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:352,safety,error,errors,352,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:146,security,compl,completely,146,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:427,testability,automat,automatically,427,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:15,usability,close,close,15,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:81,usability,command,command,81,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:327,usability,error,errors,327,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:352,usability,error,errors,352,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:506,usability,help,help,506,"Well, you were close ! It was not a path problem, as I set train/dev paths using command line arguments. But it made me notice that spacy convert completely messed up when I tried converting my data from the old JSON format to binary files... I tried again using a few lines of python and now spacy debug runs without throwing errors (apart from a few errors related to data themselves). So I guess the pipeline could not init automatically because it was fed with empty data. Thank you very much for your help. I don't have time to go any further in the training process right now, but I hope everything goes well from now on.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:30,usability,help,help,30,"Ah, makes sense! Glad I could help, I'll close this issue, hopefully the setup in the repo is helpful for you to train your own, feel free to open issues to ask questions about it, with the knowledge that I might have to defer to the spacy folks at some point :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:41,usability,close,close,41,"Ah, makes sense! Glad I could help, I'll close this issue, hopefully the setup in the repo is helpful for you to train your own, feel free to open issues to ask questions about it, with the knowledge that I might have to defer to the spacy folks at some point :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/335:94,usability,help,helpful,94,"Ah, makes sense! Glad I could help, I'll close this issue, hopefully the setup in the repo is helpful for you to train your own, feel free to open issues to ask questions about it, with the knowledge that I might have to defer to the spacy folks at some point :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/335
https://github.com/allenai/scispacy/issues/336:7,deployability,version,version,7,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:54,deployability,releas,release,54,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
https://github.com/allenai/scispacy/issues/336:62,deployability,version,version,62,"1) The version on the demo is probably not the latest release version. I should check and update that. 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context. ```. In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:. ...: text = f""The drug {drug_name} is used to treat the virus"". ...: doc = nlp(text). ...: print(doc.ents). ...: . (mesna,). (). (mebane,). (). (). (aspirin,). (). (). (scopolamine,). (entamine,). (valimine,). (henirin,). (). (). ```. Looks like it is also sensitive to capitalization. ```. In [56]: doc = nlp(""Remdesivir is a chemical""). In [57]: doc.ents. Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents. Out[59]: (). ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines). 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336
