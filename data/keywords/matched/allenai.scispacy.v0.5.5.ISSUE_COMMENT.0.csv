id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/allenai/scispacy/pull/16:115,energy efficiency,optim,optimal,115,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:111,integrability,sub,sub-optimal,111,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:179,integrability,abstract,abstracts,179,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:81,interoperability,format,format,81,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:179,modifiability,abstract,abstracts,179,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:129,usability,document,documents,129,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:160,usability,document,documents,160,"Oh, having seen the script in the history which you used to convert to the spacy format, I still think this is sub-optimal - the documents should be the actual documents from the abstracts, not just 10 random sentences concatenated together.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:124,energy efficiency,model,models,124,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:186,integrability,abstract,abstracts,186,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:325,integrability,abstract,abstracts,325,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:186,modifiability,abstract,abstracts,186,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:325,modifiability,abstract,abstracts,325,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:124,security,model,models,124,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:409,security,Team,TeamCity,409,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:470,security,access,access,470,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:510,security,password,password,510,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:570,security,access,access,570,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:286,usability,command,command,286,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/16:497,usability,user,username,497,"@DeNeutoy agree that the difference is likely due to not having pretrained vectors. and agree that it would be good for the models to be in s3 rather than the repo. As for stitching the abstracts together, I had most of this code written and then realized i could use the spacy convert command instead if i didn't stitch the abstracts together. let me see if i saved that code somewhere... As for the CI, its TeamCity on the S2 account, not totally sure how to give you access to that. there is a username and password in lastpass in the s2 folder if you happen to have access to that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/16
https://github.com/allenai/scispacy/pull/17:32,reliability,pra,practices,32,"LGTM, and thanks for the better practices examples for pytest :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/17
https://github.com/allenai/scispacy/pull/18:147,availability,down,down,147,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:170,deployability,manag,managable,170,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:170,energy efficiency,manag,managable,170,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:256,energy efficiency,reduc,reduce,256,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:87,interoperability,Semant,Semantic,87,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:42,safety,review,review,42,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:170,safety,manag,managable,170,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/18:42,testability,review,review,42,"@danielkingai2 This is actually ready for review - I have some code which reads in the Semantic Type tree, which will mean I can flatten the types down to something more managable (e.g we could try only having a hierarchy of depth X, which would massively reduce the number of types and maybe make the numbers a bit better). I'll add that separately though because it's basically separate.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/18
https://github.com/allenai/scispacy/pull/20:151,energy efficiency,model,model,151,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:168,energy efficiency,load,loading,168,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:168,performance,load,loading,168,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:246,safety,Test,Test,246,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:385,safety,Test,Test,385,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:151,security,model,model,151,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:335,security,Token,Token,335,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:476,security,Token,Token,476,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:246,testability,Test,Test,246,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:385,testability,Test,Test,385,"I just copied over your dump to spacy code, which works great! It turns out the thing making the results worse was that iw as starting from a blank en model instead of loading the language class. the new results are:. Retrained genia evaluation. Test results:. UAS: 89.45786382624513. LAS: 87.83931681901562. Tag %: 98.67606412382531. Token acc: 100.0. Retrained ontonotes evaluation. Test results:. UAS: 46.438321905674925. LAS: 35.115040743596694. Tag %: 76.52387845043262. Token acc: 99.95606651991342",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:545,availability,robust,robust,545,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:314,energy efficiency,model,model,314,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:244,integrability,inject,injection,244,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:149,performance,perform,performance,149,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:220,performance,perform,performance,220,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:399,performance,time,time,399,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:487,performance,perform,performance,487,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:545,reliability,robust,robust,545,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:31,safety,reme,remember,31,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:545,safety,robust,robust,545,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:190,security,token,tokenise,190,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:244,security,inject,injection,244,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:314,security,model,model,314,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:149,usability,perform,performance,149,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:220,usability,perform,performance,220,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:273,usability,help,help,273,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/20:487,usability,perform,performance,487,"Sweet, nice one. FYI we should remember to explore the `gold_preproc=True/False` option, because although it looks like when you are training it the performance is way worse (because if you tokenise stuff wrong it hurts performance), the noise injection thing might really help the sentence splitting. I trained a model with this option just to see what happened and with gold preprocessing at eval time, it got 88.6 UAS and 86.8 LAS, but it's possible it's worth just taking this small performance hit if the sentence segmentation is much more robust. Do you have examples of hard sentence segmentation from papers etc?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/20
https://github.com/allenai/scispacy/pull/22:0,usability,close,closed,0,closed in favour of #20,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/22
https://github.com/allenai/scispacy/pull/29:63,deployability,build,build,63,@danielkingai2 do you have an idea why this Docker image can't build in CI? I can build it locally ...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:82,deployability,build,build,82,@danielkingai2 do you have an idea why this Docker image can't build in CI? I can build it locally ...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:19,deployability,instal,install,19,@danielkingai2 the install works now 👍 .,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:99,availability,sli,slightly,99,@kyleclo @acohan we are lowercasing scispacy and the installation instructions are going to change slightly as result. You should just need to change the SciSpaCy in the model path to scispacy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:53,deployability,instal,installation,53,@kyleclo @acohan we are lowercasing scispacy and the installation instructions are going to change slightly as result. You should just need to change the SciSpaCy in the model path to scispacy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:170,energy efficiency,model,model,170,@kyleclo @acohan we are lowercasing scispacy and the installation instructions are going to change slightly as result. You should just need to change the SciSpaCy in the model path to scispacy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:99,reliability,sli,slightly,99,@kyleclo @acohan we are lowercasing scispacy and the installation instructions are going to change slightly as result. You should just need to change the SciSpaCy in the model path to scispacy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:170,security,model,model,170,@kyleclo @acohan we are lowercasing scispacy and the installation instructions are going to change slightly as result. You should just need to change the SciSpaCy in the model path to scispacy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:46,deployability,version,version,46,"@DeNeutoy ah my bad, was looking at the wrong version",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:46,integrability,version,version,46,"@DeNeutoy ah my bad, was looking at the wrong version",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/29:46,modifiability,version,version,46,"@DeNeutoy ah my bad, was looking at the wrong version",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/29
https://github.com/allenai/scispacy/pull/37:50,safety,compl,complicated,50,"I wanted to get several eyes on this because it's complicated (perhaps unnecessarily). However, it is very fast - 2x faster than the Spacy tokenizer which is written in Cython. I'm pretty pleased with that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:50,security,compl,complicated,50,"I wanted to get several eyes on this because it's complicated (perhaps unnecessarily). However, it is very fast - 2x faster than the Spacy tokenizer which is written in Cython. I'm pretty pleased with that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:139,security,token,tokenizer,139,"I wanted to get several eyes on this because it's complicated (perhaps unnecessarily). However, it is very fast - 2x faster than the Spacy tokenizer which is written in Cython. I'm pretty pleased with that!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:48,performance,perform,performed,48,"Could you elaborate on how the speed tests were performed? If it was just running via `pytest` by hand, I'd be worried about startup costs dominating.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:37,safety,test,tests,37,"Could you elaborate on how the speed tests were performed? If it was just running via `pytest` by hand, I'd be worried about startup costs dominating.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:37,testability,test,tests,37,"Could you elaborate on how the speed tests were performed? If it was just running via `pytest` by hand, I'd be worried about startup costs dominating.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:48,usability,perform,performed,48,"Could you elaborate on how the speed tests were performed? If it was just running via `pytest` by hand, I'd be worried about startup costs dominating.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:256,energy efficiency,load,load,256,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:56,integrability,pub,pubmed,56,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:63,integrability,abstract,abstracts,63,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:510,integrability,abstract,abstracts,510,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:63,modifiability,abstract,abstracts,63,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:510,modifiability,abstract,abstracts,510,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:86,performance,time,time,86,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:256,performance,load,load,256,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:683,performance,time,times,683,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:775,performance,time,time,775,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:780,performance,time,time,780,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:819,performance,time,time,819,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:824,performance,time,time,824,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:832,performance,time,times,832,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:889,performance,time,times,889,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:914,performance,time,times,914,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:10,safety,test,tests,10,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:709,safety,test,test,709,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:41,security,token,tokenizing,41,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:388,security,token,tokenizers,388,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:473,security,token,tokenizer,473,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:590,security,token,tokenizer,590,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:622,security,token,tokenizer,622,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:649,security,token,tokenizer,649,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:662,security,token,tokenizers,662,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:802,security,token,tokenizer,802,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:10,testability,test,tests,10,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:709,testability,test,test,709,"The speed tests are the following script tokenizing 10k pubmed abstracts. ```. import time. import numpy. import spacy. from scispacy.genia_tokenizer import GeniaTokenizer. from scispacy.custom_tokenizer import combined_rule_tokenizer. spacy_model = spacy.load(""en_core_web_sm""). genia_tokenizer = GeniaTokenizer(spacy_model.vocab). rule_tokenizer = combined_rule_tokenizer(spacy_model). tokenizers = {""genia"": genia_tokenizer, ""rule"": rule_tokenizer, ""spacy"": spacy_model.tokenizer}. data_path = ""baby_corpus/abstracts.shard.aa"". data = open(data_path, ""r"").readlines(). def tokenize_text(tokenizer):. for line in data:. tokenizer(line). for name, tokenizer in tokenizers.items():. times = []. print(f""Speed test: {name}""). for i in range(3):. print(f""iteration {i}""). t1 = time.time(). tokenize_text(tokenizer). t2 = time.time(). times.append(t2 - t1). print(f""Mean (3 runs) {numpy.mean(times)}, Std: {numpy.std(times)}.""). print(). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:209,availability,slo,slower,209,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:94,deployability,fail,fails,94,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:94,reliability,fail,fails,94,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:209,reliability,slo,slower,209,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:73,safety,test,tests,73,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:84,security,token,tokenizer,84,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:126,security,token,tokenizer,126,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:73,testability,test,tests,73,"I think it would be good to at least know how many of the original spacy tests this tokenizer fails. I imagine that the spacy tokenizer's long set of special strings/patterns that it handles are what makes it slower, although not sure about that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:17,deployability,fail,fails,17,Hmm I think this fails too many of the spacy tests to be useful. I should have done that earlier...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:17,reliability,fail,fails,17,Hmm I think this fails too many of the spacy tests to be useful. I should have done that earlier...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:45,safety,test,tests,45,Hmm I think this fails too many of the spacy tests to be useful. I should have done that earlier...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/pull/37:45,testability,test,tests,45,Hmm I think this fails too many of the spacy tests to be useful. I should have done that earlier...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/37
https://github.com/allenai/scispacy/issues/40:0,usability,close,closed,0,closed by #47,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/40
https://github.com/allenai/scispacy/pull/56:90,deployability,fail,fail,90,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:150,deployability,version,version,150,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:70,energy efficiency,model,models,70,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:108,energy efficiency,model,models,108,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:150,integrability,version,version,150,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:128,interoperability,incompatib,incompatible,128,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:150,modifiability,version,version,150,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:90,reliability,fail,fail,90,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:39,safety,test,tests,39,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:70,security,model,models,70,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:108,security,model,models,108,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:39,testability,test,tests,39,"Oh, this is going to be annoying - any tests which use the checked in models are going to fail, because the models are backward incompatible for this version. Ugh.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:52,energy efficiency,model,model,52,"I think given that we don't actually _use_ the full model (you're only using the tokenizer one, right?) we can just delete it and the test which uses it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:134,safety,test,test,134,"I think given that we don't actually _use_ the full model (you're only using the tokenizer one, right?) we can just delete it and the test which uses it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:52,security,model,model,52,"I think given that we don't actually _use_ the full model (you're only using the tokenizer one, right?) we can just delete it and the test which uses it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:81,security,token,tokenizer,81,"I think given that we don't actually _use_ the full model (you're only using the tokenizer one, right?) we can just delete it and the test which uses it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:134,testability,test,test,134,"I think given that we don't actually _use_ the full model (you're only using the tokenizer one, right?) we can just delete it and the test which uses it.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:118,safety,test,tests,118,"yeah, i was just trying to figure out the best way to handle this. I'm going to see if I can just delete them and the tests that require them",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:118,testability,test,tests,118,"yeah, i was just trying to figure out the best way to handle this. I'm going to see if I can just delete them and the tests that require them",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:63,energy efficiency,model,model,63,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:22,safety,test,tests,22,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:192,safety,test,tests,192,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:63,security,model,model,63,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:22,testability,test,tests,22,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:192,testability,test,tests,192,"There were a bunch of tests that were actually using the whole model, it seemed easier for now to just replace the combined_all_model with the new one, and then figure out what to do with the tests later",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:118,deployability,releas,release,118,"sure sounds good - I really want to remove all those models, it's the first thing that i'll do once we have an actual release that we can install in scigraph",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:138,deployability,instal,install,138,"sure sounds good - I really want to remove all those models, it's the first thing that i'll do once we have an actual release that we can install in scigraph",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:53,energy efficiency,model,models,53,"sure sounds good - I really want to remove all those models, it's the first thing that i'll do once we have an actual release that we can install in scigraph",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/pull/56:53,security,model,models,53,"sure sounds good - I really want to remove all those models, it's the first thing that i'll do once we have an actual release that we can install in scigraph",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/56
https://github.com/allenai/scispacy/issues/57:54,energy efficiency,load,load,54,"Does it mean that, whereever there is usage of `spacy.load(""en_core_web_sm"")` it needs to be replaced by generic singleton (dict) via `conftest.get_spacy_model()` ? [@danielkingai2 @DeNeutoy ]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:54,performance,load,load,54,"Does it mean that, whereever there is usage of `spacy.load(""en_core_web_sm"")` it needs to be replaced by generic singleton (dict) via `conftest.get_spacy_model()` ? [@danielkingai2 @DeNeutoy ]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:0,reliability,Doe,Does,0,"Does it mean that, whereever there is usage of `spacy.load(""en_core_web_sm"")` it needs to be replaced by generic singleton (dict) via `conftest.get_spacy_model()` ? [@danielkingai2 @DeNeutoy ]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:42,energy efficiency,model,model,42,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:119,energy efficiency,load,load,119,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:126,energy efficiency,model,model,126,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:48,performance,cach,cache,48,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:119,performance,load,load,119,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:113,safety,test,tests,113,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:225,safety,test,tests,225,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:42,security,model,model,42,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:126,security,model,model,126,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:113,testability,test,tests,113,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/issues/57:225,testability,test,tests,225,"I think the idea was to not use the spacy model cache in conftest at all, but instead have each group of related tests load a model like this: https://github.com/allenai/scispacy/blob/688ac1644178f592c946ba3a9a3ac9e7059b7c48/tests/test_abbreviation_detection.py#L15",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/57
https://github.com/allenai/scispacy/pull/58:44,energy efficiency,model,models,44,I'm just a bit wary of adding things to the models which 90% of people won't need. I guess it's ok as it doesn't really do too much magical stuff.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/58
https://github.com/allenai/scispacy/pull/58:105,reliability,doe,doesn,105,I'm just a bit wary of adding things to the models which 90% of people won't need. I guess it's ok as it doesn't really do too much magical stuff.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/58
https://github.com/allenai/scispacy/pull/58:44,security,model,models,44,I'm just a bit wary of adding things to the models which 90% of people won't need. I guess it's ok as it doesn't really do too much magical stuff.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/58
https://github.com/allenai/scispacy/issues/66:440,deployability,depend,dependency,440,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:440,integrability,depend,dependency,440,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:440,modifiability,depend,dependency,440,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:86,safety,detect,detection,86,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:140,safety,Detect,Detection,140,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:237,safety,detect,detection,237,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:440,safety,depend,dependency,440,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:501,safety,detect,detection,501,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:58,security,team,team,58,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:86,security,detect,detection,86,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:140,security,Detect,Detection,140,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:237,security,detect,detection,237,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:501,security,detect,detection,501,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:440,testability,depend,dependency,440,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:480,testability,plan,planned,480,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:535,testability,plan,planning,535,"@DeNeutoy . In the Conclusion section of your paper, your team has mentioned negation detection as a future work. Has the work on ""Negation Detection"" feature started? As per my knowledge, these are the following algorithms for negation detection in clinical notes:. - MetaMap/ MetaMapLite uses variation of [NegEx](https://metamap.nlm.nih.gov/Docs/MM_2013_ReleaseNotes.pdf). - [NegBio](https://github.com/ncbi-nlp/NegBio) uses patterns in dependency parsing. Is anything similar planned for negation detection in scispacy? Or are you planning a different approach?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:43,deployability,resourc,resource,43,@kaushikacharya @dakinggg I think the best resource for that would be medspacy: https://github.com/medspacy/medspacy. It's a step-up from just NegEx,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:43,energy efficiency,resourc,resource,43,@kaushikacharya @dakinggg I think the best resource for that would be medspacy: https://github.com/medspacy/medspacy. It's a step-up from just NegEx,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:43,performance,resourc,resource,43,@kaushikacharya @dakinggg I think the best resource for that would be medspacy: https://github.com/medspacy/medspacy. It's a step-up from just NegEx,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:43,safety,resourc,resource,43,@kaushikacharya @dakinggg I think the best resource for that would be medspacy: https://github.com/medspacy/medspacy. It's a step-up from just NegEx,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/issues/66:43,testability,resourc,resource,43,@kaushikacharya @dakinggg I think the best resource for that would be medspacy: https://github.com/medspacy/medspacy. It's a step-up from just NegEx,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/66
https://github.com/allenai/scispacy/pull/67:72,safety,test,tests,72,"All comments have been addressed, @DeNeutoy. . Thanks for fixing the CI tests @danielkingai2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/67
https://github.com/allenai/scispacy/pull/67:72,testability,test,tests,72,"All comments have been addressed, @DeNeutoy. . Thanks for fixing the CI tests @danielkingai2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/67
https://github.com/allenai/scispacy/issues/68:57,deployability,releas,released,57,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:121,deployability,releas,release,121,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:224,deployability,releas,release,224,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:259,deployability,releas,release,259,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:333,energy efficiency,model,models,333,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:203,integrability,repositor,repository,203,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:235,integrability,pub,publicly,235,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:203,interoperability,repositor,repository,203,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:333,security,model,models,333,"Hi! Thanks for your interest in the project. We have not released scispacy yet, because we are waiting for the spacy 2.1 release, which should be in a few weeks. I would not recommend trying to use this repository before we release it publicly. Hopefully the release will happen in < 2 weeks and will be accompanied by a full set of models and a technical report. Watch this space!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:78,deployability,releas,release,78,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:95,deployability,version,version,95,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:110,energy efficiency,model,models,110,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:95,integrability,version,version,95,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:95,modifiability,version,version,95,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:62,security,access,access,62,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:110,security,model,models,110,@DeNeutoy thanks for your reply. Is it at all possible to get access to a pre release or alpha version of the models? This project is really amazing and looks promising!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:63,deployability,releas,release,63,@Abhijit-2592 Thanks for your patience! The models and initial release are ready. . https://allenai.github.io/scispacy/. ```. pip install scispacy. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:130,deployability,instal,install,130,@Abhijit-2592 Thanks for your patience! The models and initial release are ready. . https://allenai.github.io/scispacy/. ```. pip install scispacy. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:44,energy efficiency,model,models,44,@Abhijit-2592 Thanks for your patience! The models and initial release are ready. . https://allenai.github.io/scispacy/. ```. pip install scispacy. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/68:44,security,model,models,44,@Abhijit-2592 Thanks for your patience! The models and initial release are ready. . https://allenai.github.io/scispacy/. ```. pip install scispacy. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/68
https://github.com/allenai/scispacy/issues/73:33,deployability,roll,roll,33,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:53,deployability,version,version,53,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:107,deployability,releas,release,107,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:251,deployability,releas,release,251,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:328,deployability,releas,releasing,328,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:363,deployability,releas,released,363,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:155,energy efficiency,model,models,155,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:338,energy efficiency,model,models,338,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:53,integrability,version,version,53,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:53,modifiability,version,version,53,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:232,modifiability,concern,concerning,232,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:155,security,model,models,155,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:338,security,model,models,338,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:232,testability,concern,concerning,232,"Hi! Yes, unfortunately we had to roll back the spacy version, because there was a bug in the spacy nightly release which catastrophically affected the NER models (which hopefully has now been fixed, but I noticed some github issues concerning a later release that made me think there might still be issues). We'll definitely be releasing models once spacy 2.1 is released.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:32,deployability,releas,released,32,"@DeNeutoy now that spaCy 2.1 is released, could you please provide support for the same?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:67,usability,support,support,67,"@DeNeutoy now that spaCy 2.1 is released, could you please provide support for the same?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:61,deployability,releas,release,61,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:82,deployability,version,version,82,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:107,energy efficiency,model,models,107,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:82,integrability,version,version,82,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:71,interoperability,compatib,compatible,71,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:82,modifiability,version,version,82,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:107,security,model,models,107,"@DeNeutoy, thanks for your great work! When do you expect to release a compatible version for the sciSpacy models with spaCy 2.1?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:28,deployability,releas,release,28,@dkarmon We will have a new release within 2 weeks which will support spacy 2.1. There will also be a follow-up release with a (rudimentary) entity linker which will link to a subset of UMLS.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:112,deployability,releas,release,112,@dkarmon We will have a new release within 2 weeks which will support spacy 2.1. There will also be a follow-up release with a (rudimentary) entity linker which will link to a subset of UMLS.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:176,integrability,sub,subset,176,@dkarmon We will have a new release within 2 weeks which will support spacy 2.1. There will also be a follow-up release with a (rudimentary) entity linker which will link to a subset of UMLS.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:62,usability,support,support,62,@dkarmon We will have a new release within 2 weeks which will support spacy 2.1. There will also be a follow-up release with a (rudimentary) entity linker which will link to a subset of UMLS.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:0,deployability,Version,Version,0,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:54,deployability,version,version,54,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:0,integrability,Version,Version,0,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:54,integrability,version,version,54,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:0,modifiability,Version,Version,0,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:54,modifiability,version,version,54,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/73:45,usability,support,supports,45,"Version `0.2.0` of `scispacy` is now up, and supports version `2.1.3` of `spacy`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/73
https://github.com/allenai/scispacy/issues/77:11,deployability,updat,update,11,"Done, will update paper shortly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/77
https://github.com/allenai/scispacy/issues/77:11,safety,updat,update,11,"Done, will update paper shortly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/77
https://github.com/allenai/scispacy/issues/77:11,security,updat,update,11,"Done, will update paper shortly.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/77
https://github.com/allenai/scispacy/issues/78:153,deployability,instal,install,153,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:295,deployability,instal,install,295,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:322,deployability,instal,install,322,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:381,deployability,releas,releases,381,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:624,energy efficiency,load,load,624,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:624,performance,load,load,624,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:525,usability,help,help,525,"Hi! Sorry you are having trouble getting setup. I am not able to reproduce this issue in a clean environment. Could you list the exact steps you took to install? For comparison, here is what I did:. 1. `conda create -n scispacy_install python=3.6`. 1. `conda activate scispacy_install`. 1. `pip install scispacy`. 1. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. ```. Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) . [GCC 7.3.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> nlp = spacy.load('en_core_sci_sm'). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:200,deployability,instal,installed,200,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:244,deployability,instal,installations,244,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:451,integrability,coupl,couple,451,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:347,interoperability,share,share,347,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:191,modifiability,pac,packages,191,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:451,modifiability,coupl,couple,451,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:173,reliability,doe,does,173,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:451,testability,coupl,couple,451,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:296,usability,prefer,prefer,296,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:373,usability,command,commands,373,"The easiest solution is probably to use a virtual environment. That being said, my guess is that you are somehow running your program `scispacy.py` from an environment that does not have the packages installed. Possibly your `pip` and `python` installations are not connected. If you would still prefer to not use a virtual environment, could you share the output of these commands? 1. `which -a pip`. 1. `which -a python`. 1. `pip list`. 1. the last couple lines of `brew doctor`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:95,deployability,Version,Version,95,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:928,deployability,version,version,928,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:966,deployability,version,version-string,966,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1011,deployability,version,version,1011,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:293,energy efficiency,core,core-sci-sm,293,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:799,energy efficiency,profil,profiler,799,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:95,integrability,Version,Version,95,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:687,integrability,wrap,wrapt,687,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:928,integrability,version,version,928,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:966,integrability,version,version-string,966,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1011,integrability,version,version,1011,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:760,interoperability,platform,platform,760,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:87,modifiability,Pac,Package,87,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:95,modifiability,Version,Version,95,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:928,modifiability,version,version,928,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:966,modifiability,version,version-string,966,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1011,modifiability,version,version,1011,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:799,performance,profil,profiler,799,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:838,safety,test,testing,838,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:168,security,certif,certifi,168,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:529,security,rsa,rsa,529,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:838,testability,test,testing,838,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:633,usability,tool,toolz,633,which -a pip : /usr/local/bin/pip3. which -a python: /usr/local/bin/python3. pip list:`Package Version. --------------- ----------. awscli 1.16.111. botocore 1.12.101. certifi 2018.11.29. chardet 3.0.4. colorama 0.3.9. conllu 1.2.2. cymem 2.0.2. cytoolz 0.9.0.1. dill 0.2.9. docutils 0.14. en-core-sci-sm 0.1.0. idna 2.8. jmespath 0.9.4. msgpack 0.5.6. msgpack-numpy 0.4.3.2. murmurhash 1.0.2. numpy 1.16.1. pip 18.1. plac 0.9.6. preshed 2.0.1. pyasn1 0.4.5. python-dateutil 2.8.0. PyYAML 3.13. regex 2018.1.10. requests 2.21.0. rsa 3.4.2. s3transfer 0.2.0. scispacy 0.1.0. setuptools 40.6.2. six 1.12.0. spacy 2.0.18. thinc 6.12.1. toolz 0.9.0. tqdm 4.31.1. ujson 1.35. urllib3 1.24.1. wrapt 1.10.11`. Last few lines brew doctor: . /usr/local/include/node/v8-platform.h. /usr/local/include/node/v8-profiler.h. /usr/local/include/node/v8-testing.h. /usr/local/include/node/v8-util.h. /usr/local/include/node/v8-value-serializer-version.h. /usr/local/include/node/v8-version-string.h. /usr/local/include/node/v8-version.h. /usr/local/include/node/v8.h. /usr/local/include/node/v8config.h. /usr/local/include/node/zconf.h. /usr/local/include/node/zlib.h.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:132,deployability,instal,install,132,"This seems like an issue with your python setup rather than with this package specifically. Rather than trying to debug your python install over github, the easiest thing here would be for you to setup a virtual environment (instructions for this are also in the readme). Is this something that you could try out?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:78,interoperability,specif,specifically,78,"This seems like an issue with your python setup rather than with this package specifically. Rather than trying to debug your python install over github, the easiest thing here would be for you to setup a virtual environment (instructions for this are also in the readme). Is this something that you could try out?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:70,modifiability,pac,package,70,"This seems like an issue with your python setup rather than with this package specifically. Rather than trying to debug your python install over github, the easiest thing here would be for you to setup a virtual environment (instructions for this are also in the readme). Is this something that you could try out?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:61,usability,help,helping,61,"Sure, it makes sense to use Virtual environment.. Thanks for helping",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:40,deployability,instal,installing,40,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:127,deployability,modul,module,127,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:221,deployability,modul,module,221,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1030,deployability,modul,module,1030,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1120,deployability,Modul,ModuleNotFoundError,1120,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1144,deployability,modul,module,1144,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:242,energy efficiency,load,load,242,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:370,energy efficiency,load,load,370,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:127,modifiability,modul,module,127,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:221,modifiability,modul,module,221,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:329,modifiability,pac,packages,329,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:482,modifiability,pac,packages,482,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:646,modifiability,pac,packages,646,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:900,modifiability,pac,package,900,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:980,modifiability,pac,packages,980,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1030,modifiability,modul,module,1030,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1120,modifiability,Modul,ModuleNotFoundError,1120,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1144,modifiability,modul,module,1144,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1215,modifiability,pac,package,1215,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:242,performance,load,load,242,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:370,performance,load,load,370,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:127,safety,modul,module,127,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:221,safety,modul,module,221,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1030,safety,modul,module,1030,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1120,safety,Modul,ModuleNotFoundError,1120,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1144,safety,modul,module,1144,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:59,testability,Trace,Traceback,59,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:160,usability,User,Users,160,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:273,usability,User,Users,273,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:426,usability,User,Users,426,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:590,usability,User,Users,590,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:753,usability,User,Users,753,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:924,usability,User,Users,924,"@danielkingai2 , still struggling after installing conda. `Traceback (most recent call last):. File ""scispacy.py"", line 1, in <module>. import scispacy. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 4, in <module>. nlp = spacy.load(""en_core_sci_sm""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_core_sci_sm/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:319,deployability,modul,module,319,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:397,deployability,modul,module,397,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1208,deployability,modul,module,1208,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1298,deployability,Modul,ModuleNotFoundError,1298,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1322,deployability,modul,module,1322,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:418,energy efficiency,load,load,418,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:547,energy efficiency,load,load,547,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:85,interoperability,Compatib,Compatible,85,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:319,modifiability,modul,module,319,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:397,modifiability,modul,module,397,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:506,modifiability,pac,packages,506,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:659,modifiability,pac,packages,659,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:823,modifiability,pac,packages,823,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1077,modifiability,pac,package,1077,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1157,modifiability,pac,packages,1157,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1208,modifiability,modul,module,1208,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1298,modifiability,Modul,ModuleNotFoundError,1298,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1322,modifiability,modul,module,1322,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1393,modifiability,pac,package,1393,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:418,performance,load,load,418,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:547,performance,load,load,547,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:319,safety,modul,module,319,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:397,safety,modul,module,397,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1208,safety,modul,module,1208,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1298,safety,Modul,ModuleNotFoundError,1298,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1322,safety,modul,module,1322,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:255,testability,Trace,Traceback,255,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:151,usability,help,help,151,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:335,usability,User,Users,335,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:450,usability,User,Users,450,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:603,usability,User,Users,603,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:767,usability,User,Users,767,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:930,usability,User,Users,930,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1101,usability,User,Users,1101,"Here : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/shai26/office/spacy/scispacy/scispacy.py"", line 27, in <module>. nlp = spacy.load(""en_ner_craft_md""). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/__init__.py"", line 21, in load. return util.load_model(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 114, in load_model. return load_model_from_package(name, **overrides). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/spacy/util.py"", line 134, in load_model_from_package. cls = importlib.import_module(name). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/importlib/__init__.py"", line 126, in import_module. return _bootstrap._gcd_import(name[level:], package, level). File ""/Users/shai26/anaconda3/envs/scispacy/lib/python3.6/site-packages/en_ner_craft_md/__init__.py"", line 7, in <module>. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmenter'; 'scispacy' is not a package`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:139,modifiability,pac,package,139,"Also output of `conda list`? Sorry, this is difficult to debug over github and seems to be an issue with your python setup rather than the package itself as I am not able to reproduce your issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:216,deployability,Version,Version,216,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:224,deployability,Build,Build,224,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:490,energy efficiency,core,core-sci-sm,490,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:216,integrability,Version,Version,216,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1353,integrability,wrap,wrapt,1353,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:139,modifiability,pac,packages,139,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:216,modifiability,Version,Version,216,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:290,security,certif,certificates,290,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:316,security,certif,certifi,316,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1060,security,rsa,rsa,1060,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:14,testability,understand,understand,14,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:167,usability,User,Users,167,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:1254,usability,tool,toolz,1254,"seems like, i understand it can be a pain over github.. i am also trying to figure out.. but in vain .. here is the list of conda list. `# packages in environment at /Users/shai26/anaconda3/envs/scispacy:. #. # Name Version Build Channel. awscli 1.16.113 <pip>. botocore 1.12.103 <pip>. ca-certificates 2019.1.23 0. certifi 2018.11.29 py36_0. chardet 3.0.4 <pip>. colorama 0.3.9 <pip>. conllu 1.2.2 <pip>. cymem 2.0.2 <pip>. cytoolz 0.9.0.1 <pip>. dill 0.2.9 <pip>. docutils 0.14 <pip>. en-core-sci-sm 0.1.0 <pip>. en-ner-craft-md 0.1.0 <pip>. idna 2.8 <pip>. jmespath 0.9.4 <pip>. libcxx 4.0.1 hcfea43d_1. libcxxabi 4.0.1 hcfea43d_1. libedit 3.1.20181209 hb402a30_0. libffi 3.2.1 h475c297_4. msgpack 0.5.6 <pip>. msgpack-numpy 0.4.3.2 <pip>. murmurhash 1.0.2 <pip>. ncurses 6.1 h0a44026_1. numpy 1.16.2 <pip>. openssl 1.1.1b h1de35cc_0. pip 19.0.3 py36_0. plac 0.9.6 <pip>. preshed 2.0.1 <pip>. pyasn1 0.4.5 <pip>. python 3.6.8 haf84260_0. python-dateutil 2.8.0 <pip>. PyYAML 3.13 <pip>. readline 7.0 h1de35cc_5. regex 2018.1.10 <pip>. requests 2.21.0 <pip>. rsa 3.4.2 <pip>. s3transfer 0.2.0 <pip>. scispacy 0.1.0 <pip>. setuptools 40.8.0 py36_0. six 1.12.0 <pip>. spacy 2.0.18 <pip>. sqlite 3.26.0 ha441bb4_0. thinc 6.12.1 <pip>. tk 8.6.8 ha441bb4_0. toolz 0.9.0 <pip>. tqdm 4.31.1 <pip>. ujson 1.35 <pip>. urllib3 1.24.1 <pip>. wheel 0.33.1 py36_0. wrapt 1.10.11 <pip>. xz 5.2.4 h1de35cc_4. zlib 1.2.11 h1de35cc_3`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:393,deployability,instal,install,393,Ok lets try this. ```. conda deactivate. pip uninstall en-core-sci-sm. pip uninstall en-ner-craft-md. pip uninstall scispacy. pip uninstall spacy. ```. Then verify that you can't import spacy or scispacy. Then create a new conda environment with. ```. conda create -n test_env python=3.6. conda activate test_env. ```. Then verify that you still can't import spacy or scispacy. Then. ```. pip install scispacy. ```. Then try to import it,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:58,energy efficiency,core,core-sci-sm,58,Ok lets try this. ```. conda deactivate. pip uninstall en-core-sci-sm. pip uninstall en-ner-craft-md. pip uninstall scispacy. pip uninstall spacy. ```. Then verify that you can't import spacy or scispacy. Then create a new conda environment with. ```. conda create -n test_env python=3.6. conda activate test_env. ```. Then verify that you still can't import spacy or scispacy. Then. ```. pip install scispacy. ```. Then try to import it,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:157,testability,verif,verify,157,Ok lets try this. ```. conda deactivate. pip uninstall en-core-sci-sm. pip uninstall en-ner-craft-md. pip uninstall scispacy. pip uninstall spacy. ```. Then verify that you can't import spacy or scispacy. Then create a new conda environment with. ```. conda create -n test_env python=3.6. conda activate test_env. ```. Then verify that you still can't import spacy or scispacy. Then. ```. pip install scispacy. ```. Then try to import it,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:324,testability,verif,verify,324,Ok lets try this. ```. conda deactivate. pip uninstall en-core-sci-sm. pip uninstall en-ner-craft-md. pip uninstall scispacy. pip uninstall spacy. ```. Then verify that you can't import spacy or scispacy. Then create a new conda environment with. ```. conda create -n test_env python=3.6. conda activate test_env. ```. Then verify that you still can't import spacy or scispacy. Then. ```. pip install scispacy. ```. Then try to import it,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:325,deployability,modul,module,325,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:91,interoperability,Compatib,Compatible,91,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:325,modifiability,modul,module,325,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:325,safety,modul,module,325,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:261,testability,Trace,Traceback,261,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:157,usability,help,help,157,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:413,usability,User,Users,413,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:475,usability,User,Users,475,"here it is : . `Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46). [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> import scispacy. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. ImportError: bad magic number in 'scispacy': b'\x03\xf3\r\n'`. which python:. /Users/shai26/anaconda3/envs/test_env/bin/python. which pip:. /Users/shai26/anaconda3/envs/test_env/bin/pip",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/78:38,usability,help,help,38,Thanks a ton.. really appreciate your help.. keep up the good work.. cheers..,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/78
https://github.com/allenai/scispacy/issues/79:26,energy efficiency,load,load,26,"import spacy. nlp = spacy.load(""en_ner_craft_md""). text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity. "" \. ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC)."". doc = nlp(text). for ent in doc.ents:. print(ent, ent.label_).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:26,performance,load,load,26,"import spacy. nlp = spacy.load(""en_ner_craft_md""). text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity. "" \. ""They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC)."". doc = nlp(text). for ent in doc.ents:. print(ent, ent.label_).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:120,energy efficiency,model,model,120,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:397,energy efficiency,load,load,397,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:397,performance,load,load,397,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:120,security,model,model,120,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:783,security,auth,authored,783,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:1000,security,auth,auth,1000,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:180,usability,clear,clearity,180,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:262,usability,help,help,262,"Hi Ravina,. Thank you for the information. I am trying to extract drug name and disease. name from a sentence. So which model will be best fit for this and also can. you give me a clearity what GGP and CL entity mean in medical terms. Thank you so much for your help. -Regards,. Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>. wrote:. > import spacy. > nlp = spacy.load(""en_ner_craft_md""). > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells. > with immunosuppressive activity. "". > ""They accumulate in tumor-bearing mice and humans with different types of. > cancer, including hepatocellular carcinoma (HCC)."". > doc = nlp(text). > for ent in doc.ents:. > print(ent, ent.label_). >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:43,energy efficiency,model,models,43,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:62,energy efficiency,predict,predictions,62,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:221,energy efficiency,model,models,221,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:314,interoperability,specif,specifically,314,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:62,safety,predict,predictions,62,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:43,security,model,models,43,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:221,security,model,models,221,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:548,usability,confirm,confirm,548,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:607,usability,document,documentation,607,"You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:137,energy efficiency,model,models,137,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:156,energy efficiency,predict,predictions,156,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:311,interoperability,specif,specifically,311,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:156,safety,predict,predictions,156,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:137,security,model,models,137,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:806,security,auth,authored,806,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:1023,security,auth,auth,1023,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:582,usability,confirm,confirm,582,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:644,usability,document,documentation,644,"Thank you. On Thu, 28 Feb 2019, 12:34 a.m. Daniel King, <notifications@github.com>. wrote:. > You will want to experiment with different models and combine predictions. > from multiple, limiting to the types that are of interest to you. For. > example, diseases are in the BC5CDR, As for what GGP and CL are. > specifically, there are more details about the craft corpus here:. > https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. > But loosely speaking, GGP is genes and proteins, and CL is cell types. > @DeNeutoy <https://github.com/DeNeutoy> can you confirm what GGP is? It. > isn't directly listed in the CRAFT documentation. >. > Additionally, our paper about scispacy is now on arxiv:. > https://arxiv.org/pdf/1902.07669.pdf. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-467988913>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8JQqtCfdz5-xsW2Vr5dJAJs6MxuHks5vRtakgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:100,energy efficiency,model,model,100,"Does the CHEMICAL label mean a medicine or drug ? This is the output i got using ""en_ner_bc5cdr_md"" model. Acetaminophen CHEMICAL. paracetamol CHEMICAL. loss of DISEASE. deaths DISEASE. deaths DISEASE. infectious diseases DISEASE. nutritional deficiencies.a (HCC).'doc DISEASE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:0,reliability,Doe,Does,0,"Does the CHEMICAL label mean a medicine or drug ? This is the output i got using ""en_ner_bc5cdr_md"" model. Acetaminophen CHEMICAL. paracetamol CHEMICAL. loss of DISEASE. deaths DISEASE. deaths DISEASE. infectious diseases DISEASE. nutritional deficiencies.a (HCC).'doc DISEASE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:100,security,model,model,100,"Does the CHEMICAL label mean a medicine or drug ? This is the output i got using ""en_ner_bc5cdr_md"" model. Acetaminophen CHEMICAL. paracetamol CHEMICAL. loss of DISEASE. deaths DISEASE. deaths DISEASE. infectious diseases DISEASE. nutritional deficiencies.a (HCC).'doc DISEASE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:153,security,loss,loss,153,"Does the CHEMICAL label mean a medicine or drug ? This is the output i got using ""en_ner_bc5cdr_md"" model. Acetaminophen CHEMICAL. paracetamol CHEMICAL. loss of DISEASE. deaths DISEASE. deaths DISEASE. infectious diseases DISEASE. nutritional deficiencies.a (HCC).'doc DISEASE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:58,energy efficiency,model,model,58,"The annotation guidelines for the BC5CDR data (which that model was trained on can) can be found here: https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf. and that document says that a chemical is defined as the Drugs and Chemicals [D] branch of Mesh 2015, which can be found here: https://www.nlm.nih.gov/mesh/trees.html",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:58,security,model,model,58,"The annotation guidelines for the BC5CDR data (which that model was trained on can) can be found here: https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf. and that document says that a chemical is defined as the Drugs and Chemicals [D] branch of Mesh 2015, which can be found here: https://www.nlm.nih.gov/mesh/trees.html",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:15,usability,guid,guidelines,15,"The annotation guidelines for the BC5CDR data (which that model was trained on can) can be found here: https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf. and that document says that a chemical is defined as the Drugs and Chemicals [D] branch of Mesh 2015, which can be found here: https://www.nlm.nih.gov/mesh/trees.html",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:208,usability,document,document,208,"The annotation guidelines for the BC5CDR data (which that model was trained on can) can be found here: https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf. and that document says that a chemical is defined as the Drugs and Chemicals [D] branch of Mesh 2015, which can be found here: https://www.nlm.nih.gov/mesh/trees.html",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:160,energy efficiency,model,model,160,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:160,security,model,model,160,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:470,security,auth,authored,470,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:687,security,auth,auth,687,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:117,usability,guid,guidelines,117,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:260,usability,document,document,260,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>. wrote:. > The annotation guidelines for the BC5CDR data (which that model was. > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html. > and that document says that a chemical is defined as the Drugs and. > Chemicals [D] branch of Mesh 2015, which can be found here:. > https://www.nlm.nih.gov/mesh/trees.html. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:73,integrability,pub,pubmed,73,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:408,integrability,compon,components,408,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:348,interoperability,Ontolog,Ontology,348,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:390,interoperability,Ontolog,Ontology,390,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:408,interoperability,compon,components,408,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:431,interoperability,Ontolog,Ontology,431,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:511,interoperability,Ontolog,Ontology,511,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:408,modifiability,compon,components,408,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:571,usability,user,user-images,571,**Concept Annotation in the CRAFT Corpus**. https://www.ncbi.nlm.nih.gov/pubmed/22776079. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. PDF: https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-13-161. ```. CL cell line. ChEBI chemical entities of biological interest. EG Entrez gene. GO BP Gene Ontology biological processes. GO CC Gene Ontology cellular components. GO MF Gene Ontology molecular functions. NCBITaxon NCBI taxonomy. PRO protein. SO Sequence Ontology. ```. ![CRAFT_corpus_concepts_terminology](https://user-images.githubusercontent.com/2575920/69473213-09c6ba00-0d68-11ea-8114-b22f456f5c75.png).,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:45,energy efficiency,model,models,45,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:64,energy efficiency,predict,predictions,64,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:223,energy efficiency,model,models,223,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:316,interoperability,specif,specifically,316,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:1001,reliability,doe,does,1001,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:64,safety,predict,predictions,64,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:45,security,model,models,45,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:223,security,model,models,223,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:768,security,access,access,768,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:550,usability,confirm,confirm,550,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:609,usability,document,documentation,609,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:775,usability,document,documentation,775,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:1011,usability,document,documentation,1011,"> You will want to experiment with different models and combine predictions from multiple, limiting to the types that are of interest to you. For example, diseases are in the BC5CDR, but drug names might appear in multiple models. Possibly start with BC5CDR and see if that works for you. As for what GGP and CL are specifically, there are more details about the craft corpus here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-13-161. But loosely speaking, GGP is genes and proteins, and CL is cell types. @DeNeutoy can you confirm what GGP is? It isn't directly listed in the CRAFT documentation. > . > Additionally, our paper about scispacy is now on arxiv: https://arxiv.org/pdf/1902.07669.pdf. Potentially related issue: is there easy to access documentation for entity labels within scispaCy? Labels are working fine, but I need a way to provide a short description of what each label means, since some aren't self-explanatory. I realize I can go to all the papers, but does this documentation already exist somewhere?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:32,energy efficiency,current,currently,32,"Hi, this documentation does not currently exist anywhere outside of the original dataset papers. Perhaps it should. If you end up compiling this information, we would welcome a contribution to the README or something with the info. Also, if there is any particular entity label you are unable to figure out, feel free to leave a question and I can try to find the info. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:23,reliability,doe,does,23,"Hi, this documentation does not currently exist anywhere outside of the original dataset papers. Perhaps it should. If you end up compiling this information, we would welcome a contribution to the README or something with the info. Also, if there is any particular entity label you are unable to figure out, feel free to leave a question and I can try to find the info. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:9,usability,document,documentation,9,"Hi, this documentation does not currently exist anywhere outside of the original dataset papers. Perhaps it should. If you end up compiling this information, we would welcome a contribution to the README or something with the info. Also, if there is any particular entity label you are unable to figure out, feel free to leave a question and I can try to find the info. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:156,energy efficiency,model,model,156,"hi aaryaa, . I have the same question like you. I am trying to extract drug name and disease. name Or drug/protein from a sentence using scispacy. so which model will be best fit for this? can you please? has anyone got the code for this association/relation? i have done the below table. i need similar output with columns for drug / protein relation /disease. ![187511971_480226719702340_7097340215165296287_n](https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:156,security,model,model,156,"hi aaryaa, . I have the same question like you. I am trying to extract drug name and disease. name Or drug/protein from a sentence using scispacy. so which model will be best fit for this? can you please? has anyone got the code for this association/relation? i have done the below table. i need similar output with columns for drug / protein relation /disease. ![187511971_480226719702340_7097340215165296287_n](https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:421,usability,user,user-images,421,"hi aaryaa, . I have the same question like you. I am trying to extract drug name and disease. name Or drug/protein from a sentence using scispacy. so which model will be best fit for this? can you please? has anyone got the code for this association/relation? i have done the below table. i need similar output with columns for drug / protein relation /disease. ![187511971_480226719702340_7097340215165296287_n](https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:22,energy efficiency,core,core,22,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:322,energy efficiency,model,model,322,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:322,security,model,model,322,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:756,security,auth,authored,756,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:969,security,auth,auth,969,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:64,usability,Custom,Custom-Labelled-NER-tagger,64,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/79:615,usability,user,user-images,615,"you can use standford core nlp. https://github.com/mridultuteja/Custom-Labelled-NER-tagger. On Thu, May 20, 2021 at 1:12 PM issayatim ***@***.***> wrote:. > hi aaryaa,. >. > I have the same question like you. I am trying to extract drug name and. > disease. > name Or drug/protein from a sentence using scispacy. so which model will. > be best fit for this? can you please? > has anyone got the code for this association/relation? >. > i have done the below table. i need similar output with columns for drug /. > protein relation /disease. >. > [image: 187511971_480226719702340_7097340215165296287_n]. > <https://user-images.githubusercontent.com/84497052/118938744-a49d5600-b957-11eb-8a01-19b5185884a1.png>. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/79#issuecomment-844816692>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACBEB4CFIYJMYS536GPXKLLTOS4NHANCNFSM4G2RCNNQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79
https://github.com/allenai/scispacy/issues/81:143,deployability,instal,install,143,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:477,deployability,instal,install,477,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:35,energy efficiency,CPU,CPU,35,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:127,energy efficiency,GPU,GPU,127,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:221,energy efficiency,gpu,gpu,221,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:244,energy efficiency,GPU,GPU,244,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:276,energy efficiency,GPU,GPU,276,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:345,energy efficiency,GPU,GPU,345,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:387,energy efficiency,gpu,gpu,387,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:410,energy efficiency,load,load,410,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:426,energy efficiency,model,model,426,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:496,energy efficiency,gpu,gpu,496,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:554,energy efficiency,load,load,554,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:35,performance,CPU,CPU,35,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:127,performance,GPU,GPU,127,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:221,performance,gpu,gpu,221,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:244,performance,GPU,GPU,244,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:276,performance,GPU,GPU,276,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:345,performance,GPU,GPU,345,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:387,performance,gpu,gpu,387,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:410,performance,load,load,410,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:496,performance,gpu,gpu,496,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:554,performance,load,load,554,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:114,reliability,doe,does,114,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:426,security,model,model,426,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:119,usability,support,support,119,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:360,usability,confirm,confirm,360,"We have been running everything on CPU (and this is very fast because spaCy is very fast). That being said, spaCy does support GPU, and if you install and use spacy based on the instructions here (https://spacy.io/usage/#gpu), spaCy should use GPU, and so scispaCy should use GPU. However, I am not sure what speedup you would expect from using GPU, but I did confirm that there is some gpu utilization if you load a scispacy model like so (after following the instructions to install spacy with gpu). ```. import spacy. spacy.require_gpu(). nlp = spacy.load('en_core_sci_sm'). doc = nlp(""Some text""). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:130,availability,error,error,130,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:229,deployability,modul,module,229,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:6,energy efficiency,GPU,GPU,6,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:49,energy efficiency,CPU,CPU,49,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:456,energy efficiency,core,core,456,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:461,energy efficiency,core,core,461,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:491,energy efficiency,core,core,491,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:496,energy efficiency,core,core,496,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:539,energy efficiency,core,core,539,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:576,energy efficiency,core,core,576,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:618,energy efficiency,core,core,618,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:654,energy efficiency,core,core,654,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:229,modifiability,modul,module,229,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:6,performance,GPU,GPU,6,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:49,performance,CPU,CPU,49,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:130,performance,error,error,130,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:130,safety,error,error,130,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:229,safety,modul,module,229,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:275,security,token,tokens,275,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:339,security,token,tokens,339,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:412,security,token,tokens,412,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:163,testability,Trace,Traceback,163,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:130,usability,error,error,130,"using GPU works a lot faster than using spacy on CPU for multiple queries. . I tried the above method, but it gives the following error. > doc.similarity(doc2). > Traceback (most recent call last):. > File ""<stdin>"", line 1, in <module>. > File ""doc.pyx"", line 322, in spacy.tokens.doc.Doc.similarity. > File ""doc.pyx"", line 386, in spacy.tokens.doc.Doc.vector_norm.__get__. > File ""doc.pyx"", line 361, in spacy.tokens.doc.Doc.vector.__get__. > File ""cupy/core/core.pyx"", line 1741, in cupy.core.core.ndarray.__array_ufunc__. > File ""cupy/core/_kernel.pyx"", line 787, in cupy.core._kernel.ufunc.__call__. > File ""cupy/core/_kernel.pyx"", line 86, in cupy.core._kernel._preprocess_args. > TypeError: Unsupported type <class 'numpy.ndarray'>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:41,deployability,instal,installing,41,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:77,deployability,instal,install,77,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:120,deployability,version,version,120,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:155,deployability,instal,installing,155,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:171,deployability,instal,install,171,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:198,deployability,instal,install,198,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:63,energy efficiency,gpu,gpu,63,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:120,integrability,version,version,120,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:120,modifiability,version,version,120,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:63,performance,gpu,gpu,63,Try it a new conda environment and first installing spacy with gpu with `pip install -U spacy[cuda92]` or with whatever version of cuda you have. and then installing `pip install scispacy` and `pip install <scispacy_model_link>`. Those were the steps I followed that worked for me.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:159,deployability,instal,installing,159,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:197,deployability,instal,install,197,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:239,deployability,version,version,239,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:277,deployability,instal,installing,277,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:292,deployability,instal,install,292,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:317,deployability,instal,install,317,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:181,energy efficiency,gpu,gpu,181,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:239,integrability,version,version,239,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:239,modifiability,version,version,239,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:181,performance,gpu,gpu,181,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:447,security,auth,authored,447,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:664,security,auth,auth,664,"Still similarity function not working. On Thu, Feb 28, 2019, 1:46 AM Daniel King <notifications@github.com> wrote:. > Try it a new conda environment and first installing spacy with gpu with pip. > install -U spacy[cuda92] or with whatever version of cuda you have. and. > then installing pip install scispacy and pip install <scispacy_model_link>. > Those were the steps I followed that worked for me. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/81#issuecomment-468013885>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AXL1kTNfW_BHcPZY4e4V_jjVoB_jTkfRks5vRueogaJpZM4bVJms>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:70,energy efficiency,model,model,70,"hmm, I was actually able to reproduce with the spacy `en_core_web_md` model. It seems that this an issue with spacy, so i've opened an issue on the spacy repo https://github.com/explosion/spaCy/issues/3348",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:70,security,model,model,70,"hmm, I was actually able to reproduce with the spacy `en_core_web_md` model. It seems that this an issue with spacy, so i've opened an issue on the spacy repo https://github.com/explosion/spaCy/issues/3348",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:79,deployability,releas,releases,79,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:99,deployability,upgrad,upgrade,99,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:115,deployability,version,version,115,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:115,integrability,version,version,115,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:99,modifiability,upgrad,upgrade,99,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:115,modifiability,version,version,115,"PR open to fix this (https://github.com/explosion/spaCy/pull/3362), when spaCy releases 2.1 and we upgrade to that version this issue will be fixed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:24,deployability,version,version,24,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:62,deployability,version,version,62,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:103,deployability,version,version,103,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:24,integrability,version,version,24,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:62,integrability,version,version,62,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:103,integrability,version,version,103,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:87,interoperability,compatib,compatible,87,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:24,modifiability,version,version,24,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:62,modifiability,version,version,62,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/81:103,modifiability,version,version,103,This should be fixed in version `0.2.0` which is now up. This version of `scispacy` is compatible with version `2.1.3` of `spacy`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/81
https://github.com/allenai/scispacy/issues/82:309,deployability,pipelin,pipelines,309,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:6,energy efficiency,model,model,6,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:86,energy efficiency,model,models,86,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:147,energy efficiency,model,models,147,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:12,integrability,compon,components,12,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:122,integrability,compon,components,122,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:309,integrability,pipelin,pipelines,309,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:12,interoperability,compon,components,12,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:122,interoperability,compon,components,122,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:166,interoperability,compatib,compatible,166,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:12,modifiability,compon,components,12,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:122,modifiability,compon,components,122,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:6,security,model,model,6,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:86,security,model,models,86,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:147,security,model,models,147,"spaCy model components need to use the vocab that they were created with. Each of the models has a different vocab and so components from separate models will not be compatible. There may be a workaround here that I don't know about, but for now I think what we would suggest for this use case is running the pipelines separately and then attempting to merge their outputs yourself by iterating over the resulting docs together. For example, this might look something like running en_core_sci_sm on text, then running en_ner_bionlp13cg_md on text, then iterating over the two resulting docs together and labeling entities on one doc using the other.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:10,safety,test,tested,10,"I haven't tested this at all, but it looks like you might be able to do something like here: https://github.com/explosion/spaCy/issues/1752 with vocab=False. Let us know if you experiment with this and get it to work! (I might poke around with it also)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:10,testability,test,tested,10,"I haven't tested this at all, but it looks like you might be able to do something like here: https://github.com/explosion/spaCy/issues/1752 with vocab=False. Let us know if you experiment with this and get it to work! (I might poke around with it also)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:158,energy efficiency,model,model,158,"unfortunately the command ner_A.from_disk('ner_A', vocab=False) does not work. you cannot call vocab=False and I can't find a way of detaching vocab to a NER model. it maybe the case that one need to retrain the NER using a fixed vocab, for example the one from ""en_core_sci_md"". Let me know if you have a better idea!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:64,reliability,doe,does,64,"unfortunately the command ner_A.from_disk('ner_A', vocab=False) does not work. you cannot call vocab=False and I can't find a way of detaching vocab to a NER model. it maybe the case that one need to retrain the NER using a fixed vocab, for example the one from ""en_core_sci_md"". Let me know if you have a better idea!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:158,security,model,model,158,"unfortunately the command ner_A.from_disk('ner_A', vocab=False) does not work. you cannot call vocab=False and I can't find a way of detaching vocab to a NER model. it maybe the case that one need to retrain the NER using a fixed vocab, for example the one from ""en_core_sci_md"". Let me know if you have a better idea!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/82:18,usability,command,command,18,"unfortunately the command ner_A.from_disk('ner_A', vocab=False) does not work. you cannot call vocab=False and I can't find a way of detaching vocab to a NER model. it maybe the case that one need to retrain the NER using a fixed vocab, for example the one from ""en_core_sci_md"". Let me know if you have a better idea!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/82
https://github.com/allenai/scispacy/issues/83:55,energy efficiency,model,models,55,"Hi, the word vectors found here were used to train the models, taking the most frequent ~ 100,000 word vectors based on some subset of pubmed:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/83
https://github.com/allenai/scispacy/issues/83:125,integrability,sub,subset,125,"Hi, the word vectors found here were used to train the models, taking the most frequent ~ 100,000 word vectors based on some subset of pubmed:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/83
https://github.com/allenai/scispacy/issues/83:135,integrability,pub,pubmed,135,"Hi, the word vectors found here were used to train the models, taking the most frequent ~ 100,000 word vectors based on some subset of pubmed:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/83
https://github.com/allenai/scispacy/issues/83:55,security,model,models,55,"Hi, the word vectors found here were used to train the models, taking the most frequent ~ 100,000 word vectors based on some subset of pubmed:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/83
https://github.com/allenai/scispacy/issues/84:120,deployability,depend,depends,120,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:442,energy efficiency,model,model,442,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:488,energy efficiency,load,load,488,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:120,integrability,depend,depends,120,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:120,modifiability,depend,depends,120,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:488,performance,load,load,488,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:26,reliability,doe,does,26,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:120,safety,depend,depends,120,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:411,safety,input,input,411,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:442,security,model,model,442,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:120,testability,depend,depends,120,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:220,testability,context,contexts,220,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:232,testability,understand,understand,232,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/84:411,usability,input,input,411,"Empirically, that example does not work (see below). More generally, it will likely be a case by case basis, because it depends on whether or not the training data has enough examples of both of those phrases in similar contexts to understand that they refer to similar things. See #83 for more detail about the word vectors. Of course, you could also use the vectors and/or the similarity of the vectors as an input to another normalization model. ```. >>> import spacy. >>> nlp = spacy.load('en_ner_craft_md'). >>> doc1 = nlp(""Basic Helix-Loop-Helix Transcription Factor Scleraxis""). >>> doc2 = nlp(""SCX""). >>> doc2.similarity(doc1). 0.0904146602497725. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/84
https://github.com/allenai/scispacy/issues/85:84,deployability,updat,update,84,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/issues/85:39,reliability,doe,doesn,39,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/issues/85:84,safety,updat,update,84,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/issues/85:84,security,updat,update,84,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/issues/85:47,usability,support,support,47,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/issues/85:67,usability,hint,hints,67,Hmm I think this is because python 3.5 doesn't support inline type hints. We should update the readme.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/85
https://github.com/allenai/scispacy/pull/87:65,deployability,version,version,65,"Hi, sorry this change requires a lot more than just changing the version, because we need to retrain all of the scispacy models. Thanks for the PR, but we need to do some stuff internally before we are ready for this, so we'll do it ourselves. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/87
https://github.com/allenai/scispacy/pull/87:121,energy efficiency,model,models,121,"Hi, sorry this change requires a lot more than just changing the version, because we need to retrain all of the scispacy models. Thanks for the PR, but we need to do some stuff internally before we are ready for this, so we'll do it ourselves. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/87
https://github.com/allenai/scispacy/pull/87:65,integrability,version,version,65,"Hi, sorry this change requires a lot more than just changing the version, because we need to retrain all of the scispacy models. Thanks for the PR, but we need to do some stuff internally before we are ready for this, so we'll do it ourselves. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/87
https://github.com/allenai/scispacy/pull/87:65,modifiability,version,version,65,"Hi, sorry this change requires a lot more than just changing the version, because we need to retrain all of the scispacy models. Thanks for the PR, but we need to do some stuff internally before we are ready for this, so we'll do it ourselves. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/87
https://github.com/allenai/scispacy/pull/87:121,security,model,models,121,"Hi, sorry this change requires a lot more than just changing the version, because we need to retrain all of the scispacy models. Thanks for the PR, but we need to do some stuff internally before we are ready for this, so we'll do it ourselves. Thanks though!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/87
https://github.com/allenai/scispacy/pull/88:3,deployability,fail,failed,3,CI failed. It could be related to this https://github.com/allenai/allennlp/pull/2647. How do you recommend fixing it ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3,reliability,fail,failed,3,CI failed. It could be related to this https://github.com/allenai/allennlp/pull/2647. How do you recommend fixing it ?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:123,security,Hash,HashingVectorizer,123,"Addressed the comments above. You can probably use it now while I am experimenting with `char_wb`, `min_df`, `max_df` and `HashingVectorizer`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:786,deployability,configurat,configuration,786,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1534,deployability,log,log,1534,"sults are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ****",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3045,deployability,fail,failed,3045,": 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 100",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3368,deployability,fail,failed,3368,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3694,deployability,fail,failed,3694,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:4022,deployability,fail,failed,4022,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:4352,deployability,fail,failed,4352,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:98,energy efficiency,reduc,reduces,98,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:295,energy efficiency,reduc,reduced,295,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:341,energy efficiency,reduc,reduced,341,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:463,energy efficiency,load,load,463,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:673,energy efficiency,reduc,reduce,673,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:859,energy efficiency,Load,Loading,859,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1944,energy efficiency,Load,Loading,1944,"les: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2057,energy efficiency,Load,Loading,2057,"cy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. f",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2146,energy efficiency,Load,Loading,2146,"slib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2629,energy efficiency,Load,Loading,2629,"846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:786,integrability,configur,configuration,786,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:786,modifiability,configur,configuration,786,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:454,performance,disk,disk,454,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:463,performance,load,load,463,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:503,performance,time,time,503,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:587,performance,perform,performance,587,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:680,performance,memor,memory,680,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:699,performance,bottleneck,bottleneck,699,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:859,performance,Load,Loading,859,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:881,performance,memor,memory,881,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1944,performance,Load,Loading,1944,"les: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2057,performance,Load,Loading,2057,"cy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. f",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2146,performance,Load,Loading,2146,"slib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:2629,performance,Load,Loading,2629,"846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:214,reliability,doe,doesn,214,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:665,reliability,doe,doesn,665,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3045,reliability,fail,failed,3045,": 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 100",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3368,reliability,fail,failed,3368,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:3694,reliability,fail,failed,3694,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:4022,reliability,fail,failed,4022,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:4352,reliability,fail,failed,4352,.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ***************************************************. Fitting ann index took 3282.471282 seconds. Loading ann index from data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Reading MedMentions ... for k = 1. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.785411 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1. Gold concept in candidates: 56.28%. Gold concept not in candidates: 43.68%. Candidate generation failed: 0.05%. for k = 2. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.46088 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 2. Gold concept in candidates: 64.31%. Gold concept not in candidates: 35.65%. Candidate generation failed: 0.05%. for k = 10. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 15.230339 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 10. Gold concept in candidates: 74.64%. Gold concept not in candidates: 25.31%. Candidate generation failed: 0.05%. for k = 100. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 16.611278 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 100. Gold concept in candidates: 83.28%. Gold concept not in candidates: 16.68%. Candidate generation failed: 0.05%. for k = 1000. Generating candidates for 70306 mentions. Number of empty vectors: 32. Finding neighbors took 33.373053 seconds. MedMentions entities not in UMLS: 756. MedMentions entities found in UMLS: 70306. K: 1000. Gold concept in candidates: 86.89%. Gold concept not in candidates: 13.06%. Candidate generation failed: 0.05%. ```.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1534,safety,log,log,1534,"sults are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ****",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:514,security,Hash,HashingVectorizer,514,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:786,security,configur,configuration,786,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1534,security,log,log,1534,"sults are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ****",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:1534,testability,log,log,1534,"sults are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Tfidf vocab size: 54337. Loading tfidf vectors from data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading vectorizer and vectors took 24.519187 seconds. Deleting 7/6336776 aliases because their tfidf is empty. ['F₃T' 't&a' 'g%' 'g%' 'g%' ""K'"" '/{VF}']. No ann index on data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. Fitting ann index on 6336769 aliases (takes 2 hours).  0% 10 20 30 40 50 60 70 80 90 100%. \|----\|----\|----\|----\|----\|----\|----\|----\|----\|----\|. ****",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:587,usability,perform,performance,587,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:680,usability,memor,memory,680,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:881,usability,memor,memory,881,"Ran the experiments and here are the results: . - `char_wb`: better than `char`. . - `min_df=10`: reduces vocabulary size and improves ann results a bit. - `max_df`: has no effect. Even with a low `max_df=0.2`, it doesn't affect vocabulary size. - Used char3gram only (instead of 3 and 4). This reduced vocabulary size from 500K to 50K, and reduced size of the tfidf matrix from 5GB to 3GB . - with a tfidf mastix < 4GB, it is possible to save it on the disk and load it instead of reproducing it every time. . - `HashingVectorizer`: results are usually 2.5% worse than TfidfVectorizer (performance drops even more with vocabulary size < 50K). More importantly, it doesn't reduce memory because the bottleneck is number of non-zero values in the array not the vocabulary size. So, best configuration so far is: char_wb, char3gram, TfidfVectorizer, min_df=10. Loading everything in memory takes less than a minute, but requires many GBs of ram. . Files: . ```. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. s3://ai2-s2-scispacy/data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. s3://ai2-s2-scispacy/data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Then run . ```. python3 scripts/linking.py --medmentions_path data/medmentions --umls_path data/umls_2017_aa_cat0129.json --k 1,2,10,100,1000 --tfidf_vectorizer_path data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib --ann_index_path data/nmslib_index_char3gram_withaliases_wbchar_mindf10.bin. ```. Prints the following log: . ```. Collecting aliases ... Processed 1000000 or 2783846 concepts. Processed 2000000 or 2783846 concepts. No tfidf vectorizer on data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Fitting tfidf vectorizer on 6336776 aliases. Saving tfidf vectorizer to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib. Saving tfidf vectors to data/tfidfvec_char3gram_wbchar_vectorizer_mindf10.joblib.npy. Loading tfidf vectorizer from data/tfidfvec_char3gram_wb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:426,performance,perform,performance,426,"This is great - 2% overall improvement and way smaller. I've also just noticed that the default `dtype` for the matrix returned by `TfidfVectorizer` is `numpy.float64`. It's pretty likely that doing Approximate Nearest Neighbours with `numpy.float16` vectors makes almost no difference, meaning that the tf-idf matrix might be < 1GB. Given that we can drop the dimensionality of the vocab from 500k -> 50k without any drop in performance, perhaps we should try using the `max_features=10k` or something, but use both 3 and 4 grams to compute the vocab. Combined with the float16 option, this might get us a tf-idf matrix < 100mb.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:426,usability,perform,performance,426,"This is great - 2% overall improvement and way smaller. I've also just noticed that the default `dtype` for the matrix returned by `TfidfVectorizer` is `numpy.float64`. It's pretty likely that doing Approximate Nearest Neighbours with `numpy.float16` vectors makes almost no difference, meaning that the tf-idf matrix might be < 1GB. Given that we can drop the dimensionality of the vocab from 500k -> 50k without any drop in performance, perhaps we should try using the `max_features=10k` or something, but use both 3 and 4 grams to compute the vocab. Combined with the float16 option, this might get us a tf-idf matrix < 100mb.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:423,availability,down,down,423,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:194,deployability,build,building,194,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:108,energy efficiency,reduc,reduced,108,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:231,energy efficiency,reduc,reduced,231,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:276,energy efficiency,reduc,reduce,276,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:393,energy efficiency,reduc,reduces,393,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:463,energy efficiency,reduc,reducing,463,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:548,energy efficiency,reduc,reduces,548,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:585,energy efficiency,reduc,reduces,585,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:623,energy efficiency,reduc,reduces,623,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:556,performance,perform,performance,556,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:254,reliability,doe,doesn,254,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:262,security,sign,significantly,262,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:43,usability,support,supported,43,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:556,usability,perform,performance,556,"Good suggestions. - `numpy.float16` is not supported by `scipy.sparse`. Changing `dtype` to `numpy.float32` reduced the matrix size from 3GB to 1.8GB. Will upload the new index after it is done building. . - setting `max_features` reduced vocab size but doesn't significantly reduce number of nonzero values in the matrix because it drops the rare features. Setting it to `max_features=10000` reduces matrix size to 1.7GB (down from 1.8GB). - `max_df` is good at reducing number of nonzero values (removes the most common features), but it quickly reduces performance. `max_df=100000` reduces tfidf matrix size by half but reduces accuracy by 15%",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:160,performance,Perform,Performance,160,Uploaded . ```. s3://ai2-s2-scispacy/data/data/nmslib_index.bin. s3://ai2-s2-scispacy/data/tfidfvec.joblib. s3://ai2-s2-scispacy/data/tfidfvec.joblib.npy. ```. Performance is almost the same as float64. ```. K = 1: 55.60%. K = 2: 64.01%. K = 10: 74.81%. K = 100: 83.31%. K = 1000: 86.96%. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/pull/88:160,usability,Perform,Performance,160,Uploaded . ```. s3://ai2-s2-scispacy/data/data/nmslib_index.bin. s3://ai2-s2-scispacy/data/tfidfvec.joblib. s3://ai2-s2-scispacy/data/tfidfvec.joblib.npy. ```. Performance is almost the same as float64. ```. K = 1: 55.60%. K = 2: 64.01%. K = 10: 74.81%. K = 100: 83.31%. K = 1000: 86.96%. ```,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/88
https://github.com/allenai/scispacy/issues/92:5,deployability,version,versions,5,What versions of scispacy and spacy do you have?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:5,integrability,version,versions,5,What versions of scispacy and spacy do you have?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:5,modifiability,version,versions,5,What versions of scispacy and spacy do you have?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:73,availability,down,downloaded,73,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:42,deployability,version,version,42,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:130,deployability,version,version,130,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:163,deployability,version,version,163,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:291,deployability,releas,releases,291,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:345,deployability,upgrad,upgraded,345,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:368,deployability,version,version,368,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:417,deployability,version,version,417,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:544,deployability,releas,releases,544,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:610,deployability,updat,updated,610,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:653,deployability,version,version,653,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:58,energy efficiency,model,model,58,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:188,energy efficiency,model,models,188,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:446,energy efficiency,model,models,446,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:626,energy efficiency,model,model,626,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:42,integrability,version,version,42,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:130,integrability,version,version,130,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:163,integrability,version,version,163,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:368,integrability,version,version,368,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:417,integrability,version,version,417,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:653,integrability,version,version,653,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:29,interoperability,incompatib,incompatible,29,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:393,interoperability,compatib,compatible,393,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:42,modifiability,version,version,42,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:130,modifiability,version,version,130,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:163,modifiability,version,version,163,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:345,modifiability,upgrad,upgraded,345,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:368,modifiability,version,version,368,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:417,modifiability,version,version,417,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:653,modifiability,version,version,653,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:610,safety,updat,updated,610,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:58,security,model,model,58,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:188,security,model,models,188,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:446,security,model,models,446,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:610,security,updat,updated,610,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:626,security,model,model,626,"My guess is that you have an incompatible version for the model that you downloaded. You'll want to make sure that for `scispacy` version `0.1.0` you have `spacy` version `2.0.18` and use models that have `0.1.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.1.0/en_core_sci_sm-0.1.0.tar.gz`. We just upgraded `scispacy` to version `0.2.0` which is compatible with `spacy` version `2.1.3` and requires models with `0.2.0` in the url, like this one `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.0/en_core_sci_sm-0.2.0.tar.gz`. The README has been updated to have model links for `scispacy` version `0.2.0` as of yesterday.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/issues/92:98,usability,close,close,98,"Thank you. I used ""en_core_sci_sm"" == en_core_sci_sm-0.2.0.tar.gz and the issue went away. I will close this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/92
https://github.com/allenai/scispacy/pull/96:33,performance,memor,memory,33,"Training still requires 150GB of memory, but addressing that is a different PR",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/96
https://github.com/allenai/scispacy/pull/96:33,usability,memor,memory,33,"Training still requires 150GB of memory, but addressing that is a different PR",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/96
https://github.com/allenai/scispacy/issues/100:920,availability,error,errors,920,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1448,deployability,api,api,1448,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:171,energy efficiency,model,model,171,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:449,energy efficiency,model,model,449,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:469,energy efficiency,load,loaded,469,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1448,integrability,api,api,1448,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1448,interoperability,api,api,1448,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:207,performance,perform,performance,207,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:469,performance,load,loaded,469,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:920,performance,error,errors,920,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:920,safety,error,errors,920,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1153,safety,compl,complicated,1153,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1199,safety,compl,complex,1199,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:171,security,model,model,171,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:449,security,model,model,449,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1045,security,token,tokenization,1045,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1153,security,compl,complicated,1153,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1184,security,token,tokenizer,1184,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1199,security,compl,complex,1199,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1473,security,token,tokens,1473,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:1509,security,token,token,1509,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:207,usability,perform,performance,207,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:265,usability,custom,custom,265,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:920,usability,error,errors,920,"Thanks for the detailed issue! We found that including the segmentation rules (1) was not as necessary with the retrained parser (2) decreased the processing speed of the model and (3) sometimes hurt parser performance a bit. For the sentence splitting issues, the custom sentence segmentation rules that we wrote can be found [here](https://github.com/allenai/scispacy/blob/master/scispacy/custom_sentence_segmenter.py), and they can be added to a model that you have loaded like so. ```. from spacy.language import Language. from scispacy.custom_sentence_segmenter import combined_rule_sentence_segmenter. Language.factories['combined_rule_sentence_segmenter'] = lambda nlp, **cfg: combined_rule_sentence_segmenter	. nlp.add_pipe(nlp.create_pipe('combined_rule_sentence_segmenter'), first=True). ```. You can also easily add your own rules to this function if you find that existing rules don't cover the segmentation errors that you are seeing. This may have side effects on the parser, but we have not thoroughly experimented with that. The tokenization issues (I think of your examples it is just this one ""p65.c-Rel"" / ""p50-.c-Rel"") might be more complicated to fix because the tokenizer is a complex set of regular expressions. However, one thing you could try is, after creating a doc using `en_core_sci_md`, post process the doc (or add your own pipe similarly to the sentence segmenter pipe) and make use of [Span.merge](https://spacy.io/api/span#merge) to merge tokens that you think should be one token.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:94,safety,except,exceptions,94,"Closing, as the new sentence segmenter fixes many of these problems, and it's possible to add exceptions to the tokenizer for particularly nefarious cases.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/issues/100:112,security,token,tokenizer,112,"Closing, as the new sentence segmenter fixes many of these problems, and it's possible to add exceptions to the tokenizer for particularly nefarious cases.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/100
https://github.com/allenai/scispacy/pull/110:41,interoperability,share,share,41,"@DeNeutoy @danielkingai2 , this PR is to share ideas of features. None of the additional features here is helpful",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/110
https://github.com/allenai/scispacy/pull/110:106,usability,help,helpful,106,"@DeNeutoy @danielkingai2 , this PR is to share ideas of features. None of the additional features here is helpful",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/110
https://github.com/allenai/scispacy/issues/116:48,availability,down,downsides,48,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:421,deployability,api,api,421,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:77,energy efficiency,model,model,77,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:604,energy efficiency,model,model,604,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:776,energy efficiency,model,model,776,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:421,integrability,api,api,421,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:845,integrability,topic,topic,845,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:235,interoperability,specif,specifically,235,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:289,interoperability,specif,specific,289,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:421,interoperability,api,api,421,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:77,security,model,model,77,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:486,security,ident,identification,486,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:604,security,model,model,604,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/116:776,security,model,model,776,"Thanks for the issue! Unfortunately, one of the downsides of using a trained model is that it is sometimes wrong. I would have to dig into the data further to see if there might be something weird going on with the annotations for arm specifically or not. . If there are a small number of specific cases that would be useful to fix for your dataset/task, you may want to check out spaCy's [Entity Ruler](https://spacy.io/api/entityruler). This would allow you to write rules for entity identification, for which you could ensure their correctness. You could also potentially try further training the NER model with labeled cases that match the mistakes you see, as described here: https://spacy.io/usage/training#ner. Otherwise, unfortunately mistakes are part of any trained model. You can see this thread on spaCy for their discussion of this topic as well: https://github.com/explosion/spaCy/issues/3052. Thanks again for your interest in this library!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/116
https://github.com/allenai/scispacy/issues/120:4,availability,down,downloading,4,Was downloading the wrong model. Sorry.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/120
https://github.com/allenai/scispacy/issues/120:26,energy efficiency,model,model,26,Was downloading the wrong model. Sorry.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/120
https://github.com/allenai/scispacy/issues/120:26,security,model,model,26,Was downloading the wrong model. Sorry.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/120
https://github.com/allenai/scispacy/pull/123:5,deployability,releas,release,5,Last release went wrong ooops,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/123
https://github.com/allenai/scispacy/issues/126:57,energy efficiency,model,models,57,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:89,safety,test,test,89,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:117,safety,valid,validation,117,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:147,safety,safe,safe,147,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:57,security,model,models,57,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:117,security,validat,validation,117,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:89,testability,test,test,89,"Oh, naturally we only used the training set to train the models on - the development and test sets are only used for validation. You're definitely safe to use them on medmentions!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:30,integrability,pub,publish,30,"Great! If there's a chance to publish paper with the aid of scispacy, I'll cite the original paper. Again, thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:69,energy efficiency,model,models,69,"Great - also please let us know any feedback you have when using the models, or additional features you'd love. . I'm not sure what task you are working on (entity linking maybe?) but you might also be interested in the knowledge base we have curated from UMLS - it is quite large and covers 99.9% of the entities which occur in the MedMentions annotations. You can read more about it here:. https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:69,security,model,models,69,"Great - also please let us know any feedback you have when using the models, or additional features you'd love. . I'm not sure what task you are working on (entity linking maybe?) but you might also be interested in the knowledge base we have curated from UMLS - it is quite large and covers 99.9% of the entities which occur in the MedMentions annotations. You can read more about it here:. https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:36,usability,feedback,feedback,36,"Great - also please let us know any feedback you have when using the models, or additional features you'd love. . I'm not sure what task you are working on (entity linking maybe?) but you might also be interested in the knowledge base we have curated from UMLS - it is quite large and covers 99.9% of the entities which occur in the MedMentions annotations. You can read more about it here:. https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:532,deployability,depend,depends,532,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:443,energy efficiency,model,models,443,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:532,integrability,depend,depends,532,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:532,modifiability,depend,depends,532,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:157,performance,perform,performs,157,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:262,safety,prevent,prevented,262,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:532,safety,depend,depends,532,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:200,security,token,tokenization,200,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:262,security,preven,prevented,262,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:443,security,model,models,443,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:532,testability,depend,depends,532,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:157,usability,perform,performs,157,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:228,usability,custom,custom,228,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/126:410,usability,feedback,feedback,410,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking. I'll try that, thanks! BTW, In trying sbd with `en_core_sci_md`, scispacy performs well. However, there's some minor tokenization problem and if custom rules are added, it can be prevented. https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16. (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126
https://github.com/allenai/scispacy/issues/128:1195,deployability,version,version,1195,"ad('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1221,deployability,version,version,1221,"abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the AN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1476,deployability,version,version,1476,"drogen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal At",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1502,deployability,version,version,1502,".. SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3787,deployability,modul,modulation,3787,"rolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3809,deployability,depend,dependent,3809,"s, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5075,deployability,version,version,5075,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5107,deployability,instal,installed,5107,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5126,deployability,version,version,5126,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5166,deployability,instal,installed,5166,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:197,energy efficiency,load,load,197,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:987,energy efficiency,load,load,987,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1163,energy efficiency,estimat,estimator,1163,"ionDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1445,energy efficiency,estimat,estimator,1445,"yglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:2735,energy efficiency,reduc,reduction,2735," by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy. CUI: C0026846, Name: Muscular Atrophy. Definition: Derangement in size and number of muscle fibers occurring with aging, reduction in blood supply, or following immobilization, prolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the h",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1195,integrability,version,version,1195,"ad('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1221,integrability,version,version,1221,"abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the AN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1476,integrability,version,version,1476,"drogen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal At",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1502,integrability,version,version,1502,".. SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3809,integrability,depend,dependent,3809,"s, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5075,integrability,version,version,5075,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5126,integrability,version,version,5126,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1101,modifiability,pac,packages,1101,"> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1195,modifiability,version,version,1195,"ad('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1221,modifiability,version,version,1221,"abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the AN",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1383,modifiability,pac,packages,1383,"ed motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spina",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1476,modifiability,version,version,1476,"drogen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal At",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1502,modifiability,version,version,1502,".. SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1703,modifiability,inherit,inherited,1703,"brv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy. CUI: C0026846, Name: Muscular Atrophy. Definition: Derangement in size and number of muscle fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3787,modifiability,modul,modulation,3787,"rolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3809,modifiability,depend,dependent,3809,"s, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:4128,modifiability,polymorph,polymorphic,4128,"e atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5075,modifiability,version,version,5075,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:5126,modifiability,version,version,5126,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:197,performance,load,load,197,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:987,performance,load,load,987,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3245,performance,content,content,3245," T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy. CUI: C0026846, Name: Muscular Atrophy. Definition: Derangement in size and number of muscle fibers occurring with aging, reduction in blood supply, or following immobilization, prolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, huma",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:87,safety,detect,detector,87,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1306,safety,risk,risk,1306," >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1587,safety,risk,risk,1587,"n""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Link",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3787,safety,modul,modulation,3787,"rolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3809,safety,depend,dependent,3809,"s, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:4033,safety,test,testicular,4033,", Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:87,security,detect,detector,87,"Thanks for the report! I don't seem to be able to reproduce this. For the abbreviation detector:. ```. >>> import spacy. >>> from scispacy.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ...",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1306,security,risk,risk,1306," >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1587,security,risk,risk,1587,"n""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Link",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3787,testability,modula,modulation,3787,"rolonged weightlessness, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:3809,testability,depend,dependent,3809,"s, malnutrition, and particularly in denervation. TUI(s): T046. Aliases (abbreviated, total: 51): . Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, Muscular Atrophy, muscular atrophy, Muscular atrophy, Muscular atrophy, Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:4033,testability,test,testicular,4033,", Muscular atrophy, Atrophies, Muscular, Muscular Atrophies. CUI: C0541794, Name: Skeletal muscle atrophy. Definition: A process, occurring in skeletal muscle, that is characterized by a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in th",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1131,usability,User,UserWarning,1131,"y.abbreviation import AbbreviationDetector. >>> nlp = spacy.load('en_core_sci_md'). >>> abbreviation_pipe = AbbreviationDetector(nlp). >>> nlp.add_pipe(abbreviation_pipe). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recess",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1312,usability,User,UserWarning,1312," nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inheriated motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1413,usability,User,UserWarning,1413,"y the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> print(""Abbreviation"", ""\t"", ""Definition""). Abbreviation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linke",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:1593,usability,User,UserWarning,1593,"viation Definition. >>> for abrv in doc._.abbreviations:. ... print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""). ... . SBMA (33, 34) Spinal and bulbar muscular atrophy. SBMA (6, 7) Spinal and bulbar muscular atrophy. AR (29, 30) androgen receptor. ```. For the entity linker:. ```. >>> import spacy. >>> from scispacy.umls_linking import UmlsEntityLinker. >>> nlp = spacy.load('en_core_sci_md'). >>> linker = UmlsEntityLinker(). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). /home/daniel/miniconda3/envs/scispacy/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk. UserWarning). >>> nlp.add_pipe(linker). >>> doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \. ... inherited motor neuron disease caused by the expansion \. ... of a polyglutamine tract within the androgen receptor (AR). \. ... SBMA can be caused by this easily.""). >>> entity = doc.ents[1]. >>> print(""Name: "", entity). Name: bulbar muscular atrophy. >>> for umls_ent in entity._.umls_ents:. ... print(linker.umls.cui_to_entity[umls_ent[0]]). ... . CUI: C1839259, Name: Bulbo-Spinal Atrophy, X-Linked. Definition: An X-linked recessive form of spinal muscular atrophy. It is due to a mutation of the gene encoding the ANDROGEN RECEPTOR. TUI(s): T047. Aliases (abbreviated, total: 50): . Bulbo-Spinal Atrophy, X-Linked, Bulbo-Spinal Atrophy, X-Linked, Atrophies, X-Linked Bulbo-Spinal, Bulbo Spinal Atrophy, X Linked, Bulbo-Spinal Atrophies, X-Linked, X-Linked Bulbo-Spinal Atrophies, Atrophy, X-Linked Bulbo-Spinal, X Linked Bulbo Spinal Atrophy, X-Linked Bulbo-Spinal Atrophy, X-Linked Bulbo-S",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:4341,usability,progress,progressive,4341,"a decrease in protein content, fiber diameter, force production and fatigue resistance in response to different conditions such as starvation, aging and disuse. [GOC:mtg_muscle]. TUI(s): T046. Aliases: (total: 9): . Skeletal muscle atrophy, ATROPHY SKELETAL MUSCLE, skeletal muscle atrophy, Muscular atrophy, Muscle atrophy, Amyotrophy, Muscle wasting, Amyotrophy involving the extremities, Muscle hypotrophy. CUI: C1447749, Name: AR protein, human. Definition: Androgen receptor (919 aa, ~99 kDa) is encoded by the human AR gene. This protein plays a role in the modulation of steroid-dependent gene transcription. TUI(s): T116, T192. Aliases (abbreviated, total: 16): . AR protein, human, Androgen Receptor, Dihydrotestosterone Receptor, AR, DHTR, NR3C4, AR (androgen receptor (dihydrotestosterone receptor; testicular feminization; spinal and bulbar muscular atrophy; Kennedy disease)) protein, human, polymorphic X-linked androgen receptor, human, androgen receptor, human, spinal and bulbar muscular atrophy protein, human. CUI: C0026847, Name: Spinal Muscular Atrophy. Definition: A group of disorders marked by progressive degeneration of motor neurons in the spinal cord resulting in weakness and muscular atrophy, usually without evidence of injury to the corticospinal tracts. Diseases in this category include Werdnig-Hoffmann disease and later onset SPINAL MUSCULAR ATROPHIES OF CHILDHOOD, most of which are hereditary. (Adams et al., Principles of Neurology, 6th ed, p1089). TUI(s): T047. Aliases (abbreviated, total: 39): . Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, Spinal Muscular Atrophy, MUSCULAR ATROPHY, SPINAL, atrophy muscular spinal, atrophy spinal muscular, muscular atrophy spinal, muscular spinal atrophy. ```. Could you paste in the exact code that you ran? Also what version of scispacy do you have installed and what version of `en_core_sci_md` do you have installed (you can get this information using `pip list`?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:69,availability,state,statements,69,Thank you for the prompt response! My output is blank for both `for` statements. And I have version `0.2.0` for both `_sm` and `_md`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:92,deployability,version,version,92,Thank you for the prompt response! My output is blank for both `for` statements. And I have version `0.2.0` for both `_sm` and `_md`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:69,integrability,state,statements,69,Thank you for the prompt response! My output is blank for both `for` statements. And I have version `0.2.0` for both `_sm` and `_md`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:92,integrability,version,version,92,Thank you for the prompt response! My output is blank for both `for` statements. And I have version `0.2.0` for both `_sm` and `_md`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:92,modifiability,version,version,92,Thank you for the prompt response! My output is blank for both `for` statements. And I have version `0.2.0` for both `_sm` and `_md`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:43,performance,time,time,43,"Sorry, I ran this again and it worked this time. I wonder if the linking just doesn't work the first time you try.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:101,performance,time,time,101,"Sorry, I ran this again and it worked this time. I wonder if the linking just doesn't work the first time you try.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:78,reliability,doe,doesn,78,"Sorry, I ran this again and it worked this time. I wonder if the linking just doesn't work the first time you try.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:171,availability,consist,consistently,171,"hmm, thats a bit surprising, I definitely haven't experience linking not working the first time you use it. I wonder what was going on there. If you have a procedure that consistently reproduces this issue please let us know! Otherwise, I'll go ahead and close this issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:91,performance,time,time,91,"hmm, thats a bit surprising, I definitely haven't experience linking not working the first time you use it. I wonder what was going on there. If you have a procedure that consistently reproduces this issue please let us know! Otherwise, I'll go ahead and close this issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:50,usability,experien,experience,50,"hmm, thats a bit surprising, I definitely haven't experience linking not working the first time you use it. I wonder what was going on there. If you have a procedure that consistently reproduces this issue please let us know! Otherwise, I'll go ahead and close this issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:171,usability,consist,consistently,171,"hmm, thats a bit surprising, I definitely haven't experience linking not working the first time you use it. I wonder what was going on there. If you have a procedure that consistently reproduces this issue please let us know! Otherwise, I'll go ahead and close this issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:255,usability,close,close,255,"hmm, thats a bit surprising, I definitely haven't experience linking not working the first time you use it. I wonder what was going on there. If you have a procedure that consistently reproduces this issue please let us know! Otherwise, I'll go ahead and close this issue.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:109,deployability,pipelin,pipeline,109,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:161,deployability,pipelin,pipelines,161,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:171,deployability,automat,automatically,171,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:119,energy efficiency,Current,Currently,119,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:109,integrability,pipelin,pipeline,109,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:161,integrability,pipelin,pipelines,161,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:145,modifiability,pac,packaged,145,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:66,safety,detect,detector,66,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:66,security,detect,detector,66,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:171,testability,automat,automatically,171,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:310,usability,close,close,310,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/128:359,usability,experien,experience,359,"@geetickachauhan Was it possible that you hadn't added the entity detector and the abbreviation pipes to the pipeline? Currently they don't come packaged in the pipelines automatically. This can be a particular problem in Jupyter notebooks for example, where it's unclear if a cell has been reloaded etc. I'll close this for now but definitely re-open if you experience it again or can make a small script to reproduce it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/128
https://github.com/allenai/scispacy/issues/130:43,deployability,depend,dependency,43,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:311,deployability,depend,dependencies,311,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:248,energy efficiency,current,currently,248,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:43,integrability,depend,dependency,43,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:311,integrability,depend,dependencies,311,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:43,modifiability,depend,dependency,43,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:311,modifiability,depend,dependencies,311,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:454,modifiability,pac,package,454,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:43,safety,depend,dependency,43,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:311,safety,depend,dependencies,311,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:43,testability,depend,dependency,43,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:311,testability,depend,dependencies,311,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:278,usability,custom,custom,278,"Hmm I believe this would work (i.e have no dependency at all on the actual scispacy library) if we removed this line: . https://github.com/allenai/scispacy/blob/master/proto_model/__init__.py#L7. This might actually be quite a good solution, as we currently don't even use this custom segmenter. Yes, the other dependencies are just for the entity linking/candidate generation step, which is a little more involved. . Let me think a bit about making the package not rely on these.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:150,integrability,compon,component,150,"seems fine to just remove that line, i think. the sentence segmenter (even though it is unused at the moment) could just be the same type of optional component as the linker and abbreviation detector. And that could be the general paradigm for all the extra pipes, that they are optional stuff that can be added. That seems fine to me.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:150,interoperability,compon,component,150,"seems fine to just remove that line, i think. the sentence segmenter (even though it is unused at the moment) could just be the same type of optional component as the linker and abbreviation detector. And that could be the general paradigm for all the extra pipes, that they are optional stuff that can be added. That seems fine to me.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:150,modifiability,compon,component,150,"seems fine to just remove that line, i think. the sentence segmenter (even though it is unused at the moment) could just be the same type of optional component as the linker and abbreviation detector. And that could be the general paradigm for all the extra pipes, that they are optional stuff that can be added. That seems fine to me.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:191,safety,detect,detector,191,"seems fine to just remove that line, i think. the sentence segmenter (even though it is unused at the moment) could just be the same type of optional component as the linker and abbreviation detector. And that could be the general paradigm for all the extra pipes, that they are optional stuff that can be added. That seems fine to me.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:191,security,detect,detector,191,"seems fine to just remove that line, i think. the sentence segmenter (even though it is unused at the moment) could just be the same type of optional component as the linker and abbreviation detector. And that could be the general paradigm for all the extra pipes, that they are optional stuff that can be added. That seems fine to me.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:209,availability,error,error,209,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:80,deployability,instal,installing,80,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:128,deployability,instal,installing,128,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:249,deployability,instal,installed,249,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:49,energy efficiency,load,load,49,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:65,energy efficiency,model,models,65,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:157,energy efficiency,load,loading,157,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:181,energy efficiency,model,models,181,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:167,integrability,coupl,couple,167,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:215,integrability,messag,message,215,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:215,interoperability,messag,message,215,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:167,modifiability,coupl,couple,167,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:49,performance,load,load,49,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:157,performance,load,loading,157,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:209,performance,error,error,209,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:209,safety,error,error,209,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:65,security,model,models,65,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:181,security,model,models,181,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:167,testability,coupl,couple,167,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/130:209,usability,error,error,209,"Just wanted to check if it is indeed possible to load `scispacy` models without installing `scispacy`. I've been having trouble installing `nmslib`. I tried loading a couple of the models, but was met with an error message saying `scispacy` was not installed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/130
https://github.com/allenai/scispacy/issues/132:53,deployability,updat,updated,53,"raised in #130 and fixed by #131, but docs should be updated only after we make a new release.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/132
https://github.com/allenai/scispacy/issues/132:86,deployability,releas,release,86,"raised in #130 and fixed by #131, but docs should be updated only after we make a new release.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/132
https://github.com/allenai/scispacy/issues/132:53,safety,updat,updated,53,"raised in #130 and fixed by #131, but docs should be updated only after we make a new release.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/132
https://github.com/allenai/scispacy/issues/132:53,security,updat,updated,53,"raised in #130 and fixed by #131, but docs should be updated only after we make a new release.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/132
https://github.com/allenai/scispacy/issues/132:23,usability,close,closed,23,Is this supposed to be closed?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/132
https://github.com/allenai/scispacy/issues/134:2424,availability,sli,slight,2424,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:662,deployability,version,version,662,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,deployability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1290,deployability,Configurat,Configuration,1290,"hey are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:125,energy efficiency,predict,predicted,125,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:407,energy efficiency,predict,predicted,407,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:424,energy efficiency,model,model,424,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:910,energy efficiency,measur,measuring,910,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1762,energy efficiency,predict,predicted,1762,"in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is perform",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2048,energy efficiency,measur,measure,2048,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2490,energy efficiency,predict,predicted,2490,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:662,integrability,version,version,662,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,integrability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:889,integrability,abstract,abstract,889,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1290,integrability,Configur,Configuration,1290,"hey are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1324,integrability,sub,substitution,1324,"wever, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1698,integrability,sub,substitution,1698,"rated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 reall",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:26,interoperability,specif,specifically,26,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:392,interoperability,semant,semantic,392,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,interoperability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2824,interoperability,specif,specific,2824,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:662,modifiability,version,version,662,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,modifiability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:889,modifiability,abstract,abstract,889,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1290,modifiability,Configur,Configuration,1290,"hey are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:57,performance,perform,performance,57,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:359,performance,perform,performance,359,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2594,performance,perform,performance,2594,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2760,performance,perform,performing,2760,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:342,reliability,doe,does,342,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,reliability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1132,reliability,pra,practice,1132,"ention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2424,reliability,sli,slight,2424,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:105,safety,detect,detector,105,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:125,safety,predict,predicted,125,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:208,safety,detect,detection,208,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:272,safety,reme,remember,272,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:407,safety,predict,predicted,407,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1762,safety,predict,predicted,1762,"in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is perform",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2490,safety,predict,predicted,2490,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:105,security,detect,detector,105,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:208,security,detect,detection,208,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:424,security,model,model,424,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,security,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1290,security,Configur,Configuration,1290,"hey are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:699,testability,integr,integrated,699,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2576,testability,understand,understanding,2576,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:57,usability,perform,performance,57,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:359,usability,perform,performance,359,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:785,usability,Document,Document,785,"Hi Dan,. Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1004,usability,document,document,1004," Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1077,usability,document,document,1077,"d with the mention span detector (i.e using predicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, wh",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1125,usability,UI,UI,1125,"redicted mention spans and not gold mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the go",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1159,usability,help,helps,1159,"mention spans)? Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. You are correct that the st21pv is not used at all. Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭 . I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. #### Document level scores. This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2037,usability,help,helpful,2037,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2594,usability,perform,performance,2594,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2760,usability,perform,performing,2760,"l_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. *Base Configuration*. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. K | Base | +Noun Chunks | Gold Mentions. -- | -- | -- | --. 1 | 57.34 | 60.41 | 72.35. 2 | 62.6 | 65.96 | 78.52. 10 | 71.73 | 75.83 | 86.33. 40 | 78.22 | 82.3 | 91.13. 60 | 79.85 | 83.91 | 92.31. 80 | 80.65 | 84.66 | 92.79. 100 | 81.17 | 85.19 | 93.13. #### Soft Matching Mention level scores. - No abbreviation substitution. - Mentions generated based on `en_core_web_md`. - predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. K | Recall@K. -- | --. 1 | 49.36. 2 | 54.27. 10 | 62.64. 40 | 68.62. 60 | 70.09. 80 | 70.91. 100 | 71.49. Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. Hopefully those numbers are useful for understanding the performance! There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! Mark.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:285,availability,sli,slightly,285,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1368,energy efficiency,predict,predicted,1368,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1117,integrability,coupl,couple,1117,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1241,interoperability,semant,semantic,1241,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1117,modifiability,coupl,couple,1117,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1610,modifiability,scenario,scenario,1610,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1716,performance,perform,performance,1716,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:285,reliability,sli,slightly,285,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1357,reliability,doe,does,1357,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1583,reliability,Doe,Doesn,1583,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:108,safety,detect,detection,108,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:745,safety,detect,detection,745,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1368,safety,predict,predicted,1368,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:108,security,detect,detection,108,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:745,security,detect,detection,745,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1117,testability,coupl,couple,1117,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1170,testability,understand,understanding,1170,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1534,testability,understand,understand,1534,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1770,testability,understand,understand,1770,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:63,usability,help,helpful,63,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:549,usability,document,document-level,549,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:880,usability,close,closely,880,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1716,usability,perform,performance,1716,"Hi Mark,. Thanks for such a detailed response, it's definitely helpful! Yes, I'm interested in mention span detection and entity linking (concept recognition), under similar conditions as the MedMentions paper (though more interested on the full dataset too). This evaluation is under slightly different conditions from the MedMentions paper (they focused on st21pv, 'hard' matches) but these results seem to suggest that your lightweight approach (even in alpha) can be competitive with the paper's very impractical baseline (maybe even better for document-level). I'm also realizing that evaluating EL, particularly for MedMentions, won't be as straightforward as it should be... P/R/F1 are a bit harsh indeed, specially considering that span detection in this dataset seems quite challenging. Even with gold spans I expect that recall@1 should be very hard given the number of closely related concepts (incl. many without annotations in the training set!). In any case, MedMentions seems like a great dataset otherwise, it's only really lacking detailed evaluation results. It's still very recent though. I have a couple of questions on this, mostly to make sure I'm understanding everything:. - When you say that 'we don't actually use [semantic types] at all yet in the linker', are you referring to the concept annotations on MedMentions ? The linker does match predicted spans to UMLS concepts (and thus sematic types), the difference here is that you're only considering the aliases from the UMLS KB, right? - I don't think I understand the normalised recall@1 you describe. Doesn't evaluating under a scenario where you throw away spans that didn't include the gold mention (concept) lead to overestimating performance instead of underestimating? I also didn't understand how the approximation through K works, and where the accuracy comes from. Thanks once again, this is great! Dan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:215,availability,down,down,215,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:582,deployability,contain,contains,582,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,deployability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:856,energy efficiency,current,currently,856,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1314,energy efficiency,current,currently,1314,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1472,energy efficiency,model,models,1472,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:696,integrability,sub,subset,696,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1140,integrability,sub,subset,1140,"iases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,integrability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1640,integrability,sub,subset,1640,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1767,integrability,sub,subset,1767,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1947,integrability,sub,subset,1947,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2015,integrability,sub,subset,2015,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2087,integrability,sub,subset,2087,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2142,integrability,sub,subset,2142,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2216,integrability,coupl,couple,2216,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,interoperability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1704,interoperability,specif,specific,1704,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,modifiability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2216,modifiability,coupl,couple,2216,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:725,performance,perform,performance,725,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,reliability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:183,safety,compl,complicated,183,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:320,safety,detect,detection,320,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:429,safety,detect,detection,429,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1460,safety,compl,complicated,1460,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1984,safety,detect,detection,1984,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:183,security,compl,complicated,183,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:320,security,detect,detection,320,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:429,security,detect,detection,429,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,security,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1460,security,compl,complicated,1460,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1472,security,model,models,1472,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1984,security,detect,detection,1984,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1340,testability,integr,integration,1340,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1447,testability,hook,hook,1447,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2060,testability,coverag,coverage,2060,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2216,testability,coupl,couple,2216,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:397,usability,clear,clear,397,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:725,usability,perform,performance,725,"No problem! In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2040,usability,help,helpful,2040,"ng:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions. - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:232,deployability,updat,update,232,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:27,safety,detect,detector,27,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:232,safety,updat,update,232,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:27,security,detect,detector,27,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:232,security,updat,update,232,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:93,usability,indicat,indication,93,"Ok, i'm training a mention detector now. It should be done soon, that will give a good first indication about whether removing the other types makes everything easier. The other stuff (with the kb) might take a bit longer, but i'll update this thread as I go.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:353,energy efficiency,model,model,353,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:417,energy efficiency,measur,measure-overall,417,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:104,integrability,coupl,couple,104,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:292,interoperability,format,format,292,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:104,modifiability,coupl,couple,104,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:322,safety,detect,detector,322,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:322,security,detect,detector,322,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:353,security,model,model,353,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:377,security,sign,significant,377,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:104,testability,coupl,couple,104,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:164,usability,help,help,164,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:458,usability,confirm,confirm,458,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:572,usability,progress,progress,572,"Hi Mark,. Thanks for keeping on providing more details. We're going to be working on this over the next couple of weeks (with some breaks though), so I think I can help with those evaluations (we should need them too). I've started experimenting with scibert, converting MedMentions to conll format and training a mention detector with it's default NER model, and I'm seeing a significant improvement on F1 (""test_f1-measure-overall"": 0.7646). Still need to confirm a few things though (e.g. https://github.com/allenai/scibert/issues/50). I'll let you know more as I make progress too. Still not sure if I'm more interested in the full data or st21pv, but am interested to know your results on st21pv in any case. Thanks",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2435,availability,sli,slight,2435,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2911,availability,avail,available,2911,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:693,deployability,version,version,693,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,deployability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:133,energy efficiency,predict,predicted,133,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:421,energy efficiency,predict,predicted,421,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:438,energy efficiency,model,model,438,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:949,energy efficiency,measur,measuring,949,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1776,energy efficiency,predict,predicted,1776,"mall differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2059,energy efficiency,measur,measure,2059,"aningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2501,energy efficiency,predict,predicted,2501,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2992,energy efficiency,model,model,2992,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:693,integrability,version,version,693,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,integrability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:928,integrability,abstract,abstract,928,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1375,integrability,sub,substitution,1375,"nce when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1708,integrability,sub,substitution,1708,", which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:34,interoperability,specif,specifically,34,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:406,interoperability,semant,semantic,406,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,interoperability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2847,interoperability,specif,specific,2847,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:693,modifiability,version,version,693,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,modifiability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:928,modifiability,abstract,abstract,928,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:65,performance,perform,performance,65,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:373,performance,perform,performance,373,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2611,performance,perform,performance,2611,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2783,performance,perform,performing,2783,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:356,reliability,doe,does,356,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,reliability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1171,reliability,pra,practice,1171,"on spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2435,reliability,sli,slight,2435,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2911,reliability,availab,available,2911,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:113,safety,detect,detector,113,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:133,safety,predict,predicted,133,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:222,safety,detect,detection,222,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:286,safety,reme,remember,286,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:421,safety,predict,predicted,421,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1776,safety,predict,predicted,1776,"mall differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2501,safety,predict,predicted,2501,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2911,safety,avail,available,2911,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:3050,safety,avoid,avoid,3050,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:3064,safety,detect,detection,3064,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:113,security,detect,detector,113,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:222,security,detect,detection,222,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:438,security,model,model,438,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,security,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2911,security,availab,available,2911,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2992,security,model,model,2992,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:3064,security,detect,detection,3064,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:730,testability,integr,integrated,730,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2593,testability,understand,understanding,2593,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:65,usability,perform,performance,65,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:373,usability,perform,performance,373,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:822,usability,Document,Document,822,"> Hi Dan,. > . > Thanks - are you specifically interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1043,usability,document,document,1043,"interested in the performance when combined with the mention span detector (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1116,usability,document,document,1116,"r (i.e using predicted mention spans and not gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1164,usability,UI,UI,1164,"gold mention spans)? > . > Yes, the 69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:1198,usability,help,helps,1198,"69.26 F1 is the span detection F1 for the latest en_core_sci_md/sm (I can't actually remember which one, but they are essentially the same). However, this does not include performance when considering the semantic types predicted by the model, because we don't actually use those at all yet in the linker. > . > You are correct that the st21pv is not used at all. > . > Wow I did not know that TaggerOne requires 1TB RAM 😭 😭 😭. > . > I've added some statistics below based on our development version of the linker, which was not integrated into the library. As such there may be small differences in numbers:. > . > #### Document level scores. > This evaluation is at the _document level_, based on the ""bag of entities"" in an abstract. I.e we are measuring the recall of gold concepts in the aggregated list of all mention candidates in the document. This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2048,usability,help,helpful,2048," This is meaningful because quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to av",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2611,usability,perform,performance,2611,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:2783,usability,perform,performing,2783,"cause quite often, we actually care about document level metrics (search, displaying in a UI) in practice. Additionally, it helps with the abbreviation problem, because quite often a mention will be defined in full and then used as an abbreviation. > . > _Base Configuration_. > . > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > . > K	Base	+Noun Chunks	Gold Mentions. > 1	57.34	60.41	72.35. > 2	62.6	65.96	78.52. > 10	71.73	75.83	86.33. > 40	78.22	82.3	91.13. > 60	79.85	83.91	92.31. > 80	80.65	84.66	92.79. > 100	81.17	85.19	93.13. > #### Soft Matching Mention level scores. > * No abbreviation substitution. > * Mentions generated based on `en_core_web_md`. > * predicted mentions are compared based on a ""soft overlap"", meaning that we take it to be a match with any gold mention with which it overlaps at all. > . > K	Recall@K. > 1	49.36. > 2	54.27. > 10	62.64. > 40	68.62. > 60	70.09. > 80	70.91. > 100	71.49. > Finally, it's also helpful to measure _normalised_ recall@1, where we consider only results for which the gold mention is present in the generated candidates for a given K. You can get an approximation of this just by dividing K=1/K=N (this is not quite accurate as it's possible that when we throw away entities which we can't generate candidates for, the set that is left is easier overall, so this is a slight underestimate probably). E.g normalised soft matching with predicted mentions and K=80 == 69.6% Accuracy. > . > Hopefully those numbers are useful for understanding the performance! > . > There are many pitfalls when evaluating EL systems, it's really pretty annoying. P,R and F1 really don't give you enough of an idea of how the system is performing as a whole. Let me know if you need to know anything specific about it! > . > Mark. Suppose I have the gold mentions available from Oracle. How can I provide the gold mentions to the entity linking model during inference? In other words, is there a way to avoid mention detection?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:90,availability,avail,available,90,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:177,deployability,modul,modular,177,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:305,energy efficiency,predict,predict,305,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:177,integrability,modular,modular,177,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:177,modifiability,modul,modular,177,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:437,modifiability,paramet,parameter,437,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:562,performance,time,time,562,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:90,reliability,availab,available,90,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:90,safety,avail,available,90,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:177,safety,modul,modular,177,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:305,safety,predict,predict,305,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:483,safety,test,testing,483,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:90,security,availab,available,90,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:177,testability,modula,modular,177,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:483,testability,test,testing,483,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:469,usability,document,documentation,469,"FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:92,availability,avail,available,92,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:185,deployability,modul,modular,185,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:313,energy efficiency,predict,predict,313,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:185,integrability,modular,modular,185,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:185,modifiability,modul,modular,185,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:445,modifiability,paramet,parameter,445,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:576,performance,time,time,576,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:92,reliability,availab,available,92,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:92,safety,avail,available,92,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:185,safety,modul,modular,185,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:313,safety,predict,predict,313,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:497,safety,test,testing,497,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:92,security,availab,available,92,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:185,testability,modula,modular,185,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:497,testability,test,testing,497,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:483,usability,document,documentation,483,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/134:739,usability,help,helpful,739,"> FYI, I've since explored this further on a short paper at ECIR. The (preliminary) code is available [here](https://github.com/danlou/MedLinker). > . > The MedLinker codebase is quite modular, and allows for providing gold mentions. It was used to produce results with gold mentions when writing the paper. The [predict()](https://github.com/danlou/MedLinker/blob/master/medlinker.py#L112) method in the MedLinker class accepts gold spans as a parameter. > . > There's little to no documentation/testing for that project though, and I haven't worked on that project for some time... > . > It's also possible that there's this functionality in scispacy, in which case you're probably better off relying on that. Thanks, Dan! This is super helpful.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134
https://github.com/allenai/scispacy/issues/136:122,energy efficiency,model,models,122,"@eliorc - if you take a look at the issue on SpaCy I linked to above, you can see that this is a general feature of spacy models and not specific to scispacy. Sorry about that! . It seems like the spacy vocab is also used as a cache, so it's possible that new words are added to it which were not originally included in the vocab. You might have better luck checking that the `.prob` attribute is > -20. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:137,interoperability,specif,specific,137,"@eliorc - if you take a look at the issue on SpaCy I linked to above, you can see that this is a general feature of spacy models and not specific to scispacy. Sorry about that! . It seems like the spacy vocab is also used as a cache, so it's possible that new words are added to it which were not originally included in the vocab. You might have better luck checking that the `.prob` attribute is > -20. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:227,performance,cach,cache,227,"@eliorc - if you take a look at the issue on SpaCy I linked to above, you can see that this is a general feature of spacy models and not specific to scispacy. Sorry about that! . It seems like the spacy vocab is also used as a cache, so it's possible that new words are added to it which were not originally included in the vocab. You might have better luck checking that the `.prob` attribute is > -20. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:122,security,model,models,122,"@eliorc - if you take a look at the issue on SpaCy I linked to above, you can see that this is a general feature of spacy models and not specific to scispacy. Sorry about that! . It seems like the spacy vocab is also used as a cache, so it's possible that new words are added to it which were not originally included in the vocab. You might have better luck checking that the `.prob` attribute is > -20. .",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:22,deployability,log,log,22,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:186,energy efficiency,model,model,186,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:243,performance,cach,cache,243,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:408,performance,time,time,408,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:22,safety,log,log,22,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:22,security,log,log,22,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:186,security,model,model,186,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:333,security,token,token,333,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:22,testability,log,log,22,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:440,usability,help,helpful,440,"Ah sorry - spacy uses log probabilties (for computational precision) for it's `.prob` attribute, which is calculated from unigram data. If a word is added to the vocabulary not when the model is first created (for instance, if it is used as a cache), it's probability will be set to the ""default"" value which is -20. So I think, any token with a prob greater than this value would have been seen at training time. Sorry if that's not super helpful....",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:119,testability,assert,assertion,119,"Seems legit, do we know for sure that any OOV vocabulary will be -20 or less? Is it save to use this condition for OOV assertion?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:72,usability,minim,minimum,72,"Umm, I think so? Not 100% sure about that, but I think it's the default minimum value, so maybe? Sorry I can't be more helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/136:119,usability,help,helpful,119,"Umm, I think so? Not 100% sure about that, but I think it's the default minimum value, so maybe? Sorry I can't be more helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/136
https://github.com/allenai/scispacy/issues/137:42,deployability,instal,install,42,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:121,deployability,instal,installation,121,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:479,deployability,instal,install,479,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:68,integrability,wrap,wrapper,68,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:68,interoperability,wrapper,wrapper,68,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:253,interoperability,specif,specific,253,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,safety,test,test,192,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:357,safety,test,testing,357,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,testability,test,test,192,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:357,testability,test,testing,357,"Thanks! I think nmslib is a bit tricky to install as it is a python wrapper to a C++ library. It seems like the order of installation here might have caused this problem. It's a bit tricky to test stuff like this because i'm fairly certain it is system specific. I'll leave this open and see if other people have had the same issue. If you're interested in testing it, you could try adding numpy to the requirements file at the top, cloning the repo and running `python setup.py install` in a clean conda environment. If that works for you, i'll definitely add numpy.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,availability,error,error,192,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:294,availability,error,error,294,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:311,availability,error,error,311,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:479,availability,error,error,479,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:44,deployability,depend,dependency,44,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:92,deployability,fail,failing,92,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:104,deployability,instal,installation,104,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:332,deployability,fail,failed,332,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:461,deployability,instal,installed,461,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:44,integrability,depend,dependency,44,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:44,modifiability,depend,dependency,44,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,performance,error,error,192,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:294,performance,error,error,294,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:311,performance,error,error,311,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:479,performance,error,error,479,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:92,reliability,fail,failing,92,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:332,reliability,fail,failed,332,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:5,safety,test,tested,5,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:44,safety,depend,dependency,44,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:86,safety,avoid,avoid,86,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,safety,error,error,192,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:294,safety,error,error,294,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:311,safety,error,error,311,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:479,safety,error,error,479,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:5,testability,test,tested,5,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:44,testability,depend,dependency,44,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:192,usability,error,error,192,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:294,usability,error,error,294,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:311,usability,error,error,311,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:318,usability,command,command,318,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:349,usability,statu,status,349,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:479,usability,error,error,479,"I've tested and indeed it requires numpy as dependency on top in the `setup.py` would avoid failing the installation, however I got stuck on the following point: . ```. nmslib.cc:16:10: fatal error: 'pybind11/pybind11.h' file not found. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. 1 error generated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ```. Which I""m not sure how to solve... cause `pybind11` is installed but the error persist. Looks more a problem of my system (however others might get as well).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:239,deployability,releas,release,239,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:279,deployability,instal,install,279,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:359,deployability,instal,install,359,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:296,energy efficiency,model,models,296,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:325,energy efficiency,load,load,325,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:325,performance,load,load,325,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:16,safety,test,testing,16,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:296,security,model,models,296,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:16,testability,test,testing,16,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:47,usability,help,help,47,"Hmm, thanks for testing that. This issue might help with the pybind11 thing. If this gets to be a big problem many people run into, i'll make nmslib optional, as it's only used for the linker which not everyone will use. FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Perhaps that would suit you? https://github.com/nmslib/nmslib/issues/307",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:170,availability,robust,robust,170,@DeNeutoy no problem. . I reported the issue because I though this problem might impact more people. . Actually I was looking into scispacy because I'm looking to a more robust sentence splitter working with scientific articles (e.g. ref and other noise usually result in wrong sentence segmentation). This doesn't seems to be the case.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:170,reliability,robust,robust,170,@DeNeutoy no problem. . I reported the issue because I though this problem might impact more people. . Actually I was looking into scispacy because I'm looking to a more robust sentence splitter working with scientific articles (e.g. ref and other noise usually result in wrong sentence segmentation). This doesn't seems to be the case.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:307,reliability,doe,doesn,307,@DeNeutoy no problem. . I reported the issue because I though this problem might impact more people. . Actually I was looking into scispacy because I'm looking to a more robust sentence splitter working with scientific articles (e.g. ref and other noise usually result in wrong sentence segmentation). This doesn't seems to be the case.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:170,safety,robust,robust,170,@DeNeutoy no problem. . I reported the issue because I though this problem might impact more people. . Actually I was looking into scispacy because I'm looking to a more robust sentence splitter working with scientific articles (e.g. ref and other noise usually result in wrong sentence segmentation). This doesn't seems to be the case.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:73,availability,failur,failure,73,"Oh dear, sorry about that - would you might posting a small selection of failure cases?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:73,deployability,fail,failure,73,"Oh dear, sorry about that - would you might posting a small selection of failure cases?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:73,performance,failur,failure,73,"Oh dear, sorry about that - would you might posting a small selection of failure cases?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:73,reliability,fail,failure,73,"Oh dear, sorry about that - would you might posting a small selection of failure cases?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,availability,failur,failure,113,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,deployability,fail,failure,113,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:25,energy efficiency,current,currently,25,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:248,energy efficiency,model,model,248,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,performance,failur,failure,113,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,reliability,fail,failure,113,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:186,safety,compl,completely,186,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:186,security,compl,completely,186,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:248,security,model,model,248,"ah don't worry... that's currently happening with any sentence segmenter actually... . I can try to provide some failure cases, however I think the need is more radical... like having a completely different dataset than newswires for training such model",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:78,availability,error,error,78,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:205,availability,ERROR,ERROR,205,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:579,availability,ERROR,ERROR,579,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1951,availability,error,error,1951,"o6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2085,availability,error,error,2085,"6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stor",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2175,availability,ERROR,ERROR,2175,"b/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3634,availability,ERROR,ERROR,3634,"em-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:30,deployability,instal,install,30,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:94,deployability,instal,installation,94,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:164,deployability,depend,depends,164,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:187,deployability,instal,install,187,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:321,deployability,instal,install-,321,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:615,deployability,build,build,615,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1008,deployability,build,building,1008,"e also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #inclu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1046,deployability,build,build,1046,"er also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1062,deployability,build,build,1062," in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. comp",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1100,deployability,build,build,1100,"ion does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1145,deployability,build,build,1145,"er library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ---------------",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1208,deployability,build,build,1208," Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nms",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1275,deployability,build,build,1275,"etuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1348,deployability,build,build,1348,"py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with sta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1676,deployability,instal,install-,1676,"result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1809,deployability,build,build,1809,".o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): fi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2106,deployability,fail,failed,2106,"p.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2182,deployability,Fail,Failed,2182,"arity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b17",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2189,deployability,build,building,2189,"arch. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2251,deployability,Build,Building,2251,"earch/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2297,deployability,Build,Building,2297,"6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finishe",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2488,deployability,Build,Building,2488,"PIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2536,deployability,Build,Building,2536,"local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2729,deployability,Build,Building,2729,"6_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 whic",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2774,deployability,Build,Building,2774,"ude/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2964,deployability,Build,Building,2964,"ybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treeli",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3014,deployability,Build,Building,3014,"bind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3209,deployability,Build,Building,3209,"b. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, bo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3256,deployability,Build,Building,3256,"wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3610,deployability,Fail,Failed,3610," directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing insta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3620,deployability,build,build,3620,": /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: d",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3752,deployability,Instal,Installing,3752,"up.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4608,deployability,instal,installation,4608,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4729,deployability,instal,install,4729,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4775,deployability,instal,install,4775,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4829,deployability,instal,install,4829,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4883,deployability,instal,install,4883,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4937,deployability,instal,install,4937,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1748,energy efficiency,core,core,1748,"/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for ga",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4507,energy efficiency,estimat,estimator-nightly,4507,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4588,energy efficiency,gpu,gpu,4588,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:164,integrability,depend,depends,164,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2270,integrability,wrap,wrapt,2270,"g build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2316,integrability,wrap,wrapt,2316,"y_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'don",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3573,integrability,wrap,wrapt,3573,"inished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3944,integrability,schema,schema,3944,"feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4452,integrability,wrap,wrapt,4452,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3738,interoperability,incompatib,incompatible,3738,"for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nms",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:164,modifiability,depend,depends,164,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1026,modifiability,extens,extension,1026,"nstall inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3773,modifiability,pac,packages,3773,"lding wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3783,modifiability,deco,decorator,3783," for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for n",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:78,performance,error,error,78,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:205,performance,ERROR,ERROR,205,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:579,performance,ERROR,ERROR,579,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1951,performance,error,error,1951,"o6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2085,performance,error,error,2085,"6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stor",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2175,performance,ERROR,ERROR,2175,"b/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2405,performance,cach,cache-,2405,"hod. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2646,performance,cach,cache-,2646,"lude/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2881,performance,cach,cache-,2881,"NFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3126,performance,cach,cache-,3126," 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3365,performance,cach,cache-,3365,"d in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3467,performance,network,networkx,3467,"cb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3491,performance,lock,lockfile,3491,"g wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3634,performance,ERROR,ERROR,3634,"em-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3794,performance,network,networkx,3794,"(setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: sti",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3820,performance,lock,lockfile,3820,"status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4588,performance,gpu,gpu,4588,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:107,reliability,doe,does,107,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2106,reliability,fail,failed,2106,"p.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2182,reliability,Fail,Failed,2182,"arity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b17",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3610,reliability,Fail,Failed,3610," directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing insta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:78,safety,error,error,78,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:164,safety,depend,depends,164,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:205,safety,ERROR,ERROR,205,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:212,safety,Compl,Complete,212,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:579,safety,ERROR,ERROR,579,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1951,safety,error,error,1951,"o6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2085,safety,error,error,2085,"6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stor",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2175,safety,ERROR,ERROR,2175,"b/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3634,safety,ERROR,ERROR,3634,"em-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:212,security,Compl,Complete,212,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:289,security,token,tokenize,289,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:369,security,token,tokenize,369,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3467,security,network,networkx,3467,"cb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3491,security,lock,lockfile,3491,"g wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3794,security,network,networkx,3794,"(setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: sti",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3820,security,lock,lockfile,3820,"status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4329,security,rsa,rsa,4329,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:164,testability,depend,depends,164,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:78,usability,error,error,78,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:205,usability,ERROR,ERROR,205,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:233,usability,command,command,233,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:465,usability,close,close,465,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:579,usability,ERROR,ERROR,579,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```. ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:. ERROR: running bdist_wheel. running build. running build_ext. creating tmp. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:1951,usability,error,error,1951,"o6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden. building 'nmslib' extension. creating build. creating build/temp.linux-x86_64-3.6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2085,usability,error,error,2085,"6. creating build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stor",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2092,usability,command,command,2092,"ing build/temp.linux-x86_64-3.6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in di",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2123,usability,statu,status,2123,"6/nmslib. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2175,usability,ERROR,ERROR,2175,"b/similarity_search. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space. creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2348,usability,statu,status,2348,"ld/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method. gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2589,usability,statu,status,2589,"on3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:2824,usability,statu,status,2824,"86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden. nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory. #include <pybind11/pybind11.h>. ^~~~~~~~~~~~~~~~~~~~~. compilation terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfil",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3069,usability,statu,status,3069," terminated. error: command 'gcc' failed with exit status 1. ----------------------------------------. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. Building wheel for wrapt (setup.py): started. Building wheel for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, i",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3308,usability,statu,status,3308," for wrapt (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd. Building wheel for absl-py (setup.py): started. Building wheel for absl-py (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srs",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:3634,usability,ERROR,ERROR,3634,"em-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48. Building wheel for gast (setup.py): started. Building wheel for gast (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd. Building wheel for termcolor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4391,usability,learn,learn,4391,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4971,usability,statu,status,4971,"lor (setup.py): started. Building wheel for termcolor (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6. Building wheel for PyYAML (setup.py): started. Building wheel for PyYAML (setup.py): finished with status 'done'. Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b. Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML. Failed to build nmslib. ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible. Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu. Found existing installation: docutils 0.15.2. Uninstalling docutils-0.15.2:. Successfully uninstalled docutils-0.15.2. Running setup.py install for nmslib: started. Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: still running... Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:80,deployability,instal,install,80,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,deployability,depend,dependency,114,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:157,deployability,instal,installed,157,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,integrability,depend,dependency,114,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:133,interoperability,bind,bindings,133,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,modifiability,depend,dependency,114,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:133,modifiability,bind,bindings,133,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:178,modifiability,extens,extension,178,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,safety,depend,dependency,114,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,testability,depend,dependency,114,"@searchivarius, I wonder if you have any ideas about this issue with the nmslib install? It looks like the pybind dependency for the bindings is not getting installed before the extension is built... . https://github.com/nmslib/nmslib/blob/master/python_bindings/setup.py#L43.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:60,deployability,instal,installation,60,Hi @DeNeutoy I am a bit lost here. Which part of the nmslib installation doesn't work and it's supposed to be working? Thank you!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:73,reliability,doe,doesn,73,Hi @DeNeutoy I am a bit lost here. Which part of the nmslib installation doesn't work and it's supposed to be working? Thank you!,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:93,availability,error,error,93,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:174,availability,error,error,174,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:224,availability,failur,failure,224,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:434,availability,failur,failure,434,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:224,deployability,fail,failure,224,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:269,deployability,build,build,269,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:278,deployability,fail,failing,278,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:426,deployability,instal,install,426,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:434,deployability,fail,failure,434,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:208,interoperability,share,share,208,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:93,performance,error,error,93,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:174,performance,error,error,174,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:224,performance,failur,failure,224,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:296,performance,time,time,296,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:434,performance,failur,failure,434,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:224,reliability,fail,failure,224,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:278,reliability,fail,failing,278,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:434,reliability,fail,failure,434,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:93,safety,error,error,93,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:174,safety,error,error,174,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:93,usability,error,error,93,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:174,usability,error,error,174,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:216,usability,minim,minimal,216,"Sorry - perhaps a bit premature to ask you as i'm not sure under exactly what conditions the error that @eliorc reported above occurs. @eliorc you said you'd reproduced that error in a Dockerfile - could you share a minimal failure, so we can figure out why the nmslib build is failing the first time? This may not be a bug (e.g it may require the `python3-dev` headers or something), but several people have come across this install failure now and it would be nice to be able to raise a better warning or common fixes etc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:32,deployability,updat,update,32,@DeNeutoy I'll try to do so and update here,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:32,safety,updat,update,32,@DeNeutoy I'll try to do so and update here,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:32,security,updat,update,32,@DeNeutoy I'll try to do so and update here,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:334,availability,error,error,334,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:151,deployability,fail,fails,151,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:169,deployability,instal,install,169,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:478,energy efficiency,gpu,gpu,478,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:334,performance,error,error,334,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:351,performance,network,networkx,351,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:478,performance,gpu,gpu,478,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:151,reliability,fail,fails,151,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:334,safety,error,error,334,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:351,security,network,networkx,351,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:334,usability,error,error,334,"Ok, I've reproduced this outside of the Docker too, the following requirements file reproduces this, on an Ubuntu 18.04, Python 3.6.8, pip 19.2.1 - It fails at first to install `nmslib`, and then succeeds when retrying in the end. requirements.txt, this is the original one, obviously I'm sure removing some libs will stil lcause the error:. ```bash. networkx. dvc. obonet. xlrd. pandas. scipy. numpy. matplotlib. seaborn. tqdm. gensim. spacy. scispacy==0.2.2. nltk. tensorflow-gpu==2.0.0-beta1. pymongo. sacred. gitpython. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:74,availability,error,error,74,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:370,availability,failur,failure,370,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,deployability,fail,failed,95,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:153,deployability,instal,installation,153,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:271,deployability,depend,dependency,271,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:370,deployability,fail,failure,370,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:166,integrability,event,eventually,166,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:271,integrability,depend,dependency,271,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:271,modifiability,depend,dependency,271,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:74,performance,error,error,74,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:370,performance,failur,failure,370,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,reliability,fail,failed,95,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:370,reliability,fail,failure,370,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:74,safety,error,error,74,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:271,safety,depend,dependency,271,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:271,testability,depend,dependency,271,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:63,usability,behavi,behavior,63,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:74,usability,error,error,74,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:81,usability,command,command,81,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:112,usability,statu,status,112,"I'm not sure that this is actually an issue. I've had the same behavior (`error: command 'gcc' failed with exit status 1` gets output along the way, but installation eventually succeeds), and haven't had any issues actually using `nmslib`. Maybe we just add `numpy` as a dependency in `setup.py` to address @lfoppiano's original problem and don't worry about that `gcc` failure along the way.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,availability,error,error,95,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:146,availability,down,downloading,146,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4,deployability,instal,installation,4,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:17,deployability,fail,failed,17,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:70,deployability,instal,install,70,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:243,deployability,instal,install,243,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:274,deployability,instal,install,274,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:59,integrability,event,eventually,59,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,performance,error,error,95,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:17,reliability,fail,failed,17,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,safety,error,error,95,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:95,usability,error,error,95,"The installation failed for me also (i.e. scispacy did not eventually install) due to the same error with nmslib @lfoppiano lists above. His fix, downloading nmslib separately using the `akode` conda channel worked, and I was able to use `pip install scispacy --no-deps` to install scispacy afterwards.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:196,deployability,instal,installed,196,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:261,deployability,updat,updated,261,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:370,deployability,build,building,370,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:445,deployability,version,version,445,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:507,deployability,releas,release,507,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:521,deployability,version,version,521,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:445,integrability,version,version,445,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:521,integrability,version,version,521,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:227,interoperability,specif,specifically,227,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:385,modifiability,extens,extensions,385,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:445,modifiability,version,version,445,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:521,modifiability,version,version,521,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:261,safety,updat,updated,261,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:261,security,updat,updated,261,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:563,usability,help,helps,563,"FYI I merged a pin for pybind into nmslib, which should hopefully fix some of these issues: https://github.com/nmslib/nmslib/pull/405. Because pybind is a common library, it is frequently already installed, so unless a library specifically pins it, it won't be updated. In particular, there was an issue with pypi's ability to find pybind C headers which are needed for building C/C++ extensions, which they have now apparently fixed in a newer version of pybind than what nmslib were using. So once nmslib release a new version, we can pin to that and see if it helps.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:4,deployability,instal,install,4,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:12,deployability,fail,failed,12,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:70,deployability,instal,installation,70,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,deployability,configurat,configuration,105,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,integrability,configur,configuration,105,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,modifiability,configur,configuration,105,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:12,reliability,fail,failed,12,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,security,configur,configuration,105,"pip install failed for me as well. . It also screwed up with my conda installation - thanks to the conda configuration that the tutorial wants me to create. Now, tensorflow/sklearn .. none of these libraries can be imported on my python. Disappointed.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:12,deployability,instal,installing,12,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:161,deployability,version,version,161,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:276,deployability,instal,installation,276,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:161,integrability,version,version,161,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:104,modifiability,variab,variables,104,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:161,modifiability,version,version,161,"@adityajo - installing conda will add the conda root environment to your PATH or PYTHONPATH environment variables, which dictate where your OS looks to find the version of python to run. This is probably what happened, and should be easily fixable by:. 1) Removing your conda installation. or . 2) Removing anything involving `anaconda` or `miniconda` from your python path.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:141,availability,avail,available,141,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:276,availability,unavail,unavailable,276,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:29,deployability,instal,installing,29,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:84,deployability,instal,install,84,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:108,deployability,fail,fails,108,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:156,energy efficiency,current,current,156,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:195,energy efficiency,cloud,cloud,195,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:123,modifiability,pac,package,123,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:358,modifiability,pac,package,358,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:108,reliability,fail,fails,108,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:141,reliability,availab,available,141,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:141,safety,avail,available,141,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:141,security,availab,available,141,"I also have the same problem installing with regard to nmslib. Unfortunately, conda install -c akode nmslib fails with the package not being available from current channels. I see it on Anaconda cloud, so don't know what channel it wants. I am on Windows 10, so perhaps it is unavailable for that. I was wondering if there were another way to get the nmslib package.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:176,deployability,instal,install,176,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:245,deployability,instal,install,245,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:28,integrability,repositor,repository,28,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:28,interoperability,repositor,repository,28,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:232,security,privil,privilege,232,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:303,usability,statu,status,303,"@ehobbs2 Clone the `nmslib` repository first by doing `git clone git@github.com:nmslib/nmslib.git`. . After that, go into the `python_bindings` folder, and do `python setup.py install` in your Windows cmd prompt. You may need admin privilege to install correctly (Open the cmd prompt with administrator status). . EDIT: that's what did it for me. Your mileage may vary!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:143,availability,down,download,143,"Thanks very much for the suggestion. I see that this builds from source, but at the moment I don't have a C++ compiler installed. I'll have to download that and try again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:53,deployability,build,builds,53,"Thanks very much for the suggestion. I see that this builds from source, but at the moment I don't have a C++ compiler installed. I'll have to download that and try again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:119,deployability,instal,installed,119,"Thanks very much for the suggestion. I see that this builds from source, but at the moment I don't have a C++ compiler installed. I'll have to download that and try again.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:22,availability,replic,replicate,22,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:55,deployability,instal,install,55,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:85,deployability,instal,install,85,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:137,deployability,instal,install,137,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:166,deployability,instal,install,166,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:201,deployability,instal,install,201,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:122,modifiability,scenario,scenario,122,I think I was able to replicate the issue by trying to install with `python setup.py install`. What worked for me in this scenario is to install `pybind11` with `pip install pybind11` before trying to install `scispacy`. Could you try this and see if it works for you?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:416,availability,down,downloaded,416,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:127,deployability,instal,installed,127,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:158,deployability,instal,install,158,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:280,deployability,instal,install,280,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:503,deployability,build,build,503,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:249,modifiability,pac,packages,249,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:558,safety,compl,complete,558,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:558,security,compl,complete,558,"Thanks for suggesting this; however, unfortunately, it didn't take care of the problem. In my case, it looks like pybind11 was installed since when I went to install it, I got ""Requirement is already satisfied: pybind11 in C:\...\anaconda3\lib\site-packages (2.3.0)."" Running pip install scispacy after this results in the original problem with nmslib. It wants to built the library from source, and I still haven't downloaded a C++ compiler yet (this being a holiday weekend). The process is set up to build the wheel for this library, so it would probably complete OK if the compiler were there.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:14,deployability,instal,installed,14,Is `pybind11` installed in your base environment or in your active environment? i.e. what is the `...` of the path you provided?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:15,usability,user,users,15,"The path is C:\users\myname. This is in the base environment, which is the active one.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:96,deployability,instal,installation,96,"Ah, not sure if this will make a difference or not, but I would suggest going through the whole installation procedure in a new conda environment, rather than in your base environment",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:131,deployability,instal,install,131,"I tried doing that, creating a separate environment. However, I'm not very knowledgeable about Anaconda, and when I went to do pip install in the scispacy environment following the installation instructions, it told me it couldn't find pip. I was interested in giving scispacy a quick test to see what it extracted from some documents I have, so just reverted to using the base environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:181,deployability,instal,installation,181,"I tried doing that, creating a separate environment. However, I'm not very knowledgeable about Anaconda, and when I went to do pip install in the scispacy environment following the installation instructions, it told me it couldn't find pip. I was interested in giving scispacy a quick test to see what it extracted from some documents I have, so just reverted to using the base environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:285,safety,test,test,285,"I tried doing that, creating a separate environment. However, I'm not very knowledgeable about Anaconda, and when I went to do pip install in the scispacy environment following the installation instructions, it told me it couldn't find pip. I was interested in giving scispacy a quick test to see what it extracted from some documents I have, so just reverted to using the base environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:285,testability,test,test,285,"I tried doing that, creating a separate environment. However, I'm not very knowledgeable about Anaconda, and when I went to do pip install in the scispacy environment following the installation instructions, it told me it couldn't find pip. I was interested in giving scispacy a quick test to see what it extracted from some documents I have, so just reverted to using the base environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:325,usability,document,documents,325,"I tried doing that, creating a separate environment. However, I'm not very knowledgeable about Anaconda, and when I went to do pip install in the scispacy environment following the installation instructions, it told me it couldn't find pip. I was interested in giving scispacy a quick test to see what it extracted from some documents I have, so just reverted to using the base environment.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:47,deployability,releas,release,47,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:87,deployability,instal,install,87,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:167,deployability,instal,install,167,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:104,energy efficiency,model,models,104,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:133,energy efficiency,load,load,133,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:133,performance,load,load,133,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:104,security,model,models,104,"I'm having the same issue. > FYI from the next release, you should just be able to pip install only the models and just use spacy to load them, without having to even install scispacy. Is there an ETA on this?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:53,deployability,version,version,53,@sudarshan85 This should be the case for the current version (0.2.3),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:45,energy efficiency,current,current,45,@sudarshan85 This should be the case for the current version (0.2.3),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:53,integrability,version,version,53,@sudarshan85 This should be the case for the current version (0.2.3),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:53,modifiability,version,version,53,@sudarshan85 This should be the case for the current version (0.2.3),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:72,deployability,fail,failed,72,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:79,deployability,instal,installation,79,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:102,deployability,instal,installation,102,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:120,deployability,fail,fails,120,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:145,integrability,event,eventually,145,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:33,interoperability,share,share,33,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:136,performance,time,time,136,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:72,reliability,fail,failed,72,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:120,reliability,fail,fails,120,Is anyone on this thread able to share a dockerfile that reproduces the failed installation? (Not the installation that fails the first time but eventually succeeds),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:33,deployability,version,version,33,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:147,deployability,instal,installation,147,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:218,deployability,version,version,218,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:298,deployability,version,version,298,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:309,deployability,instal,install,309,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:342,deployability,instal,installer,342,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:355,deployability,manag,manage,355,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:366,deployability,instal,installation,366,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:355,energy efficiency,manag,manage,355,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:33,integrability,version,version,33,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:218,integrability,version,version,218,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:298,integrability,version,version,298,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:76,interoperability,specif,specifications,76,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:109,interoperability,incompatib,incompatible,109,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:182,interoperability,Specif,Specifications,182,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:33,modifiability,version,version,33,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:218,modifiability,version,version,218,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:298,modifiability,version,version,298,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:355,safety,manag,manage,355,"I guess it has to do with python version. UnsatisfiableError: The following specifications were found. to be incompatible with the existing python installation in your environment:. Specifications:. - nmslib -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7. But i cannot choose the 3.6 version to install when i am using anaconda installer to manage the installation.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:212,deployability,manag,manage-environments,212,You can create a conda environment with python 3.6 like so `conda create -n myenv python=3.6`. See more about working with conda environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:212,energy efficiency,manag,manage-environments,212,You can create a conda environment with python 3.6 like so `conda create -n myenv python=3.6`. See more about working with conda environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:212,safety,manag,manage-environments,212,You can create a conda environment with python 3.6 like so `conda create -n myenv python=3.6`. See more about working with conda environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:195,usability,user,user-guide,195,You can create a conda environment with python 3.6 like so `conda create -n myenv python=3.6`. See more about working with conda environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:114,deployability,instal,installing,114,I think this issue might have been resolved now that nmslib provides wheels? Could someone who was having trouble installing before try again in a clean environment? Otherwise we might able to close this issue @DeNeutoy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:193,usability,close,close,193,I think this issue might have been resolved now that nmslib provides wheels? Could someone who was having trouble installing before try again in a clean environment? Otherwise we might able to close this issue @DeNeutoy,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:59,deployability,build,build,59,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:176,deployability,Build,Build,176,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:249,deployability,version,version,249,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:249,integrability,version,version,249,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:249,modifiability,version,version,249,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:148,reliability,doe,does,148,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,safety,test,test,105,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:105,testability,test,test,105,"Hi @danielkingai2 and all, I am actually, creating another build this evening. I appreciate if you could test the outcome tomorrow. Yes, now NMSLIB does provide binary wheels. Build from sources should be easier now too, as we require a more recent version of pybind11. . Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:60,deployability,instal,install,60,@searchivarius What did you want us to try? Just normal pip install?,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:63,usability,efficien,efficiency,63,"@danielkingai2 yes, and how using precompiled binaries affects efficiency. Thank you!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:46,deployability,instal,install,46,"@searchivarius everything went smoothly, both install from binary and install from source. Don't have any formal tests for efficiency, but no noticeable difference between the two for our use case. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:70,deployability,instal,install,70,"@searchivarius everything went smoothly, both install from binary and install from source. Don't have any formal tests for efficiency, but no noticeable difference between the two for our use case. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,safety,test,tests,113,"@searchivarius everything went smoothly, both install from binary and install from source. Don't have any formal tests for efficiency, but no noticeable difference between the two for our use case. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:113,testability,test,tests,113,"@searchivarius everything went smoothly, both install from binary and install from source. Don't have any formal tests for efficiency, but no noticeable difference between the two for our use case. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:123,usability,efficien,efficiency,123,"@searchivarius everything went smoothly, both install from binary and install from source. Don't have any formal tests for efficiency, but no noticeable difference between the two for our use case. Thanks!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:67,deployability,updat,updates,67,"Closing this issue as I believe it has been resolved by the latest updates to `nmslib`. If you still have issues, feel free to reopen it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:67,safety,updat,updates,67,"Closing this issue as I believe it has been resolved by the latest updates to `nmslib`. If you still have issues, feel free to reopen it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/137:67,security,updat,updates,67,"Closing this issue as I believe it has been resolved by the latest updates to `nmslib`. If you still have issues, feel free to reopen it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137
https://github.com/allenai/scispacy/issues/138:142,energy efficiency,GPU,GPU,142,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:250,energy efficiency,model,models,250,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:337,energy efficiency,model,model,337,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:455,energy efficiency,model,model,455,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:34,integrability,batch,batch,34,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:34,performance,batch,batch,34,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:142,performance,GPU,GPU,142,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:250,security,model,models,250,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:337,security,model,model,337,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:455,security,model,model,455,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:224,usability,document,document,224,"Hi! To do this faster, you should batch them together and compute the result using a matrix multiplication, which you can speed up by using a GPU, for example. More generally you should be a little careful of relying on the document vectors of spacy models, as I think they are just word vector averages. Also note that the scispacy med model only has 20k vectors, which is not a huge amount, so many words will be OOV (i.e not have a vector). Is a large model (~2GB with vectors) something that you would be interested in?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:349,availability,cluster,cluster,349,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:31,deployability,stack,stackexchange,31,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:129,deployability,stack,stackoverflow,129,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:349,deployability,cluster,cluster,349,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:249,energy efficiency,model,models,249,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:91,security,sign,significantly,91,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:249,security,model,models,249,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:377,usability,document,documents,377,Thanks for reply. This post on stackexchange which uses matrix multiplication via numpy is significantly boosting speed: https://stackoverflow.com/questions/41905029/create-cosine-similarity-matrix-numpy?rq=1. I would be interested in larger vector models. I realize there are caveats to using them but am trying to pair them with other features to cluster/classify biomedical documents.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:62,deployability,releas,release,62,"Cool, i'll bear that in mind for what to work on for the next release!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/138:0,energy efficiency,Cool,Cool,0,"Cool, i'll bear that in mind for what to work on for the next release!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/138
https://github.com/allenai/scispacy/issues/139:116,energy efficiency,model,model,116,"Hey, you might find it easiest to just extract the vectors and words from the vocabulary and put them into a gensim model. We used the vectors found on this page:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/139
https://github.com/allenai/scispacy/issues/139:116,security,model,model,116,"Hey, you might find it easiest to just extract the vectors and words from the vocabulary and put them into a gensim model. We used the vectors found on this page:. http://bio.nlplab.org/.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/139
https://github.com/allenai/scispacy/issues/140:579,performance,time,timer,579,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:773,performance,time,timer,773,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:882,safety,Test,Test,882,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:33,security,modif,modified,33,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:994,security,Token,Token,994,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:882,testability,Test,Test,882,"Here is the evaluation funtion I modified from `train_parser_and_tagger.py`:. ```python. def evaluate_parser_and_tagger(train_json_path: str,. dev_json_path: str,. test_json_path: str,. model_path: str = None,. ontonotes_path: str = None):. msg = Printer(). train_json_path = cached_path(train_json_path). dev_json_path = cached_path(dev_json_path). test_json_path = cached_path(test_json_path). train_corpus = GoldCorpus(train_json_path, dev_json_path). test_corpus = GoldCorpus(train_json_path, test_json_path). nlp_loaded = util.load_model_from_path(model_path). start_time = timer(). test_docs = test_corpus.dev_docs(nlp_loaded). test_docs = list(test_docs). nwords = sum(len(doc_gold[0]) for doc_gold in test_docs). scorer = nlp_loaded.evaluate(test_docs). end_time = timer(). gpu_wps = None. cpu_wps = nwords/(end_time-start_time). print(""Retrained genia evaluation""). print(""Test results:""). print(""UAS:"", scorer.uas). print(""LAS:"", scorer.las). print(""Tag %:"", scorer.tags_acc). print(""Token acc:"", scorer.token_acc). ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:678,availability,robust,robust,678,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:739,deployability,continu,continuously,739,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:752,deployability,updat,updating,752,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:177,energy efficiency,model,models,177,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:564,energy efficiency,current,currently,564,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:781,energy efficiency,model,models,781,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:862,energy efficiency,model,models,862,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:678,reliability,robust,robust,678,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:222,safety,test,test,222,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:336,safety,test,test,336,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:678,safety,robust,robust,678,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:752,safety,updat,updating,752,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:177,security,model,models,177,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:752,security,updat,updating,752,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:781,security,model,models,781,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:862,security,model,models,862,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:222,testability,test,test,222,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:336,testability,test,test,336,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:468,usability,command,command,468,"Hello,. Sorry about this! Looks like you can reproduce the POS and UAS scores, so that's good. For LAS, this is definitely some kind of bug, because when I evaluate the trained models I get a LAS of 80.57 for both dev and test, which is different again from you. Please could you try running `spacy evaluate en_core_sci_md /path/to/dev/test` and let me know what the results are? I want to know if there is a difference between that script above and the official eval command, because that would point to a bug somewhere in the evaluation procedure in spacy. I am currently at ACL so I won't be able to investigate this today, but rest assured that the results in the paper are robust (they don't match the ones in the repo because we are continuously updating them - training the models is a probabilistic process and also pulls in upstream improvements in the models themselves from spacy, so they are not going to stay tied to the exact scores in the paper) and this is just some bug :).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:347,performance,Time,Time,347,"Hello,. Thank you for your prompt reply! I tried to run `spacy evaluate en_core_sci_sm/md /path/to/dev/test` and get the following results. It seems the LAS score is almost the same as the score reported in github repo, but the POS and UAS scores are much higher than the provided evaluation script. . ```. ------------en_core_sci_sm------------. Time 332.79 s. Words 446014. Words/s 1340. TOK 100.00. POS 99.47. UAS 92.51. LAS 87.33. NER P 0.00. NER R 0.00. NER F 0.00. ------------en_core_sci_md------------. Time 78.16 s. Words 446014. Words/s 5706. TOK 100.00. POS 99.55. UAS 93.40. LAS 88.10. NER P 0.00. NER R 0.00. NER F 0.00. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:511,performance,Time,Time,511,"Hello,. Thank you for your prompt reply! I tried to run `spacy evaluate en_core_sci_sm/md /path/to/dev/test` and get the following results. It seems the LAS score is almost the same as the score reported in github repo, but the POS and UAS scores are much higher than the provided evaluation script. . ```. ------------en_core_sci_sm------------. Time 332.79 s. Words 446014. Words/s 1340. TOK 100.00. POS 99.47. UAS 92.51. LAS 87.33. NER P 0.00. NER R 0.00. NER F 0.00. ------------en_core_sci_md------------. Time 78.16 s. Words 446014. Words/s 5706. TOK 100.00. POS 99.55. UAS 93.40. LAS 88.10. NER P 0.00. NER R 0.00. NER F 0.00. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:103,safety,test,test,103,"Hello,. Thank you for your prompt reply! I tried to run `spacy evaluate en_core_sci_sm/md /path/to/dev/test` and get the following results. It seems the LAS score is almost the same as the score reported in github repo, but the POS and UAS scores are much higher than the provided evaluation script. . ```. ------------en_core_sci_sm------------. Time 332.79 s. Words 446014. Words/s 1340. TOK 100.00. POS 99.47. UAS 92.51. LAS 87.33. NER P 0.00. NER R 0.00. NER F 0.00. ------------en_core_sci_md------------. Time 78.16 s. Words 446014. Words/s 5706. TOK 100.00. POS 99.55. UAS 93.40. LAS 88.10. NER P 0.00. NER R 0.00. NER F 0.00. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:103,testability,test,test,103,"Hello,. Thank you for your prompt reply! I tried to run `spacy evaluate en_core_sci_sm/md /path/to/dev/test` and get the following results. It seems the LAS score is almost the same as the score reported in github repo, but the POS and UAS scores are much higher than the provided evaluation script. . ```. ------------en_core_sci_sm------------. Time 332.79 s. Words 446014. Words/s 1340. TOK 100.00. POS 99.47. UAS 92.51. LAS 87.33. NER P 0.00. NER R 0.00. NER F 0.00. ------------en_core_sci_md------------. Time 78.16 s. Words 446014. Words/s 5706. TOK 100.00. POS 99.55. UAS 93.40. LAS 88.10. NER P 0.00. NER R 0.00. NER F 0.00. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:135,deployability,version,version,135,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:156,deployability,instal,installed,156,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:429,deployability,instal,install,429,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:557,deployability,version,version,557,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:699,deployability,pipelin,pipeline,699,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:621,energy efficiency,model,models,621,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:135,integrability,version,version,135,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:557,integrability,version,version,557,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:699,integrability,pipelin,pipeline,699,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:580,interoperability,share,share,580,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:135,modifiability,version,version,135,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:557,modifiability,version,version,557,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:586,modifiability,paramet,parameters,586,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:353,safety,compl,completely,353,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:353,security,compl,completely,353,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:621,security,model,models,621,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:336,testability,verif,verify,336,"Hmm this is quite strange, because I could actually reproduce your first bug - could you provide some information on exactly the spacy version etc you have installed? I think it's likely that these numbers are incorrect somehow, but it's confusing that they are now higher when previously the scores were lower. . Ideally, if you could verify this in a completely fresh environment and provide me with the exact steps you ran to install everything, I can try and investigate why this is happening. Initially, I thought it was to do with spacy 2.0 - in this version of spacy, they share parameters for the NER and parsing models, because it assumes that they are trained jointly. This means that our pipeline approach, where we train first the tagger and parser, and then the NER afterward, might cause some problem.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:114,usability,help,help,114,@yuhui-zh15 any chance you can let me know some more details about the evaluation you ran above? I think it would help me to figure this out :),MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:60,deployability,instal,install,60,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:275,deployability,version,version,275,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:275,integrability,version,version,275,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:275,modifiability,version,version,275,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:15,testability,simpl,simply,15,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:15,usability,simpl,simply,15,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:40,usability,command,commands,40,"Hello,. I just simply ran the following commands:. ```. pip install spacy. spacy evaluate en_core_sci_sm /path/to/data. spacy evaluate en_core_sci_md /path/to/data. ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:749,availability,consist,consistently,749,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:957,availability,state,state,957,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:141,deployability,pipelin,pipeline,141,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:341,deployability,upgrad,upgraded,341,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:391,deployability,observ,observed,391,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:545,deployability,releas,release,545,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:697,deployability,version,version,697,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:775,deployability,version,versions,775,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:999,deployability,pipelin,pipeline,999,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:428,energy efficiency,model,model,428,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:601,energy efficiency,model,models,601,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:882,energy efficiency,reduc,reduction,882,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:947,energy efficiency,optim,optimizer,947,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1135,energy efficiency,model,model,1135,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1258,energy efficiency,model,model,1258,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1483,energy efficiency,model,model,1483,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:141,integrability,pipelin,pipeline,141,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:627,integrability,coupl,couple,627,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:697,integrability,version,version,697,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:775,integrability,version,versions,775,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:957,integrability,state,state,957,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:999,integrability,pipelin,pipeline,999,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:341,modifiability,upgrad,upgraded,341,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:627,modifiability,coupl,couple,627,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:697,modifiability,version,version,697,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:775,modifiability,version,versions,775,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:947,performance,optimiz,optimizer,947,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1144,performance,disk,disk,1144,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1185,performance,perform,performance,1185,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:818,reliability,doe,doesn,818,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1220,reliability,doe,doesn,1220,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:428,security,model,model,428,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:601,security,model,models,601,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1135,security,model,model,1135,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1258,security,model,model,1258,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1483,security,model,model,1483,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:391,testability,observ,observed,391,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:627,testability,coupl,couple,627,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:749,usability,consist,consistently,749,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:1185,usability,perform,performance,1185,"@honnibal - i'd love your opinion here. Scispacy's NER data is separate from the tagging and parsing data. Because of this, we train first a pipeline with a tagger and parser, and then use [this script](https://github.com/allenai/scispacy/blob/master/scripts/train_ner.py) to add in an NER pipe. We didn't notice this until now, but when we upgraded from `spacy==2.0.18` to `spacy==2.1`, we observed that after training the NER model, the Labelled Attachment Score of the parser is dropping by about 4%. I think this might be related to the 2.1 release sharing more weights between the parser and NER models - is that right? A couple of investigations we've done:. - It's not due to a newer spacy version than 2.1.0, the same reported results occur consistently across 2.1.x versions. - The amount of training for NER doesn't affect how much the LAS drops off one epoch is the same reduction as N. This made me wonder if we needed to preserve the optimizer state somehow when we reload the previous pipeline with the tagger and parser in it? - Additionally, just adding the NER pipe, calling `nlp.begin_training()` and writing the new model to disk results in the same decrease in LAS performance. - Before 2.1, the LAS doesn't change after we train the NER model. It's also a bit weird to me that it's only the LAS that's affected, rather than the whole parser. It's reasonable if the answer to this is ""they need to be trained together"". We've been thinking of using a strong BERT model trained on med mentions to annotate generic mention spans in the GENIA corpus anyway, so it's possible that we can just accelerate this.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:87,integrability,discover,discovering,87,"Seems that we found the issue, see the #146 for more detail. @yuhui-zh15 Thank you for discovering this issue! @honnibal I do still think that this only became a problem with spacy 2.1.x, and find it a bit odd that only LAS was affected, but the issue does seem to be resolved by disabling the parser/tagger pipes when calling `nlp.begin_training`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:87,interoperability,discover,discovering,87,"Seems that we found the issue, see the #146 for more detail. @yuhui-zh15 Thank you for discovering this issue! @honnibal I do still think that this only became a problem with spacy 2.1.x, and find it a bit odd that only LAS was affected, but the issue does seem to be resolved by disabling the parser/tagger pipes when calling `nlp.begin_training`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:252,reliability,doe,does,252,"Seems that we found the issue, see the #146 for more detail. @yuhui-zh15 Thank you for discovering this issue! @honnibal I do still think that this only became a problem with spacy 2.1.x, and find it a bit odd that only LAS was affected, but the issue does seem to be resolved by disabling the parser/tagger pipes when calling `nlp.begin_training`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/140:87,usability,discov,discovering,87,"Seems that we found the issue, see the #146 for more detail. @yuhui-zh15 Thank you for discovering this issue! @honnibal I do still think that this only became a problem with spacy 2.1.x, and find it a bit odd that only LAS was affected, but the issue does seem to be resolved by disabling the parser/tagger pipes when calling `nlp.begin_training`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140
https://github.com/allenai/scispacy/issues/141:106,usability,visual,visualizers,106,"`colors` should be a dictionary mapping `{""TAG"": ""color""}`, not a list, see here:. https://spacy.io/usage/visualizers#ent",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:253,reliability,doe,does,253,"> `colors` should be a dictionary mapping `{""TAG"": ""color""}`, not a list, see here:. > . > https://spacy.io/usage/visualizers#ent. Sorry, I should correct myself, `colors` is a dictionary with the same names in my `entities` list as keys. And, still it does not work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:114,usability,visual,visualizers,114,"> `colors` should be a dictionary mapping `{""TAG"": ""color""}`, not a list, see here:. > . > https://spacy.io/usage/visualizers#ent. Sorry, I should correct myself, `colors` is a dictionary with the same names in my `entities` list as keys. And, still it does not work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:170,safety,detect,detects,170,"Can you provide an actual code snippet, because doing this with custom colours has actually worked for me before, so I know that it works :) . The entities that scispacy detects are called `ENT`, do you have that in your colour dict?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:170,security,detect,detects,170,"Can you provide an actual code snippet, because doing this with custom colours has actually worked for me before, so I know that it works :) . The entities that scispacy detects are called `ENT`, do you have that in your colour dict?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:64,usability,custom,custom,64,"Can you provide an actual code snippet, because doing this with custom colours has actually worked for me before, so I know that it works :) . The entities that scispacy detects are called `ENT`, do you have that in your colour dict?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:380,availability,avail,available,380,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:65,deployability,pipelin,pipeline,65,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:513,deployability,pipelin,pipeline,513,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:45,energy efficiency,load,load,45,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:74,energy efficiency,model,model,74,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:114,energy efficiency,load,load,114,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:186,energy efficiency,load,load,186,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:206,energy efficiency,model,model,206,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:247,energy efficiency,load,load,247,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:460,energy efficiency,model,models,460,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:522,energy efficiency,model,model,522,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:65,integrability,pipelin,pipeline,65,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:513,integrability,pipelin,pipeline,513,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:193,interoperability,specif,specific,193,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:45,performance,load,load,45,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:114,performance,load,load,114,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:186,performance,load,load,186,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:247,performance,load,load,247,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:380,reliability,availab,available,380,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:380,safety,avail,available,380,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:74,security,model,model,74,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:206,security,model,model,206,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:380,security,availab,available,380,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:460,security,model,models,460,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:522,security,model,model,522,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:163,usability,visual,visualized,163,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:291,usability,visual,visualized,291,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:362,usability,visual,visualize,362,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:532,usability,visual,visualize,532,"I just realized what the problem was. When I load the full spaCy pipeline model like the following:. `nlp = spacy.load(""en_core_sci_md"")`. Named entities won't be visualized, but when I load a specific NER model, like the following:. `nlp = spacy.load(""en_ner_bionlp13cg_md"")`. I do see the visualized named entities. . then I think my question is, if I want to visualize all the available named entities in scispacy (27 named entities,) should I call the NER models separately? Isn't there a way to use the full pipeline model and visualize all the named entities at once?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:283,availability,state,stateful,283,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:346,deployability,pipelin,pipeline,346,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:86,energy efficiency,model,model,86,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:177,energy efficiency,model,model,177,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:216,energy efficiency,model,models,216,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:274,energy efficiency,model,model,274,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:335,energy efficiency,model,model,335,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:283,integrability,state,stateful,283,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:346,integrability,pipelin,pipeline,346,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:358,reliability,doe,doesn,358,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:86,security,model,model,86,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:177,security,model,model,177,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:216,security,model,models,216,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:274,security,model,model,274,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:335,security,model,model,335,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:99,usability,help,help,99,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:147,usability,visual,visualize,147,"Again, if you provide me with a code snippet of what you did for the `en_core_sci_md` model, I can help - I know for a fact that it is possible to visualize entities using this model. In terms of combining the other models together, this is a bit tricky in Spacy as the NER model is stateful (meaning that if you run more than one NER model in a pipeline it doesn't work properly). So unfortunately that might be a bit tricky, sorry!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:123,energy efficiency,load,load,123,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:161,energy efficiency,load,load,161,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4453,energy efficiency,model,model,4453,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4527,energy efficiency,model,model,4527,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:365,integrability,sub,subtype,365,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:971,integrability,sub,subtypes,971,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1481,integrability,sub,subtypes,1481,"ive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `ge",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1604,integrability,sub,subtype,1604,"University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote for getting the color options like the following (everybody, feel free to use it if",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1733,integrability,sub,subtypes,1733,"r2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote for getting the color options like the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visua",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:237,interoperability,distribut,distribution,237,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1400,interoperability,distribut,distributed,1400,"perative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entit",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4514,interoperability,specif,specific,4514,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1018,modifiability,paramet,parameters,1018,"xact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignan",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:123,performance,load,load,123,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:161,performance,load,load,161,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:685,performance,perform,performed,685,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1075,performance,perform,performed,1075," from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1176,performance,perform,performed,1176,"""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression featur",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1521,performance,time,times,1521,"008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:2067,reliability,doe,does,2067," were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote for getting the color options like the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. """,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1121,safety,test,tests,1121,"oad(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:2278,safety,except,exceptions,2278,"l. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote for getting the color options like the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTU",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:796,security,ident,identify,796,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1230,security,sign,significant,1230," distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1497,security,sign,significantly,1497,"a samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4453,security,model,model,4453,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4527,security,model,model,4527,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1121,testability,test,tests,1121,"oad(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:227,usability,learn,learn,227,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:685,usability,perform,performed,685,"@DeNeutoy This is the exact code I'm using:. ```. import scispacy. import spacy. from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive case",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1075,usability,perform,performed,1075," from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""). nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:1176,usability,perform,performed,1176,"""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression featur",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:2732,usability,visual,visualizing,2732,"pes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 expression features, or presence of stem cells. That can be the reason of tumor aggressivity and chemoresistance. exceptions are Her2 positive tumors because they have different base of carcinogenesis."""""". doc = nlp(text). options = get_entity_options(). displacy.render(doc, style='ent', options=options). ```. Where `get_entity_options()` is a method I wrote for getting the color options like the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4476,usability,visual,visualization,4476,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4546,usability,visual,visualization,4546,"the following (everybody, feel free to use it if you find it useful):. ```. import random . def get_entity_options(random_colors=False):. """""". generating color options for visualizing the named entities. """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]). for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", . ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"",. ""DISEASE"", ""CHEMICAL"",. ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. . colors = {""ENT"":""#E8DAEF""}. . if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. . options = {""ents"": entities, ""colors"": colors}. . return options. ```. Using the full model, I can't see any visualization, but when I switch to a specific NER model I do see the visualization.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3767,availability,repair,repair,3767,"a 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:2978,deployability,instal,install,2978,"_CHEMICAL"":""#76D7C4"", ""ANATOMICAL_SYSTEM"":""#82E0AA"", \. ""IMMATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", \. ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known abou",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3037,deployability,releas,releases,3037,"MATERIAL_ANATOMICAL_ENTITY"":""#A2D9CE"", ""MULTI-TISSUE_STRUCTURE"":""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", \. ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in hu",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3100,deployability,instal,installed,3100,"""#85C1E9"", ""DEVELOPING_ANATOMICAL_STRUCTURE"":""#A9DFBF"", \. ""ORGANISM_SUBDIVISION"":""#58D68D"", ""CELLULAR_COMPONENT"":""#7FB3D5""}. entities_cats = [entities_cat_1, entities_cat_2, entities_cat_3, entities_cat_4]. for item in entities_cats:. colors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3339,deployability,version,version,3339,"lors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4973,deployability,modul,modulating,4973,"about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methot",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6112,deployability,modul,modulates,6112,"ill unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:24,energy efficiency,cool,cool,24,"@phosseini : ***very*** cool, thank you! I added your code (for my own use / tests) as a method, giving the following results! :-). ---. **entity_options.py**. ```. ## Source: https://github.com/allenai/scispacy/issues/141#issuecomment-518274586. ## Author: https://github.com/phosseini. ## File: /mnt/Vancouver/apps/spacy/entity_options.py. ## Env: Python 3.7 venv:. ## Use:. ## import entity_options. ## from entity_options import get_entity_options. ## displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). ## Ent: https://github.com/allenai/scispacy/issues/79#issuecomment-557766506 ## CRAFT entities. import random . def get_entity_options(random_colors=False):. """""" generating color options for visualizing the named entities """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"", \. ""DISEASE"", ""CHEMICAL"", ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", \. ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", \. ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. colors = {""ENT"":""#E8DAEF""}. if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", \. ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4700,energy efficiency,predict,predicts,4700,"ns to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6372,energy efficiency,load,load,6372," with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D9CE',. 'ORGANISM_SUBDIVISION': '#58D68D',. 'PROTEIN': '#82E0AA',. 'RNA': '#82E0AA',. 'SIMPLE_CHEMICAL': '#76D7C4',. 'SO': '#F7DC6F',. 'TAXON': '#F4D03F',. 'TISSUE': '#A9DFBF'},. 'ents': ['GGP',. 'SO',. 'TAXON',. 'CHEBI',. 'GO',. 'CL',. 'DNA',. 'CELL_TYPE',.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3339,integrability,version,version,3339,"lors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4008,integrability,mediat,mediated,4008,"naws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcription",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4876,integrability,mediat,mediated,4876,"of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Int",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4008,interoperability,mediat,mediated,4008,"naws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcription",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4876,interoperability,mediat,mediated,4876,"of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Int",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5391,interoperability,coordinat,coordinately,5391,"nwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3339,modifiability,version,version,3339,"lors = {**colors, **item}. options = {""ents"": entities, ""colors"": colors}. # print(options). return options. ```. ---. **Python 3.7 venv**. ```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3710,modifiability,maintain,maintain,3710," Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4973,modifiability,modul,modulating,4973,"about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methot",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6112,modifiability,modul,modulates,6112,"ill unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6372,performance,load,load,6372," with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D9CE',. 'ORGANISM_SUBDIVISION': '#58D68D',. 'PROTEIN': '#82E0AA',. 'RNA': '#82E0AA',. 'SIMPLE_CHEMICAL': '#76D7C4',. 'SO': '#F7DC6F',. 'TAXON': '#F4D03F',. 'TISSUE': '#A9DFBF'},. 'ents': ['GGP',. 'SO',. 'TAXON',. 'CHEBI',. 'GO',. 'CL',. 'DNA',. 'CELL_TYPE',.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3727,reliability,stabil,stability,3727,"cache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patie",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3767,reliability,repair,repair,3767,"a 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:77,safety,test,tests,77,"@phosseini : ***very*** cool, thank you! I added your code (for my own use / tests) as a method, giving the following results! :-). ---. **entity_options.py**. ```. ## Source: https://github.com/allenai/scispacy/issues/141#issuecomment-518274586. ## Author: https://github.com/phosseini. ## File: /mnt/Vancouver/apps/spacy/entity_options.py. ## Env: Python 3.7 venv:. ## Use:. ## import entity_options. ## from entity_options import get_entity_options. ## displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). ## Ent: https://github.com/allenai/scispacy/issues/79#issuecomment-557766506 ## CRAFT entities. import random . def get_entity_options(random_colors=False):. """""" generating color options for visualizing the named entities """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"", \. ""DISEASE"", ""CHEMICAL"", ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", \. ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", \. ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. colors = {""ENT"":""#E8DAEF""}. if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", \. ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3710,safety,maintain,maintain,3710," Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4700,safety,predict,predicts,4700,"ns to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4973,safety,modul,modulating,4973,"about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methot",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5367,safety,compl,complex,5367,"n breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5507,safety,compl,complex,5507,"rtantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6112,safety,modul,modulates,6112,"ill unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:250,security,Auth,Author,250,"@phosseini : ***very*** cool, thank you! I added your code (for my own use / tests) as a method, giving the following results! :-). ---. **entity_options.py**. ```. ## Source: https://github.com/allenai/scispacy/issues/141#issuecomment-518274586. ## Author: https://github.com/phosseini. ## File: /mnt/Vancouver/apps/spacy/entity_options.py. ## Env: Python 3.7 venv:. ## Use:. ## import entity_options. ## from entity_options import get_entity_options. ## displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). ## Ent: https://github.com/allenai/scispacy/issues/79#issuecomment-557766506 ## CRAFT entities. import random . def get_entity_options(random_colors=False):. """""" generating color options for visualizing the named entities """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"", \. ""DISEASE"", ""CHEMICAL"", ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", \. ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", \. ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. colors = {""ENT"":""#E8DAEF""}. if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", \. ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3814,security,control,control,3814,"rwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4301,security,sign,significantly,4301,"oria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are rep",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5139,security,ident,identified,5139,"d with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5367,security,compl,complex,5367,"n breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:5507,security,compl,complex,5507,"rtantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:77,testability,test,tests,77,"@phosseini : ***very*** cool, thank you! I added your code (for my own use / tests) as a method, giving the following results! :-). ---. **entity_options.py**. ```. ## Source: https://github.com/allenai/scispacy/issues/141#issuecomment-518274586. ## Author: https://github.com/phosseini. ## File: /mnt/Vancouver/apps/spacy/entity_options.py. ## Env: Python 3.7 venv:. ## Use:. ## import entity_options. ## from entity_options import get_entity_options. ## displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). ## Ent: https://github.com/allenai/scispacy/issues/79#issuecomment-557766506 ## CRAFT entities. import random . def get_entity_options(random_colors=False):. """""" generating color options for visualizing the named entities """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"", \. ""DISEASE"", ""CHEMICAL"", ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", \. ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", \. ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. colors = {""ENT"":""#E8DAEF""}. if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", \. ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3814,testability,control,control,3814,"rwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:4973,testability,modula,modulating,4973,"about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-stimulated proliferation. Importantly, this mechanism is manifested in breast cancer patient samples and TCGA database, which showed that low SIRT1 gene expression in tumor tissues compared with normal adjacent tissues predicts poor prognosis in patients with breast cancer. Taken together, our findings suggest that BRCA1 attenuates AR-stimulated proliferation of breast cancer cells via SIRT1 mediated pathway. | 30714292. Breast cancer susceptibility gene 1 (BRCA1) has been implicated in modulating metabolism via transcriptional regulation. However, direct metabolic targets of BRCA1 and the underlying regulatory mechanisms are still unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methot",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:6112,testability,modula,modulates,6112,"ill unknown. Here, we identified several metabolic genes, including the gene which encodes glutamate‐oxaloacetate transaminase 2 (GOT2), a key enzyme for aspartate biosynthesis, which are repressed by BRCA1. We report that BRCA1 forms a co‐repressor complex with ZBRK1 that coordinately represses GOT 2 expression via a ZBRK1 recognition element in the promoter of GOT2. Impairment of this complex results in upregulation of GOT2, which in turn increases aspartate and alpha ketoglutarate production, leading to rapid cell proliferation of breast cancer cells. Importantly, we found that GOT2 can serve as an independent prognostic factor for overall survival and disease‐free survival of patients with breast cancer, especially triple‐negative breast cancer. Interestingly, we also demonstrated that GOT2 overexpression sensitized breast cancer cells to methotrexate, suggesting a promising precision therapeutic strategy for breast cancer treatment. In summary, our findings reveal that BRCA1 modulates aspartate biosynthesis through transcriptional repression of GOT2, and provides a biological basis for treatment choices in breast cancer. | BRCA1/2. BRCA1 and BRCA2 (BRCA1/2) are human genes that produce tumor suppressor proteins."". >>> nlp = spacy.load(""en_ner_craft_md""). >>> doc = nlp(text). >>> import entity_options. >>> from entity_options import get_entity_options. >>> get_entity_options() ## default: (random_colors=False). {'colors': {'AMINO_ACID': '#85C1E9',. 'ANATOMICAL_SYSTEM': '#82E0AA',. 'CANCER': '#ABEBC6',. 'CELL': '#76D7C4',. 'CELLULAR_COMPONENT': '#7FB3D5',. 'CELL_LINE': '#E8DAEF',. 'CELL_TYPE': '#AED6F1',. 'CHEBI': '#FAD7A0',. 'CHEMICAL': '#D2B4DE',. 'CL': '#F5B041',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#A9DFBF',. 'DISEASE': '#D7BDE2',. 'DNA': '#82E0AA',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#AED6F1',. 'GGP': '#F9E79F',. 'GO': '#F8C471',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#A2D9CE',. 'MULTI-TISSUE_STRUCTURE': '#85C1E9',. 'ORGAN': '#82E0AA',. 'ORGANISM': '#A2D",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:729,usability,visual,visualizing,729,"@phosseini : ***very*** cool, thank you! I added your code (for my own use / tests) as a method, giving the following results! :-). ---. **entity_options.py**. ```. ## Source: https://github.com/allenai/scispacy/issues/141#issuecomment-518274586. ## Author: https://github.com/phosseini. ## File: /mnt/Vancouver/apps/spacy/entity_options.py. ## Env: Python 3.7 venv:. ## Use:. ## import entity_options. ## from entity_options import get_entity_options. ## displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). ## Ent: https://github.com/allenai/scispacy/issues/79#issuecomment-557766506 ## CRAFT entities. import random . def get_entity_options(random_colors=False):. """""" generating color options for visualizing the named entities """""". def color_generator(number_of_colors):. color = [""#""+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]. return color. entities = [""GGP"", ""SO"", ""TAXON"", ""CHEBI"", ""GO"", ""CL"", ""DNA"", ""CELL_TYPE"", ""CELL_LINE"", ""RNA"", ""PROTEIN"", \. ""DISEASE"", ""CHEMICAL"", ""CANCER"", ""ORGAN"", ""TISSUE"", ""ORGANISM"", ""CELL"", ""AMINO_ACID"", \. ""GENE_OR_GENE_PRODUCT"", ""SIMPLE_CHEMICAL"", ""ANATOMICAL_SYSTEM"", ""IMMATERIAL_ANATOMICAL_ENTITY"", \. ""MULTI-TISSUE_STRUCTURE"", ""DEVELOPING_ANATOMICAL_STRUCTURE"", ""ORGANISM_SUBDIVISION"", ""CELLULAR_COMPONENT""]. colors = {""ENT"":""#E8DAEF""}. if random_colors:. color = color_generator(len(entities)). for i in range(len(entities)):. colors[entities[i]] = color[i]. else:. entities_cat_1 = {""GGP"":""#F9E79F"", ""SO"":""#F7DC6F"", ""TAXON"":""#F4D03F"", ""CHEBI"":""#FAD7A0"", ""GO"":""#F8C471"", ""CL"":""#F5B041""}. entities_cat_2 = {""DNA"":""#82E0AA"", ""CELL_TYPE"":""#AED6F1"", ""CELL_LINE"":""#E8DAEF"", ""RNA"":""#82E0AA"", ""PROTEIN"":""#82E0AA""}. entities_cat_3 = {""DISEASE"":""#D7BDE2"", ""CHEMICAL"":""#D2B4DE""}. entities_cat_4 = {""CANCER"":""#ABEBC6"", ""ORGAN"":""#82E0AA"", ""TISSUE"":""#A9DFBF"", ""ORGANISM"":""#A2D9CE"", ""CELL"":""#76D7C4"", \. ""AMINO_ACID"":""#85C1E9"", ""GENE_OR_GENE_PRODUCT"":""#AED6F1"", ""SIMPLE_CHEMICAL"":""#76D7C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:3481,usability,help,help,3481,"```. (py3.7) [victoria@victoria spacy]$ date; pwd; ls -l. Tue 26 Nov 2019 01:45:28 PM PST. /mnt/Vancouver/apps/spacy. total 20. -rw-r--r-- 1 victoria victoria 2287 Nov 26 13:42 entity_options.py. drwxr-xr-x 2 victoria victoria 4096 Nov 26 13:34 __pycache__. -rw------- 1 victoria victoria 3560 Nov 26 12:02 readme-victoria-spacy.txt. drwxr-xr-x 3 victoria victoria 4096 Nov 19 11:41 scispacy. -rw-r--r-- 1 victoria victoria 2624 Nov 26 11:59 spacy_srl.py. (py3.7) [victoria@victoria spacy]$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_craft_md-0.2.4.tar.gz. ... Successfully installed blis-0.4.1 catalogue-0.0.8 en-ner-craft-md-0.2.4 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1. (py3.7) [victoria@victoria spacy]$ env | grep -i virtual. VIRTUAL_ENV=/home/victoria/venv/py3.7. (py3.7) [victoria@victoria spacy]$ python --version. Python 3.7.4. (py3.7) [victoria@victoria spacy]$ python. Python 3.7.4 (default, Nov 20 2019, 11:36:53) . [GCC 9.2.0] on linux. Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import spacy. >>> from spacy import displacy. >>> text = ""26902145. Breast cancer susceptibility gene 1 (BRCA1) is a tumor suppressor protein that functions to maintain genomic stability through critical roles in DNA repair, cell-cycle arrest, and transcriptional control. The androgen receptor (AR) is expressed in more than 70% of breast cancers and has been implicated in breast cancer pathogenesis. However, little is known about the role of BRCA1 in AR-mediated cell proliferation in human breast cancer. Here, we report that a high expression of AR in breast cancer patients was associated with shorter overall survival (OS) using a tissue microarray with 149 non-metastatic breast cancer patient samples. We reveal that overexpression of BRCA1 significantly inhibited expression of AR through activation of SIRT1 in breast cancer cells. Meanwhile, SIRT1 induction or treatment with a SIRT1 agonist, resveratrol, inhibits AR-st",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:9140,usability,visual,visualizer,9140,"STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. >>> get_entity_options(random_colors=True). {'colors': {'AMINO_ACID': '#30CBF7',. 'ANATOMICAL_SYSTEM': '#6DF980',. 'CANCER': '#1AE0F9',. 'CELL': '#5813C7',. 'CELLULAR_COMPONENT': '#0D350E',. 'CELL_LINE': '#1AA436',. 'CELL_TYPE': '#F837CC',. 'CHEBI': '#54B69E',. 'CHEMICAL': '#BADCA1',. 'CL': '#D845FB',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#0D9CB4',. 'DISEASE': '#78A2E5',. 'DNA': '#CAD406',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#EC2144',. 'GGP': '#A6AA7D',. 'GO': '#8312F0',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#F7E433',. 'MULTI-TISSUE_STRUCTURE': '#221891',. 'ORGAN': '#786BC0',. 'ORGANISM': '#43534C',. 'ORGANISM_SUBDIVISION': '#B6F342',. 'PROTEIN': '#4454D9',. 'RNA': '#64C158',. 'SIMPLE_CHEMICAL': '#F8616A',. 'SO': '#344E4D',. 'TAXON': '#63B69D',. 'TISSUE': '#0DE67C'},. 'ents': ['GGP',. 'SO',. 'TAXON',. 'CHEBI',. 'GO',. 'CL',. 'DNA',. 'CELL_TYPE',. 'CELL_LINE',. 'RNA',. 'PROTEIN',. 'DISEASE',. 'CHEMICAL',. 'CANCER',. 'ORGAN',. 'TISSUE',. 'ORGANISM',. 'CELL',. 'AMINO_ACID',. 'GENE_OR_GENE_PRODUCT',. 'SIMPLE_CHEMICAL',. 'ANATOMICAL_SYSTEM',. 'IMMATERIAL_ANATOMICAL_ENTITY',. 'MULTI-TISSUE_STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. ## default: get_entity_options(random_colors=False). ## displacy.serve(doc, style=""ent"", options=get_entity_options()). >>> displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). Using the 'ent' visualizer. Serving on http://0.0.0.0:5000 ... 127.0.0.1 -- -- [26/Nov/2019 13:43:47] ""GET / HTTP/1.1"" 200 20529. ```. ---. **Screenshots**. `random colors = False`:. ![spacy_tagged_text_browser-2019-11-26c](https://user-images.githubusercontent.com/2575920/69676848-78e92a80-1056-11ea-8ade-29aec5884d01.png). `random colors = True`:. ![spacy_tagged_text_browser-2019-11-26d](https://user-images.githubusercontent.com/2575920/69676857-7c7cb180-1056-11ea-9cd1-86c1feb206c6.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:9356,usability,user,user-images,9356,"STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. >>> get_entity_options(random_colors=True). {'colors': {'AMINO_ACID': '#30CBF7',. 'ANATOMICAL_SYSTEM': '#6DF980',. 'CANCER': '#1AE0F9',. 'CELL': '#5813C7',. 'CELLULAR_COMPONENT': '#0D350E',. 'CELL_LINE': '#1AA436',. 'CELL_TYPE': '#F837CC',. 'CHEBI': '#54B69E',. 'CHEMICAL': '#BADCA1',. 'CL': '#D845FB',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#0D9CB4',. 'DISEASE': '#78A2E5',. 'DNA': '#CAD406',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#EC2144',. 'GGP': '#A6AA7D',. 'GO': '#8312F0',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#F7E433',. 'MULTI-TISSUE_STRUCTURE': '#221891',. 'ORGAN': '#786BC0',. 'ORGANISM': '#43534C',. 'ORGANISM_SUBDIVISION': '#B6F342',. 'PROTEIN': '#4454D9',. 'RNA': '#64C158',. 'SIMPLE_CHEMICAL': '#F8616A',. 'SO': '#344E4D',. 'TAXON': '#63B69D',. 'TISSUE': '#0DE67C'},. 'ents': ['GGP',. 'SO',. 'TAXON',. 'CHEBI',. 'GO',. 'CL',. 'DNA',. 'CELL_TYPE',. 'CELL_LINE',. 'RNA',. 'PROTEIN',. 'DISEASE',. 'CHEMICAL',. 'CANCER',. 'ORGAN',. 'TISSUE',. 'ORGANISM',. 'CELL',. 'AMINO_ACID',. 'GENE_OR_GENE_PRODUCT',. 'SIMPLE_CHEMICAL',. 'ANATOMICAL_SYSTEM',. 'IMMATERIAL_ANATOMICAL_ENTITY',. 'MULTI-TISSUE_STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. ## default: get_entity_options(random_colors=False). ## displacy.serve(doc, style=""ent"", options=get_entity_options()). >>> displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). Using the 'ent' visualizer. Serving on http://0.0.0.0:5000 ... 127.0.0.1 -- -- [26/Nov/2019 13:43:47] ""GET / HTTP/1.1"" 200 20529. ```. ---. **Screenshots**. `random colors = False`:. ![spacy_tagged_text_browser-2019-11-26c](https://user-images.githubusercontent.com/2575920/69676848-78e92a80-1056-11ea-8ade-29aec5884d01.png). `random colors = True`:. ![spacy_tagged_text_browser-2019-11-26d](https://user-images.githubusercontent.com/2575920/69676857-7c7cb180-1056-11ea-9cd1-86c1feb206c6.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/141:9524,usability,user,user-images,9524,"STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. >>> get_entity_options(random_colors=True). {'colors': {'AMINO_ACID': '#30CBF7',. 'ANATOMICAL_SYSTEM': '#6DF980',. 'CANCER': '#1AE0F9',. 'CELL': '#5813C7',. 'CELLULAR_COMPONENT': '#0D350E',. 'CELL_LINE': '#1AA436',. 'CELL_TYPE': '#F837CC',. 'CHEBI': '#54B69E',. 'CHEMICAL': '#BADCA1',. 'CL': '#D845FB',. 'DEVELOPING_ANATOMICAL_STRUCTURE': '#0D9CB4',. 'DISEASE': '#78A2E5',. 'DNA': '#CAD406',. 'ENT': '#E8DAEF',. 'GENE_OR_GENE_PRODUCT': '#EC2144',. 'GGP': '#A6AA7D',. 'GO': '#8312F0',. 'IMMATERIAL_ANATOMICAL_ENTITY': '#F7E433',. 'MULTI-TISSUE_STRUCTURE': '#221891',. 'ORGAN': '#786BC0',. 'ORGANISM': '#43534C',. 'ORGANISM_SUBDIVISION': '#B6F342',. 'PROTEIN': '#4454D9',. 'RNA': '#64C158',. 'SIMPLE_CHEMICAL': '#F8616A',. 'SO': '#344E4D',. 'TAXON': '#63B69D',. 'TISSUE': '#0DE67C'},. 'ents': ['GGP',. 'SO',. 'TAXON',. 'CHEBI',. 'GO',. 'CL',. 'DNA',. 'CELL_TYPE',. 'CELL_LINE',. 'RNA',. 'PROTEIN',. 'DISEASE',. 'CHEMICAL',. 'CANCER',. 'ORGAN',. 'TISSUE',. 'ORGANISM',. 'CELL',. 'AMINO_ACID',. 'GENE_OR_GENE_PRODUCT',. 'SIMPLE_CHEMICAL',. 'ANATOMICAL_SYSTEM',. 'IMMATERIAL_ANATOMICAL_ENTITY',. 'MULTI-TISSUE_STRUCTURE',. 'DEVELOPING_ANATOMICAL_STRUCTURE',. 'ORGANISM_SUBDIVISION',. 'CELLULAR_COMPONENT']}. ## default: get_entity_options(random_colors=False). ## displacy.serve(doc, style=""ent"", options=get_entity_options()). >>> displacy.serve(doc, style=""ent"", options=get_entity_options(random_colors=True)). Using the 'ent' visualizer. Serving on http://0.0.0.0:5000 ... 127.0.0.1 -- -- [26/Nov/2019 13:43:47] ""GET / HTTP/1.1"" 200 20529. ```. ---. **Screenshots**. `random colors = False`:. ![spacy_tagged_text_browser-2019-11-26c](https://user-images.githubusercontent.com/2575920/69676848-78e92a80-1056-11ea-8ade-29aec5884d01.png). `random colors = True`:. ![spacy_tagged_text_browser-2019-11-26d](https://user-images.githubusercontent.com/2575920/69676857-7c7cb180-1056-11ea-9cd1-86c1feb206c6.png).",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141
https://github.com/allenai/scispacy/issues/142:0,usability,Close,Closed,0,Closed offline.,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/142
https://github.com/allenai/scispacy/issues/143:167,deployability,pipelin,pipelines,167,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:45,energy efficiency,Current,Currently,45,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:107,energy efficiency,model,models,107,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:162,energy efficiency,core,core,162,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:167,integrability,pipelin,pipelines,167,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:230,interoperability,specif,specifics,230,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:334,interoperability,standard,standard,334,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:107,security,model,models,107,"Hi! Thanks for your interest in the project. Currently you can use scispacy in the same way as other spacy models, so you can look at the spacy tutorials and the core pipelines should work in exactly the same way. If you have any specifics that you found hard to get started with, we could definitely write some tutorials for the non-standard parts of the library. Was there something in particular you found tricky?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:193,availability,avail,available,193,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:492,availability,avail,available,492,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:673,availability,replic,replicate,673,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1011,availability,avail,available,1011,"you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=A",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1545,deployability,pipelin,pipelines,1545,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:165,energy efficiency,current,current,165,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:461,energy efficiency,current,currently,461,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1420,energy efficiency,Current,Currently,1420,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1482,energy efficiency,model,models,1482,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1540,energy efficiency,core,core,1540,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1545,integrability,pipelin,pipelines,1545,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:428,interoperability,specif,specific,428,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:779,interoperability,specif,specific,779,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1616,interoperability,specif,specifics,1616,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1723,interoperability,standard,standard,1723,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1080,modifiability,exten,extent,1080,"ly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREX",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:193,reliability,availab,available,193,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:492,reliability,availab,available,492,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1011,reliability,availab,available,1011,"you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=A",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:193,safety,avail,available,193,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:492,safety,avail,available,492,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:832,safety,test,test,832,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1011,safety,avail,available,1011,"you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=A",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:193,security,availab,available,193,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:492,security,availab,available,492,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1011,security,availab,available,1011,"you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=A",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1482,security,model,models,1482,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1854,security,auth,authored,1854,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:2223,security,auth,auth,2223,". feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecomment-518760592>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AB423EIMUY3JAMUKBMWPCI3QDGWC5ANCNFSM4IJWN6JQ>. > . >.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:832,testability,test,test,832,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1054,testability,understand,understanding,1054,"o sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:104,usability,experien,experience,104,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:921,usability,stop,stopwords,921,"Hi Mark,. Thank you very much for your reply. I am very new to sci-spacy and honestly, do not have much experience with. spacy either. I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notificatio",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:1132,usability,user,users,1132,"I am just very curious of the current breadth of features available in. sci-spacy. Until now, I have used the abbreviation feature and UMLS linker. feature. I assume native spacy do not have UMLS linker, and this is. precisely my motivation for asking the above question. What are the bionlp. specific use cases sci-spacy can currently handle which are not available. in native spacy? Because there are only two examples it is hard to realize. the full potential of sci-spacy. I attempted the examples on github-repo page and was able to replicate the. results so no issues there. I believe that sci-spacy can do a good name-entity recognition specific to. biomedical domains though I have yet to test it myself. However, I wonder. can I use it to pre-process text to remove biomedical stopwords, or. generate word embeddings. I am not saying I request these features if not. available but it would be great to have an understanding of the full extent. of capabilities of sci-spacy especially for users who are new to BioNLP. such as myself. Thank you for your reply and also for developing sci-spacy in first place! Warm regards,. Varshit Dusad. On Tue, Aug 6, 2019 at 10:41 PM Mark Neumann <notifications@github.com>. wrote:. > Hi! >. > Thanks for your interest in the project. >. > Currently you can use scispacy in the same way as other spacy models, so. > you can look at the spacy tutorials and the core pipelines should work in. > exactly the same way. >. > If you have any specifics that you found hard to get started with, we. > could definitely write some tutorials for the non-standard parts of the. > library. Was there something in particular you found tricky? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/allenai/scispacy/issues/143?email_source=notifications&email_token=AB423EKXXQIURLR3FP6N3WTQDGWC5A5CNFSM4IJWN6J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3V2REA#issuecom",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:116,deployability,depend,dependency,116,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:212,energy efficiency,model,models,212,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:279,energy efficiency,model,models,279,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:116,integrability,depend,dependency,116,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:171,integrability,Pub,PubMed,171,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:266,interoperability,specif,specific,266,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:456,interoperability,specif,specific,456,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:116,modifiability,depend,dependency,116,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:20,safety,compl,complete,20,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:116,safety,depend,dependency,116,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:348,safety,detect,detection,348,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:20,security,compl,complete,20,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:52,security,modif,modified,52,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:61,security,token,tokenizer,61,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:212,security,model,models,212,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:279,security,model,models,279,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:348,security,detect,detection,348,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:116,testability,depend,dependency,116,"Here is (I think) a complete list of features: . 1. modified tokenizer to better handle academic text. 1. retrained dependency parser and pos tagger on GENIA treebank. 1. PubMed word vectors (in medium and large models). 1. Retrained NER on MedMentions. 1. Four bio specific NER models trained on BIONLP13CG, BC5CDR, JNLPBA, CRAFT. 1. Abbreviation detection pipe. 1. Entity linking candidate generation pipe for linking entities to UMLS. Do you have other specific questions?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:317,availability,cluster,clustering,317,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:212,deployability,build,building,212,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:227,deployability,pipelin,pipeline,227,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:317,deployability,cluster,clustering,317,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:551,energy efficiency,current,current,551,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:227,integrability,pipelin,pipeline,227,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:277,integrability,abstract,abstract,277,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:367,integrability,abstract,abstract,367,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:277,modifiability,abstract,abstract,277,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:367,modifiability,abstract,abstract,367,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:24,safety,input,input,24,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:82,safety,input,inputs,82,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:24,usability,input,input,24,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:82,usability,input,inputs,82,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/143:605,usability,learn,learning,605,"Hi Dan,. Thanks for the input. I am working on a medical use case and i need some inputs :. 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ? 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143
https://github.com/allenai/scispacy/issues/144:113,safety,detect,detectors,113,"looks like the linking pipe crashes if there are no entities found in the doc, which is pretty rare for the base detectors trained on medmentions. i'll fix real quick",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/144
https://github.com/allenai/scispacy/issues/144:113,security,detect,detectors,113,"looks like the linking pipe crashes if there are no entities found in the doc, which is pretty rare for the base detectors trained on medmentions. i'll fix real quick",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/144
https://github.com/allenai/scispacy/issues/149:0,reliability,Doe,Does,0,"Does it not work at all, even for more normal sentences?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/149:16,availability,fault,fault,16,"Sorry, it is my fault. The issue was resolved after correctly setting the spacy. :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/149:16,energy efficiency,fault,fault,16,"Sorry, it is my fault. The issue was resolved after correctly setting the spacy. :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/149:16,performance,fault,fault,16,"Sorry, it is my fault. The issue was resolved after correctly setting the spacy. :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/149:16,reliability,fault,fault,16,"Sorry, it is my fault. The issue was resolved after correctly setting the spacy. :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/149:16,safety,fault,fault,16,"Sorry, it is my fault. The issue was resolved after correctly setting the spacy. :)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/149
https://github.com/allenai/scispacy/issues/150:270,deployability,pipelin,pipeline,270,"Hi! Good news - we have that functionality already! https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature. if you set `resolve_abbreviations : bool = True` and add the [abbreviation detector](https://github.com/allenai/scispacy#abbreviationdetector) to the pipeline _before_ the linker, the linker will use the long forms of any abbreviations that it finds.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:270,integrability,pipelin,pipeline,270,"Hi! Good news - we have that functionality already! https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature. if you set `resolve_abbreviations : bool = True` and add the [abbreviation detector](https://github.com/allenai/scispacy#abbreviationdetector) to the pipeline _before_ the linker, the linker will use the long forms of any abbreviations that it finds.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:195,safety,detect,detector,195,"Hi! Good news - we have that functionality already! https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature. if you set `resolve_abbreviations : bool = True` and add the [abbreviation detector](https://github.com/allenai/scispacy#abbreviationdetector) to the pipeline _before_ the linker, the linker will use the long forms of any abbreviations that it finds.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:195,security,detect,detector,195,"Hi! Good news - we have that functionality already! https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature. if you set `resolve_abbreviations : bool = True` and add the [abbreviation detector](https://github.com/allenai/scispacy#abbreviationdetector) to the pipeline _before_ the linker, the linker will use the long forms of any abbreviations that it finds.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:1860,availability,state,state,1860,"re generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cells, IPS. CUI: C0872076, Name: Pluripotent Stem Cells. Definition: Cells that can give rise to cells of the three different GERM LAYERS. TUI(s): T025. Aliases (abbreviated, total: 12):. Pluripotent Stem Cells, pluripotent stem cells, pluripotent stem cell, pluripotent stem cell, pluripotent stem cell, Pluripotent Stem Cell, Pluripotent Stem Cell, Stem Cell, Pluripotent, Pluripotent stem cell, Pluripotent stem cell. (scispacy) ELSSLCM-4176012:scispacy danielr$. Best regards,. Ron Daniel, Ph.D. Director, Elsevier Labs | ELSEVIER. +1 619 208 3064 | r.daniel@elsevier.com<mailto:r.daniel@elsevier.com>. From: Mark Neumann <notifications@github.com>. Reply-To: allenai/scispacy <reply@reply.github.com>. Date: Tuesday, August 13, 2019 at 2:08 PM. To: allenai/scispacy <scispacy@noreply.github.com>. Cc: Ron Daniel <R.Daniel@elsevier.com>, Author <a",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:994,deployability,pipelin,pipeline,994,"Hi Mark,. Thanks for your prompt reply. My apologies for taking a few days to reply. I think I am already using resolve_abbreviations=True in the proper manner. Here’s a bit of test code and output. Note that the abbreviations code does find ‘Human induced pluripotent stem cells’ as the long form for hiPSC, but the UMLS codes for hiPSC don’t include that one. # Test of scispacy to see problem where it doesn't find. # CUI C3658289 for Human Induced Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:1011,deployability,Pipelin,Pipeline,1011,"s for your prompt reply. My apologies for taking a few days to reply. I think I am already using resolve_abbreviations=True in the proper manner. Here’s a bit of test code and output. Note that the abbreviations code does find ‘Human induced pluripotent stem cells’ as the long form for hiPSC, but the UMLS codes for hiPSC don’t include that one. # Test of scispacy to see problem where it doesn't find. # CUI C3658289 for Human Induced Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:1445,deployability,Pipelin,Pipeline,1445,"ed Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cells, IPS. CUI: C0872076, Name: Pluripotent Stem Cells. Definition: Cells that can give rise to cells of the three different GERM LAYERS. TUI(s): T025. Aliases (abbreviated, total: 12):. Pluripotent Stem Cells, pluripotent stem cells, pluripotent stem cell, pluripotent stem cell, pluripotent stem cell, Pluripotent Stem Cell, Pluripotent Stem Cell, Stem Cell, Pluripotent, Pluripotent stem cell, Pluripotent stem cell. (scispacy) ELSSL",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:3264,deployability,pipelin,pipeline,3264,"s that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cells, IPS. CUI: C0872076, Name: Pluripotent Stem Cells. Definition: Cells that can give rise to cells of the three different GERM LAYERS. TUI(s): T025. Aliases (abbreviated, total: 12):. Pluripotent Stem Cells, pluripotent stem cells, pluripotent stem cell, pluripotent stem cell, pluripotent stem cell, Pluripotent Stem Cell, Pluripotent Stem Cell, Stem Cell, Pluripotent, Pluripotent stem cell, Pluripotent stem cell. (scispacy) ELSSLCM-4176012:scispacy danielr$. Best regards,. Ron Daniel, Ph.D. Director, Elsevier Labs | ELSEVIER. +1 619 208 3064 | r.daniel@elsevier.com<mailto:r.daniel@elsevier.com>. From: Mark Neumann <notifications@github.com>. Reply-To: allenai/scispacy <reply@reply.github.com>. Date: Tuesday, August 13, 2019 at 2:08 PM. To: allenai/scispacy <scispacy@noreply.github.com>. Cc: Ron Daniel <R.Daniel@elsevier.com>, Author <author@noreply.github.com>. Subject: Re: [allenai/scispacy] Abbreviations and UMLS linking (#150). *** External email: use caution ***. Hi! Good news - we have that functionality already! https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature. if you set resolve_abbreviations : bool = True and add the abbreviation detector<https://github.com/allenai/scispacy#abbreviationdetector> to the pipeline before the linker, the linker will use the long forms of any abbreviations that it finds. —. You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/150?email_source=notifications&email_token=AAGVMXYFGH7E6R2NZ4ZPOO3QEMPEHA5CNFSM4ILM75Q2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4G7VGA#issuecomment-521009816>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AAGVMX76EQRNL4XQF3OZ47DQEMPEHANCNFSM4ILM75QQ>.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:627,energy efficiency,load,load,627,"Hi Mark,. Thanks for your prompt reply. My apologies for taking a few days to reply. I think I am already using resolve_abbreviations=True in the proper manner. Here’s a bit of test code and output. Note that the abbreviations code does find ‘Human induced pluripotent stem cells’ as the long form for hiPSC, but the UMLS codes for hiPSC don’t include that one. # Test of scispacy to see problem where it doesn't find. # CUI C3658289 for Human Induced Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:994,integrability,pipelin,pipeline,994,"Hi Mark,. Thanks for your prompt reply. My apologies for taking a few days to reply. I think I am already using resolve_abbreviations=True in the proper manner. Here’s a bit of test code and output. Note that the abbreviations code does find ‘Human induced pluripotent stem cells’ as the long form for hiPSC, but the UMLS codes for hiPSC don’t include that one. # Test of scispacy to see problem where it doesn't find. # CUI C3658289 for Human Induced Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:1011,integrability,Pipelin,Pipeline,1011,"s for your prompt reply. My apologies for taking a few days to reply. I think I am already using resolve_abbreviations=True in the proper manner. Here’s a bit of test code and output. Note that the abbreviations code does find ‘Human induced pluripotent stem cells’ as the long form for hiPSC, but the UMLS codes for hiPSC don’t include that one. # Test of scispacy to see problem where it doesn't find. # CUI C3658289 for Human Induced Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cel",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
https://github.com/allenai/scispacy/issues/150:1445,integrability,Pipelin,Pipeline,1445,"ed Pluripotent stem cells. import spacy. import scispacy. from scispacy.abbreviation import AbbreviationDetector. from scispacy.umls_linking import UmlsEntityLinker. nlp = spacy.load(""en_core_sci_sm""). abbreviation_pipe = AbbreviationDetector(nlp). nlp.add_pipe(abbreviation_pipe). linker = UmlsEntityLinker(resolve_abbreviations=True). nlp.add_pipe(linker). text1 = ""Human induced pluripotent stem cells (hiPSC) are generated from ""\. ""reprogrammed fibroblasts by overexpression of pluripotency factors.\n"". # Process the text w/ full SciSpacy pipeline. print(""Pipeline: "", nlp.pipe_names). doc1 = nlp(text1). print(""Abbreviations:""). for a in doc1._.abbreviations:. print(a, a._.long_form). print(""\nEntities: "", doc1.ents). # Comment out next block and uncomment loop below it to see all UMLS entities. entity = doc1.ents[3] # for the hiPSC case. print(""Entity Name: "", entity). for umls_ent in entity._.umls_ents:. print(""\t"", linker.umls.cui_to_entity[umls_ent[0]]). $ python test_abbrv.py. Pipeline: ['tagger', 'parser', 'ner', 'AbbreviationDetector', 'UmlsEntityLinker']. Abbreviations:. hiPSC Human induced pluripotent stem cells. Entities: (Human, induced, pluripotent stem cells, hiPSC, fibroblasts, overexpression, pluripotency factors). Entity Name: hiPSC. CUI: C2717959, Name: Induced Pluripotent Stem Cells. Definition: Cells from adult organisms that have been reprogrammed into a pluripotential state similar to that of EMBRYONIC STEM CELLS. TUI(s): T025. Aliases: (total: 6):. Induced Pluripotent Stem Cell, iPSC, IPS Cells, Cell, IPS, IPS Cell, Cells, IPS. CUI: C0872076, Name: Pluripotent Stem Cells. Definition: Cells that can give rise to cells of the three different GERM LAYERS. TUI(s): T025. Aliases (abbreviated, total: 12):. Pluripotent Stem Cells, pluripotent stem cells, pluripotent stem cell, pluripotent stem cell, pluripotent stem cell, Pluripotent Stem Cell, Pluripotent Stem Cell, Stem Cell, Pluripotent, Pluripotent stem cell, Pluripotent stem cell. (scispacy) ELSSL",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/150
